{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    " - [x] check if changing to tensors with gradient automatically changes the inputs\n",
    " - [ ] get multidimensional regression to work\n",
    " - [ ] check if gradient is calculated correctly for the inputs\n",
    " - [ ] check if in an toy example we could guess a missing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defaults': {'lr': 0.001,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False},\n",
       " '_zero_grad_profile_name': 'Optimizer.zero_grad#SGD.zero_grad',\n",
       " 'state': defaultdict(dict, {}),\n",
       " 'param_groups': [{'params': [Parameter containing:\n",
       "    tensor([[-0.4486]], requires_grad=True),\n",
       "    Parameter containing:\n",
       "    tensor([0.4860], requires_grad=True)],\n",
       "   'lr': 0.001,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False}],\n",
       " '_warned_capturable_if_run_uncaptured': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.1723\n",
      "Epoch [10/60], Loss: 0.1721\n",
      "Epoch [15/60], Loss: 0.1720\n",
      "Epoch [20/60], Loss: 0.1720\n",
      "Epoch [25/60], Loss: 0.1720\n",
      "Epoch [30/60], Loss: 0.1720\n",
      "Epoch [35/60], Loss: 0.1720\n",
      "Epoch [40/60], Loss: 0.1720\n",
      "Epoch [45/60], Loss: 0.1720\n",
      "Epoch [50/60], Loss: 0.1720\n",
      "Epoch [55/60], Loss: 0.1719\n",
      "Epoch [60/60], Loss: 0.1719\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.tensor(x_train, requires_grad=True)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if we could create our own linear regression where we recover a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Always keep normalization in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[1, 2], [2, 3], [4, 5], [14, 3], [1, 12.312], [1, 2.345], \n",
    "                    [5, 2], [12, 4], [12, 5], [19, 12], [38, 5], \n",
    "                    [12, 1], [12, 32], [11, 1], [1, 1]], dtype=np.float32)\n",
    "\n",
    "y_train = (x_train * np.array([2, 4], dtype=np.float32)).sum(axis=1).reshape(-1, 1) + 0.1 * (np.random.rand(15) - 0.5)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 225.7617\n",
      "Epoch [10/60], Loss: 54.0790\n",
      "Epoch [15/60], Loss: 15.5246\n",
      "Epoch [20/60], Loss: 4.4686\n",
      "Epoch [25/60], Loss: 1.2893\n",
      "Epoch [30/60], Loss: 0.3750\n",
      "Epoch [35/60], Loss: 0.1121\n",
      "Epoch [40/60], Loss: 0.0364\n",
      "Epoch [45/60], Loss: 0.0146\n",
      "Epoch [50/60], Loss: 0.0083\n",
      "Epoch [55/60], Loss: 0.0065\n",
      "Epoch [60/60], Loss: 0.0059\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x has 2 columns but y has 15 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plot the graph\u001b[39;00m\n\u001b[1;32m     19\u001b[0m predicted \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_train))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOriginal data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_train, predicted, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitted line\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[0;32m~/programming/web_development/dataframetrainer/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2785\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/web_development/dataframetrainer/.venv/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/programming/web_development/dataframetrainer/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/web_development/dataframetrainer/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:522\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    520\u001b[0m ncx, ncy \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ncx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ncy \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ncx \u001b[38;5;241m!=\u001b[39m ncy:\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns but y has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ncx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ncy \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[0;31mValueError\u001b[0m: x has 2 columns but y has 15 columns"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.tensor(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the model parameters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "d = OrderedDict({'weight': Parameter(data=torch.tensor([[2, 3.9]], dtype=torch.float32), requires_grad=True),\n",
    "                 'bias': Parameter(data=torch.tensor([0], dtype=torch.float32), requires_grad=True)})\n",
    "model._parameters = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us remove one parameter and check whether we still find an acceptable solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[1, 2], [2, 3], [4, 5], [14, 3], [1, 12.312], [1, 2.345], \n",
    "                    [5, 2], [12, 4], [12, 5], [19, 12], [38, 5], \n",
    "                    [12, 1], [12, 32], [11, 1], [1, 1]], dtype=np.float32)\n",
    "\n",
    "y_train = (x_train * np.array([2, 4], dtype=np.float32)).sum(axis=1).reshape(-1, 1) + 0.1 * (np.random.rand(15) - 0.5)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manipulate the training data to exclude one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.00000000e+00],\n",
       "       [4.00000000e+00, 1.40000000e+01],\n",
       "       [1.00000000e+00, 1.00000000e+00],\n",
       "       [5.00000000e+00, 1.20000000e+01],\n",
       "       [1.20000000e+01, 1.90000000e+01],\n",
       "       [3.80000000e+01, 1.20000000e+01],\n",
       "       [1.20000000e+01, 1.10000000e+01],\n",
       "       [1.00000000e+00, 2.34290436e-01],\n",
       "       [8.86693210e-01, 1.43554590e-01],\n",
       "       [7.17692831e-01, 5.41982109e-01],\n",
       "       [7.86232719e-01, 2.68858053e-01],\n",
       "       [3.80623010e-01, 9.07778784e-01],\n",
       "       [2.73902574e-01, 5.80978158e-01],\n",
       "       [3.03796041e-01, 9.83689234e-01],\n",
       "       [6.29990309e-01, 2.69741869e-02]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((x_train[:, 0], np.random.rand(x_train.shape[0]))).reshape(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.tensor(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Chess Elo and try to implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_data.csv\")\n",
    "df.columns = ['month', 'white', 'black', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_ids = list(set(list(df[\"white\"].unique()) + list(df[\"black\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_srs = pd.Series(player_ids).reset_index().rename(columns={0: 'id'}).set_index('id')['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['white_mapped'] = df['white'].map(mapping_srs)\n",
    "df['black_mapped'] = df['black'].map(mapping_srs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_tensors = torch.randn(len(mapping_srs), 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size1 + input_size2, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(3, 3, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert score so that pytorch can handle it\n",
    "df[\"score\"] = df[\"score\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_tensors = torch.randn(len(mapping_srs), 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7498,  0.7656, -1.6891],\n",
       "        [-0.7537, -0.7414,  0.3946],\n",
       "        [ 0.2428,  0.0327, -0.0109],\n",
       "        ...,\n",
       "        [ 0.3606, -0.0112, -1.5369],\n",
       "        [ 0.5878, -0.5341, -0.2857],\n",
       "        [-0.2961, -1.1575,  1.0276]], requires_grad=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1. , 2.], [3., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, :] = t[0, :] * 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9500, 1.9000],\n",
       "        [3.0000, 4.0000]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_tensors = torch.randn(len(mapping_srs), 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "lr_p = 0.05\n",
    "\n",
    "curr_df = df.iloc[idx*batch_size:(idx+1)*batch_size]\n",
    "white_tensors = torch.clone(player_tensors[curr_df[\"white_mapped\"].values, :])\n",
    "black_tensors = torch.clone(player_tensors[curr_df[\"black_mapped\"].values, :])\n",
    "predicted = model(white_tensors, black_tensors)\n",
    "result = torch.from_numpy(curr_df['score'].values).reshape(-1, 1)\n",
    "loss = criterion(result, predicted)\n",
    "\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_tensors.grad = torch.zeros(player_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7090, -1.5282, -0.1766], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors[61, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2449e-05, -4.8616e-04, -2.8824e-04])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors.grad[61, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00905229, -0.14931692,  0.35107905], dtype=float32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(player_tensors - lr_p * player_tensors.grad)[61, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.14205339550971985\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "lr_p = 0.05\n",
    "\n",
    "curr_df = df.iloc[idx*batch_size:(idx+1)*batch_size]\n",
    "white_tensors = torch.clone(player_tensors[curr_df[\"white_mapped\"].values, :])\n",
    "black_tensors = torch.clone(player_tensors[curr_df[\"black_mapped\"].values, :])\n",
    "predicted = model(white_tensors, black_tensors)\n",
    "result = torch.from_numpy(curr_df['score'].values).reshape(-1, 1)\n",
    "loss = criterion(result, predicted)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "player_tensors.grad = torch.zeros(player_tensors.shape)\n",
    "loss.backward()\n",
    "player_tensors = player_tensors - lr_p * player_tensors.grad\n",
    "optimizer.step()\n",
    "print(f\"This is the loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_tensors = torch.randn(len(mapping_srs), 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.1360001266002655\n",
      "This is the loss 0.17638623714447021\n",
      "This is the loss 0.1419927030801773\n",
      "This is the loss 0.16574245691299438\n",
      "This is the loss 0.1774844378232956\n",
      "This is the loss 0.17787256836891174\n",
      "This is the loss 0.14479435980319977\n",
      "This is the loss 0.14542533457279205\n",
      "This is the loss 0.13149216771125793\n",
      "This is the loss 0.12388581037521362\n",
      "This is the loss 0.12102214246988297\n",
      "This is the loss 0.13899151980876923\n",
      "This is the loss 0.1290937066078186\n",
      "This is the loss 0.13390779495239258\n",
      "This is the loss 0.12751109898090363\n",
      "This is the loss 0.12318005412817001\n",
      "This is the loss 0.1522451937198639\n",
      "This is the loss 0.1453549712896347\n",
      "This is the loss 0.16942746937274933\n",
      "This is the loss 0.11738632619380951\n",
      "This is the loss 0.1310575008392334\n",
      "This is the loss 0.15155379474163055\n",
      "This is the loss 0.11702240258455276\n",
      "This is the loss 0.14296644926071167\n",
      "This is the loss 0.15209458768367767\n",
      "This is the loss 0.12222142517566681\n",
      "This is the loss 0.15920475125312805\n",
      "This is the loss 0.11629971116781235\n",
      "This is the loss 0.14857012033462524\n",
      "This is the loss 0.12656858563423157\n",
      "This is the loss 0.1546110063791275\n",
      "This is the loss 0.12606897950172424\n",
      "This is the loss 0.15046627819538116\n",
      "This is the loss 0.12004396319389343\n",
      "This is the loss 0.12957558035850525\n",
      "This is the loss 0.14746715128421783\n",
      "This is the loss 0.12842893600463867\n",
      "This is the loss 0.13494840264320374\n",
      "This is the loss 0.09806110709905624\n",
      "This is the loss 0.14065614342689514\n",
      "This is the loss 0.13176646828651428\n",
      "This is the loss 0.11545796692371368\n",
      "This is the loss 0.14235332608222961\n",
      "This is the loss 0.1526435762643814\n",
      "This is the loss 0.1471501886844635\n",
      "This is the loss 0.1398095190525055\n",
      "This is the loss 0.16172538697719574\n",
      "This is the loss 0.10052498430013657\n",
      "This is the loss 0.16954195499420166\n",
      "This is the loss 0.11304384469985962\n",
      "This is the loss 0.13507363200187683\n",
      "This is the loss 0.13409921526908875\n",
      "This is the loss 0.13536205887794495\n",
      "This is the loss 0.134990856051445\n",
      "This is the loss 0.12552142143249512\n",
      "This is the loss 0.11927536129951477\n",
      "This is the loss 0.13735726475715637\n",
      "This is the loss 0.15112243592739105\n",
      "This is the loss 0.14838331937789917\n",
      "This is the loss 0.12703022360801697\n",
      "This is the loss 0.12463582307100296\n",
      "This is the loss 0.11316137760877609\n",
      "This is the loss 0.14914824068546295\n",
      "This is the loss 0.16096198558807373\n",
      "This is the loss 0.1267017424106598\n",
      "This is the loss 0.12027931213378906\n",
      "This is the loss 0.13863646984100342\n",
      "This is the loss 0.1497737020254135\n",
      "This is the loss 0.13322660326957703\n",
      "This is the loss 0.1446375697851181\n",
      "This is the loss 0.1511232852935791\n",
      "This is the loss 0.17058296501636505\n",
      "This is the loss 0.16298453509807587\n",
      "This is the loss 0.15400923788547516\n",
      "This is the loss 0.1324000209569931\n",
      "This is the loss 0.16079725325107574\n",
      "This is the loss 0.15059156715869904\n",
      "This is the loss 0.12889011204242706\n",
      "This is the loss 0.13048890233039856\n",
      "This is the loss 0.14746737480163574\n",
      "This is the loss 0.14793409407138824\n",
      "This is the loss 0.14042678475379944\n",
      "This is the loss 0.14906133711338043\n",
      "This is the loss 0.14140672981739044\n",
      "This is the loss 0.12792786955833435\n",
      "This is the loss 0.14063629508018494\n",
      "This is the loss 0.1414688378572464\n",
      "This is the loss 0.13375484943389893\n",
      "This is the loss 0.15297092497348785\n",
      "This is the loss 0.12235815823078156\n",
      "This is the loss 0.12384519726037979\n",
      "This is the loss 0.16203942894935608\n",
      "This is the loss 0.14779630303382874\n",
      "This is the loss 0.1627214550971985\n",
      "This is the loss 0.11218027770519257\n",
      "This is the loss 0.10787834972143173\n",
      "This is the loss 0.14382338523864746\n",
      "This is the loss 0.11053662747144699\n",
      "This is the loss 0.11385436356067657\n",
      "This is the loss 0.1439976990222931\n",
      "This is the loss 0.13584217429161072\n",
      "This is the loss 0.13213030993938446\n",
      "This is the loss 0.1306227445602417\n",
      "This is the loss 0.16242721676826477\n",
      "This is the loss 0.1517462581396103\n",
      "This is the loss 0.13962091505527496\n",
      "This is the loss 0.13204556703567505\n",
      "This is the loss 0.1440998613834381\n",
      "This is the loss 0.14052081108093262\n",
      "This is the loss 0.15198616683483124\n",
      "This is the loss 0.1148553267121315\n",
      "This is the loss 0.15501640737056732\n",
      "This is the loss 0.1642957627773285\n",
      "This is the loss 0.15688744187355042\n",
      "This is the loss 0.10838554054498672\n",
      "This is the loss 0.13439150154590607\n",
      "This is the loss 0.08917879313230515\n",
      "This is the loss 0.14428135752677917\n",
      "This is the loss 0.1336384266614914\n",
      "This is the loss 0.17393670976161957\n",
      "This is the loss 0.10826927423477173\n",
      "This is the loss 0.11514709144830704\n",
      "This is the loss 0.12981481850147247\n",
      "This is the loss 0.12633642554283142\n",
      "This is the loss 0.1598486453294754\n",
      "This is the loss 0.13180311024188995\n",
      "This is the loss 0.11542992293834686\n",
      "This is the loss 0.1619897335767746\n",
      "This is the loss 0.13026687502861023\n",
      "This is the loss 0.15723636746406555\n",
      "This is the loss 0.11903326213359833\n",
      "This is the loss 0.14561815559864044\n",
      "This is the loss 0.1422176957130432\n",
      "This is the loss 0.13021911680698395\n",
      "This is the loss 0.15446023643016815\n",
      "This is the loss 0.1278407871723175\n",
      "This is the loss 0.15681001543998718\n",
      "This is the loss 0.13676442205905914\n",
      "This is the loss 0.11484663188457489\n",
      "This is the loss 0.1287236213684082\n",
      "This is the loss 0.10526816546916962\n",
      "This is the loss 0.11152999103069305\n",
      "This is the loss 0.1568789780139923\n",
      "This is the loss 0.13256731629371643\n",
      "This is the loss 0.13537423312664032\n",
      "This is the loss 0.11500982940196991\n",
      "This is the loss 0.12049026042222977\n",
      "This is the loss 0.14477388560771942\n",
      "This is the loss 0.13602229952812195\n",
      "This is the loss 0.13114677369594574\n",
      "This is the loss 0.17192986607551575\n",
      "This is the loss 0.1499411165714264\n",
      "This is the loss 0.1323365420103073\n",
      "This is the loss 0.1110033169388771\n",
      "This is the loss 0.13677527010440826\n",
      "This is the loss 0.142437145113945\n",
      "This is the loss 0.14142049849033356\n",
      "This is the loss 0.11220121383666992\n",
      "This is the loss 0.14422141015529633\n",
      "This is the loss 0.12836842238903046\n",
      "This is the loss 0.11661536991596222\n",
      "This is the loss 0.12874461710453033\n",
      "This is the loss 0.12809067964553833\n",
      "This is the loss 0.13437949120998383\n",
      "This is the loss 0.13280647993087769\n",
      "This is the loss 0.12857067584991455\n",
      "This is the loss 0.16846123337745667\n",
      "This is the loss 0.14962361752986908\n",
      "This is the loss 0.15129894018173218\n",
      "This is the loss 0.1453363299369812\n",
      "This is the loss 0.12039755284786224\n",
      "This is the loss 0.14300718903541565\n",
      "This is the loss 0.12408340722322464\n",
      "This is the loss 0.13977555930614471\n",
      "This is the loss 0.1541423797607422\n",
      "This is the loss 0.12037023901939392\n",
      "This is the loss 0.1463836431503296\n",
      "This is the loss 0.1424773931503296\n",
      "This is the loss 0.1372121125459671\n",
      "This is the loss 0.14696358144283295\n",
      "This is the loss 0.15111473202705383\n",
      "This is the loss 0.1559111475944519\n",
      "This is the loss 0.14057455956935883\n",
      "This is the loss 0.14666227996349335\n",
      "This is the loss 0.14244189858436584\n",
      "This is the loss 0.1452745497226715\n",
      "This is the loss 0.12491079419851303\n",
      "This is the loss 0.1502290666103363\n",
      "This is the loss 0.1572647988796234\n",
      "This is the loss 0.10711728036403656\n",
      "This is the loss 0.13037709891796112\n",
      "This is the loss 0.1200035810470581\n",
      "This is the loss 0.14947324991226196\n",
      "This is the loss 0.1375327706336975\n",
      "This is the loss 0.13561701774597168\n",
      "This is the loss 0.1306927651166916\n",
      "This is the loss 0.11597946286201477\n",
      "This is the loss 0.1467718482017517\n",
      "This is the loss 0.11849092692136765\n",
      "This is the loss 0.11547507345676422\n",
      "This is the loss 0.11225872486829758\n",
      "This is the loss 0.12369274348020554\n",
      "This is the loss 0.12675216794013977\n",
      "This is the loss 0.11020363867282867\n",
      "This is the loss 0.1467614322900772\n",
      "This is the loss 0.11900772154331207\n",
      "This is the loss 0.13963624835014343\n",
      "This is the loss 0.12623852491378784\n",
      "This is the loss 0.11644811928272247\n",
      "This is the loss 0.12126164138317108\n",
      "This is the loss 0.16601552069187164\n",
      "This is the loss 0.13139137625694275\n",
      "This is the loss 0.16832225024700165\n",
      "This is the loss 0.15226508677005768\n",
      "This is the loss 0.16542179882526398\n",
      "This is the loss 0.11656458675861359\n",
      "This is the loss 0.12716630101203918\n",
      "This is the loss 0.0903559997677803\n",
      "This is the loss 0.12104640901088715\n",
      "This is the loss 0.16134043037891388\n",
      "This is the loss 0.10657352209091187\n",
      "This is the loss 0.14757268130779266\n",
      "This is the loss 0.12063246965408325\n",
      "This is the loss 0.1252652257680893\n",
      "This is the loss 0.14193426072597504\n",
      "This is the loss 0.14911575615406036\n",
      "This is the loss 0.12147996574640274\n",
      "This is the loss 0.14184415340423584\n",
      "This is the loss 0.15277671813964844\n",
      "This is the loss 0.13085365295410156\n",
      "This is the loss 0.12274082005023956\n",
      "This is the loss 0.11958047747612\n",
      "This is the loss 0.13727831840515137\n",
      "This is the loss 0.14019516110420227\n",
      "This is the loss 0.14334167540073395\n",
      "This is the loss 0.17779074609279633\n",
      "This is the loss 0.14182311296463013\n",
      "This is the loss 0.1435304582118988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.16216903924942017\n",
      "This is the loss 0.12998628616333008\n",
      "This is the loss 0.16194309294223785\n",
      "This is the loss 0.1377306878566742\n",
      "This is the loss 0.15201923251152039\n",
      "This is the loss 0.1355239897966385\n",
      "This is the loss 0.18430352210998535\n",
      "This is the loss 0.1499878615140915\n",
      "This is the loss 0.13651876151561737\n",
      "This is the loss 0.12374527007341385\n",
      "This is the loss 0.14243653416633606\n",
      "This is the loss 0.1260196417570114\n",
      "This is the loss 0.11731830984354019\n",
      "This is the loss 0.12931323051452637\n",
      "This is the loss 0.11193614453077316\n",
      "This is the loss 0.13951988518238068\n",
      "This is the loss 0.14698661863803864\n",
      "This is the loss 0.15753190219402313\n",
      "This is the loss 0.14805695414543152\n",
      "This is the loss 0.11675775051116943\n",
      "This is the loss 0.1489066779613495\n",
      "This is the loss 0.1302705556154251\n",
      "This is the loss 0.1345127820968628\n",
      "This is the loss 0.12766863405704498\n",
      "This is the loss 0.1305215060710907\n",
      "This is the loss 0.13577473163604736\n",
      "This is the loss 0.10671680420637131\n",
      "This is the loss 0.12182450294494629\n",
      "This is the loss 0.1433381736278534\n",
      "This is the loss 0.14277327060699463\n",
      "This is the loss 0.10437993705272675\n",
      "This is the loss 0.1443794071674347\n",
      "This is the loss 0.11537671089172363\n",
      "This is the loss 0.13287508487701416\n",
      "This is the loss 0.14330975711345673\n",
      "This is the loss 0.15257026255130768\n",
      "This is the loss 0.15417228639125824\n",
      "This is the loss 0.1315554529428482\n",
      "This is the loss 0.15647879242897034\n",
      "This is the loss 0.15928812325000763\n",
      "This is the loss 0.11967533826828003\n",
      "This is the loss 0.10420200228691101\n",
      "This is the loss 0.11782930791378021\n",
      "This is the loss 0.12543988227844238\n",
      "This is the loss 0.12124402821063995\n",
      "This is the loss 0.13745316863059998\n",
      "This is the loss 0.12355551868677139\n",
      "This is the loss 0.1272108256816864\n",
      "This is the loss 0.13099068403244019\n",
      "This is the loss 0.13621525466442108\n",
      "This is the loss 0.14169296622276306\n",
      "This is the loss 0.12977701425552368\n",
      "This is the loss 0.11645462363958359\n",
      "This is the loss 0.12583579123020172\n",
      "This is the loss 0.17016005516052246\n",
      "This is the loss 0.13727343082427979\n",
      "This is the loss 0.13302697241306305\n",
      "This is the loss 0.1356668621301651\n",
      "This is the loss 0.1112850233912468\n",
      "This is the loss 0.13434778153896332\n",
      "This is the loss 0.13870319724082947\n",
      "This is the loss 0.13568054139614105\n",
      "This is the loss 0.1474633663892746\n",
      "This is the loss 0.12144681066274643\n",
      "This is the loss 0.13955888152122498\n",
      "This is the loss 0.12938573956489563\n",
      "This is the loss 0.17755471169948578\n",
      "This is the loss 0.12243969738483429\n",
      "This is the loss 0.11965559422969818\n",
      "This is the loss 0.1483110785484314\n",
      "This is the loss 0.14691589772701263\n",
      "This is the loss 0.13111495971679688\n",
      "This is the loss 0.15182703733444214\n",
      "This is the loss 0.14282439649105072\n",
      "This is the loss 0.17119857668876648\n",
      "This is the loss 0.1275826245546341\n",
      "This is the loss 0.12076770514249802\n",
      "This is the loss 0.12983672320842743\n",
      "This is the loss 0.15343020856380463\n",
      "This is the loss 0.1400476098060608\n",
      "This is the loss 0.1308821588754654\n",
      "This is the loss 0.13732725381851196\n",
      "This is the loss 0.12862306833267212\n",
      "This is the loss 0.0950692743062973\n",
      "This is the loss 0.1351885050535202\n",
      "This is the loss 0.10647022724151611\n",
      "This is the loss 0.12140125036239624\n",
      "This is the loss 0.1292402446269989\n",
      "This is the loss 0.16414406895637512\n",
      "This is the loss 0.1467761993408203\n",
      "This is the loss 0.14994055032730103\n",
      "This is the loss 0.1395668238401413\n",
      "This is the loss 0.16787590086460114\n",
      "This is the loss 0.13787990808486938\n",
      "This is the loss 0.15438249707221985\n",
      "This is the loss 0.15580534934997559\n",
      "This is the loss 0.13689666986465454\n",
      "This is the loss 0.13561129570007324\n",
      "This is the loss 0.1423230767250061\n",
      "This is the loss 0.1442786455154419\n",
      "This is the loss 0.13636094331741333\n",
      "This is the loss 0.14459143579006195\n",
      "This is the loss 0.15510544180870056\n",
      "This is the loss 0.15393544733524323\n",
      "This is the loss 0.12146615982055664\n",
      "This is the loss 0.149324432015419\n",
      "This is the loss 0.1387299746274948\n",
      "This is the loss 0.12831921875476837\n",
      "This is the loss 0.1400207281112671\n",
      "This is the loss 0.11192774772644043\n",
      "This is the loss 0.15735334157943726\n",
      "This is the loss 0.12283120304346085\n",
      "This is the loss 0.1433330774307251\n",
      "This is the loss 0.13283036649227142\n",
      "This is the loss 0.11744267493486404\n",
      "This is the loss 0.14397934079170227\n",
      "This is the loss 0.1221664696931839\n",
      "This is the loss 0.12451362609863281\n",
      "This is the loss 0.13592153787612915\n",
      "This is the loss 0.16406744718551636\n",
      "This is the loss 0.13923116028308868\n",
      "This is the loss 0.12270605564117432\n",
      "This is the loss 0.15796993672847748\n",
      "This is the loss 0.1782914698123932\n",
      "This is the loss 0.14149993658065796\n",
      "This is the loss 0.13649222254753113\n",
      "This is the loss 0.15698155760765076\n",
      "This is the loss 0.15590831637382507\n",
      "This is the loss 0.11825983226299286\n",
      "This is the loss 0.1456059068441391\n",
      "This is the loss 0.11669231951236725\n",
      "This is the loss 0.13932503759860992\n",
      "This is the loss 0.14837560057640076\n",
      "This is the loss 0.1372498720884323\n",
      "This is the loss 0.12109453976154327\n",
      "This is the loss 0.14038462936878204\n",
      "This is the loss 0.11207057535648346\n",
      "This is the loss 0.12677618861198425\n",
      "This is the loss 0.08962654322385788\n",
      "This is the loss 0.1333201825618744\n",
      "This is the loss 0.11089539527893066\n",
      "This is the loss 0.1362217217683792\n",
      "This is the loss 0.10440702736377716\n",
      "This is the loss 0.12798084318637848\n",
      "This is the loss 0.13640975952148438\n",
      "This is the loss 0.10530604422092438\n",
      "This is the loss 0.1398973912000656\n",
      "This is the loss 0.16151313483715057\n",
      "This is the loss 0.14443650841712952\n",
      "This is the loss 0.1557258516550064\n",
      "This is the loss 0.1420023888349533\n",
      "This is the loss 0.1536477953195572\n",
      "This is the loss 0.16301372647285461\n",
      "This is the loss 0.14800198376178741\n",
      "This is the loss 0.16256986558437347\n",
      "This is the loss 0.14182418584823608\n",
      "This is the loss 0.1307169497013092\n",
      "This is the loss 0.14431846141815186\n",
      "This is the loss 0.12073582410812378\n",
      "This is the loss 0.14225676655769348\n",
      "This is the loss 0.11941179633140564\n",
      "This is the loss 0.11507013440132141\n",
      "This is the loss 0.16530321538448334\n",
      "This is the loss 0.14262327551841736\n",
      "This is the loss 0.12009641528129578\n",
      "This is the loss 0.14852960407733917\n",
      "This is the loss 0.14076285064220428\n",
      "This is the loss 0.14158958196640015\n",
      "This is the loss 0.12469020485877991\n",
      "This is the loss 0.13625939190387726\n",
      "This is the loss 0.13915163278579712\n",
      "This is the loss 0.13530083000659943\n",
      "This is the loss 0.1376083791255951\n",
      "This is the loss 0.13514173030853271\n",
      "This is the loss 0.14592428505420685\n",
      "This is the loss 0.1566198468208313\n",
      "This is the loss 0.12738683819770813\n",
      "This is the loss 0.12970653176307678\n",
      "This is the loss 0.1025085300207138\n",
      "This is the loss 0.1388338953256607\n",
      "This is the loss 0.13282515108585358\n",
      "This is the loss 0.1688733696937561\n",
      "This is the loss 0.14115971326828003\n",
      "This is the loss 0.1446029245853424\n",
      "This is the loss 0.12795576453208923\n",
      "This is the loss 0.11819539219141006\n",
      "This is the loss 0.11143416911363602\n",
      "This is the loss 0.1387699991464615\n",
      "This is the loss 0.14449192583560944\n",
      "This is the loss 0.15714281797409058\n",
      "This is the loss 0.1484338939189911\n",
      "This is the loss 0.14089131355285645\n",
      "This is the loss 0.13849663734436035\n",
      "This is the loss 0.14720690250396729\n",
      "This is the loss 0.13810478150844574\n",
      "This is the loss 0.13767775893211365\n",
      "This is the loss 0.11075182259082794\n",
      "This is the loss 0.15088224411010742\n",
      "This is the loss 0.1631423532962799\n",
      "This is the loss 0.13492169976234436\n",
      "This is the loss 0.16122066974639893\n",
      "This is the loss 0.13744626939296722\n",
      "This is the loss 0.10343974083662033\n",
      "This is the loss 0.14079125225543976\n",
      "This is the loss 0.1347484141588211\n",
      "This is the loss 0.16959042847156525\n",
      "This is the loss 0.10482687503099442\n",
      "This is the loss 0.18580245971679688\n",
      "This is the loss 0.1397497057914734\n",
      "This is the loss 0.15807630121707916\n",
      "This is the loss 0.15554927289485931\n",
      "This is the loss 0.12176074832677841\n",
      "This is the loss 0.12722815573215485\n",
      "This is the loss 0.12963885068893433\n",
      "This is the loss 0.11117693036794662\n",
      "This is the loss 0.13753092288970947\n",
      "This is the loss 0.13691547513008118\n",
      "This is the loss 0.11943408101797104\n",
      "This is the loss 0.12402216345071793\n",
      "This is the loss 0.14912691712379456\n",
      "This is the loss 0.13883930444717407\n",
      "This is the loss 0.1659265160560608\n",
      "This is the loss 0.1297149956226349\n",
      "This is the loss 0.09965420514345169\n",
      "This is the loss 0.12051557004451752\n",
      "This is the loss 0.11672615259885788\n",
      "This is the loss 0.12082080543041229\n",
      "This is the loss 0.1548687368631363\n",
      "This is the loss 0.1528150588274002\n",
      "This is the loss 0.12452609837055206\n",
      "This is the loss 0.1446714699268341\n",
      "This is the loss 0.14422288537025452\n",
      "This is the loss 0.14364945888519287\n",
      "This is the loss 0.11738487333059311\n",
      "This is the loss 0.12495964765548706\n",
      "This is the loss 0.1173200011253357\n",
      "This is the loss 0.10844666510820389\n",
      "This is the loss 0.17052465677261353\n",
      "This is the loss 0.12402516603469849\n",
      "This is the loss 0.1091754212975502\n",
      "This is the loss 0.13046656548976898\n",
      "This is the loss 0.12984910607337952\n",
      "This is the loss 0.16560545563697815\n",
      "This is the loss 0.11281561106443405\n",
      "This is the loss 0.10135558992624283\n",
      "This is the loss 0.15397031605243683\n",
      "This is the loss 0.1280764937400818\n",
      "This is the loss 0.14113695919513702\n",
      "This is the loss 0.15423229336738586\n",
      "This is the loss 0.13006921112537384\n",
      "This is the loss 0.1416272670030594\n",
      "This is the loss 0.13914203643798828\n",
      "This is the loss 0.12859736382961273\n",
      "This is the loss 0.19645380973815918\n",
      "This is the loss 0.16283762454986572\n",
      "This is the loss 0.12496422231197357\n",
      "This is the loss 0.143088236451149\n",
      "This is the loss 0.15127260982990265\n",
      "This is the loss 0.16171643137931824\n",
      "This is the loss 0.13991470634937286\n",
      "This is the loss 0.14974349737167358\n",
      "This is the loss 0.1324196308851242\n",
      "This is the loss 0.12161517888307571\n",
      "This is the loss 0.15040773153305054\n",
      "This is the loss 0.12882061302661896\n",
      "This is the loss 0.14040616154670715\n",
      "This is the loss 0.15087707340717316\n",
      "This is the loss 0.14051154255867004\n",
      "This is the loss 0.16784976422786713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.13623590767383575\n",
      "This is the loss 0.1297474354505539\n",
      "This is the loss 0.15262949466705322\n",
      "This is the loss 0.11806830763816833\n",
      "This is the loss 0.15036587417125702\n",
      "This is the loss 0.14904239773750305\n",
      "This is the loss 0.1215054988861084\n",
      "This is the loss 0.15548956394195557\n",
      "This is the loss 0.1563500463962555\n",
      "This is the loss 0.11417660862207413\n",
      "This is the loss 0.14102376997470856\n",
      "This is the loss 0.13377532362937927\n",
      "This is the loss 0.14299964904785156\n",
      "This is the loss 0.12485706806182861\n",
      "This is the loss 0.14642882347106934\n",
      "This is the loss 0.12493165582418442\n",
      "This is the loss 0.132239431142807\n",
      "This is the loss 0.1283019632101059\n",
      "This is the loss 0.13623803853988647\n",
      "This is the loss 0.10910751670598984\n",
      "This is the loss 0.13006997108459473\n",
      "This is the loss 0.1511823683977127\n",
      "This is the loss 0.12544095516204834\n",
      "This is the loss 0.13899946212768555\n",
      "This is the loss 0.1027107983827591\n",
      "This is the loss 0.1392897069454193\n",
      "This is the loss 0.1211308017373085\n",
      "This is the loss 0.11757238209247589\n",
      "This is the loss 0.14220711588859558\n",
      "This is the loss 0.12039865553379059\n",
      "This is the loss 0.13933102786540985\n",
      "This is the loss 0.13875612616539001\n",
      "This is the loss 0.14742225408554077\n",
      "This is the loss 0.16912667453289032\n",
      "This is the loss 0.1377883106470108\n",
      "This is the loss 0.13886457681655884\n",
      "This is the loss 0.1390455961227417\n",
      "This is the loss 0.1196020096540451\n",
      "This is the loss 0.13856172561645508\n",
      "This is the loss 0.14528518915176392\n",
      "This is the loss 0.1272915154695511\n",
      "This is the loss 0.11952920258045197\n",
      "This is the loss 0.08523674309253693\n",
      "This is the loss 0.13722273707389832\n",
      "This is the loss 0.12903104722499847\n",
      "This is the loss 0.15346534550189972\n",
      "This is the loss 0.11347752809524536\n",
      "This is the loss 0.14141102135181427\n",
      "This is the loss 0.14595173299312592\n",
      "This is the loss 0.10744217783212662\n",
      "This is the loss 0.12029244005680084\n",
      "This is the loss 0.1536019891500473\n",
      "This is the loss 0.15934917330741882\n",
      "This is the loss 0.16837546229362488\n",
      "This is the loss 0.14528195559978485\n",
      "This is the loss 0.14933371543884277\n",
      "This is the loss 0.08701997250318527\n",
      "This is the loss 0.15180224180221558\n",
      "This is the loss 0.09928690642118454\n",
      "This is the loss 0.12145870923995972\n",
      "This is the loss 0.13317140936851501\n",
      "This is the loss 0.12609165906906128\n",
      "This is the loss 0.1355505883693695\n",
      "This is the loss 0.13799181580543518\n",
      "This is the loss 0.14930316805839539\n",
      "This is the loss 0.12075395882129669\n",
      "This is the loss 0.1414642333984375\n",
      "This is the loss 0.13227804005146027\n",
      "This is the loss 0.13544528186321259\n",
      "This is the loss 0.16280823945999146\n",
      "This is the loss 0.16281849145889282\n",
      "This is the loss 0.13856381177902222\n",
      "This is the loss 0.16371846199035645\n",
      "This is the loss 0.15218962728977203\n",
      "This is the loss 0.1470395028591156\n",
      "This is the loss 0.14183717966079712\n",
      "This is the loss 0.1699901819229126\n",
      "This is the loss 0.1544591337442398\n",
      "This is the loss 0.17103053629398346\n",
      "This is the loss 0.13945482671260834\n",
      "This is the loss 0.14253626763820648\n",
      "This is the loss 0.14623640477657318\n",
      "This is the loss 0.1816709339618683\n",
      "This is the loss 0.15821810066699982\n",
      "This is the loss 0.17049196362495422\n",
      "This is the loss 0.1278219372034073\n",
      "This is the loss 0.10659437626600266\n",
      "This is the loss 0.11174841970205307\n",
      "This is the loss 0.14147315919399261\n",
      "This is the loss 0.14494507014751434\n",
      "This is the loss 0.14030224084854126\n",
      "This is the loss 0.11925962567329407\n",
      "This is the loss 0.10234694182872772\n",
      "This is the loss 0.14137445390224457\n",
      "This is the loss 0.1241668164730072\n",
      "This is the loss 0.12306443601846695\n",
      "This is the loss 0.12158511579036713\n",
      "This is the loss 0.12391100823879242\n",
      "This is the loss 0.11031594127416611\n",
      "This is the loss 0.14858120679855347\n",
      "This is the loss 0.11545930057764053\n",
      "This is the loss 0.16280265152454376\n",
      "This is the loss 0.13093578815460205\n",
      "This is the loss 0.13226193189620972\n",
      "This is the loss 0.11521423608064651\n",
      "This is the loss 0.12931959331035614\n",
      "This is the loss 0.16386738419532776\n",
      "This is the loss 0.1638210266828537\n",
      "This is the loss 0.15144576132297516\n",
      "This is the loss 0.11590428650379181\n",
      "This is the loss 0.1554003655910492\n",
      "This is the loss 0.1659293919801712\n",
      "This is the loss 0.14437180757522583\n",
      "This is the loss 0.1557360738515854\n",
      "This is the loss 0.11853337287902832\n",
      "This is the loss 0.1418343335390091\n",
      "This is the loss 0.11666029691696167\n",
      "This is the loss 0.1684255301952362\n",
      "This is the loss 0.16778284311294556\n",
      "This is the loss 0.15428398549556732\n",
      "This is the loss 0.13469071686267853\n",
      "This is the loss 0.13368970155715942\n",
      "This is the loss 0.12478165328502655\n",
      "This is the loss 0.138150155544281\n",
      "This is the loss 0.11852900683879852\n",
      "This is the loss 0.13824981451034546\n",
      "This is the loss 0.11656415462493896\n",
      "This is the loss 0.08210885524749756\n",
      "This is the loss 0.111885204911232\n",
      "This is the loss 0.1272895336151123\n",
      "This is the loss 0.15020589530467987\n",
      "This is the loss 0.12911757826805115\n",
      "This is the loss 0.12054102122783661\n",
      "This is the loss 0.13324443995952606\n",
      "This is the loss 0.13481222093105316\n",
      "This is the loss 0.13637742400169373\n",
      "This is the loss 0.13250839710235596\n",
      "This is the loss 0.13150814175605774\n",
      "This is the loss 0.1362173706293106\n",
      "This is the loss 0.138052836060524\n",
      "This is the loss 0.12270534038543701\n",
      "This is the loss 0.1491425335407257\n",
      "This is the loss 0.16095976531505585\n",
      "This is the loss 0.14922288060188293\n",
      "This is the loss 0.1438090205192566\n",
      "This is the loss 0.14397673308849335\n",
      "This is the loss 0.1501920223236084\n",
      "This is the loss 0.17125049233436584\n",
      "This is the loss 0.1692361831665039\n",
      "This is the loss 0.1430698186159134\n",
      "This is the loss 0.16618981957435608\n",
      "This is the loss 0.16524532437324524\n",
      "This is the loss 0.1491026133298874\n",
      "This is the loss 0.16531836986541748\n",
      "This is the loss 0.13828504085540771\n",
      "This is the loss 0.11472171545028687\n",
      "This is the loss 0.12926816940307617\n",
      "This is the loss 0.12112684547901154\n",
      "This is the loss 0.1604282110929489\n",
      "This is the loss 0.1600014865398407\n",
      "This is the loss 0.13868682086467743\n",
      "This is the loss 0.18780557811260223\n",
      "This is the loss 0.1308586299419403\n",
      "This is the loss 0.1660899966955185\n",
      "This is the loss 0.14156468212604523\n",
      "This is the loss 0.19293159246444702\n",
      "This is the loss 0.1497160792350769\n",
      "This is the loss 0.1356199085712433\n",
      "This is the loss 0.15019865334033966\n",
      "This is the loss 0.15828321874141693\n",
      "This is the loss 0.14180123805999756\n",
      "This is the loss 0.15135884284973145\n",
      "This is the loss 0.14428171515464783\n",
      "This is the loss 0.13694444298744202\n",
      "This is the loss 0.14680923521518707\n",
      "This is the loss 0.13016454875469208\n",
      "This is the loss 0.12733112275600433\n",
      "This is the loss 0.14890949428081512\n",
      "This is the loss 0.15529586374759674\n",
      "This is the loss 0.12515409290790558\n",
      "This is the loss 0.13989940285682678\n",
      "This is the loss 0.12082108855247498\n",
      "This is the loss 0.09996208548545837\n",
      "This is the loss 0.12235207855701447\n",
      "This is the loss 0.127473846077919\n",
      "This is the loss 0.14194703102111816\n",
      "This is the loss 0.12084980309009552\n",
      "This is the loss 0.15599602460861206\n",
      "This is the loss 0.13180747628211975\n",
      "This is the loss 0.12494245916604996\n",
      "This is the loss 0.1496978998184204\n",
      "This is the loss 0.11048652976751328\n",
      "This is the loss 0.11506408452987671\n",
      "This is the loss 0.13449063897132874\n",
      "This is the loss 0.12310872226953506\n",
      "This is the loss 0.15891820192337036\n",
      "This is the loss 0.1530255675315857\n",
      "This is the loss 0.15182918310165405\n",
      "This is the loss 0.14208416640758514\n",
      "This is the loss 0.12366528809070587\n",
      "This is the loss 0.13909217715263367\n",
      "This is the loss 0.13071660697460175\n",
      "This is the loss 0.12771391868591309\n",
      "This is the loss 0.15149715542793274\n",
      "This is the loss 0.15722742676734924\n",
      "This is the loss 0.15261530876159668\n",
      "This is the loss 0.11538742482662201\n",
      "This is the loss 0.12379249185323715\n",
      "This is the loss 0.13108310103416443\n",
      "This is the loss 0.15943065285682678\n",
      "This is the loss 0.1838943362236023\n",
      "This is the loss 0.1233728677034378\n",
      "This is the loss 0.12568044662475586\n",
      "This is the loss 0.1427345722913742\n",
      "This is the loss 0.1309213638305664\n",
      "This is the loss 0.12603670358657837\n",
      "This is the loss 0.12665916979312897\n",
      "This is the loss 0.16535624861717224\n",
      "This is the loss 0.1275584101676941\n",
      "This is the loss 0.14635306596755981\n",
      "This is the loss 0.11753737926483154\n",
      "This is the loss 0.11910706758499146\n",
      "This is the loss 0.10687339305877686\n",
      "This is the loss 0.1417740285396576\n",
      "This is the loss 0.14681828022003174\n",
      "This is the loss 0.15721920132637024\n",
      "This is the loss 0.1382683515548706\n",
      "This is the loss 0.12876076996326447\n",
      "This is the loss 0.12332265079021454\n",
      "This is the loss 0.12183308601379395\n",
      "This is the loss 0.10823416709899902\n",
      "This is the loss 0.13064426183700562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.12487205862998962\n",
      "This is the loss 0.13813984394073486\n",
      "This is the loss 0.100165955722332\n",
      "This is the loss 0.11899814754724503\n",
      "This is the loss 0.1090712919831276\n",
      "This is the loss 0.13591346144676208\n",
      "This is the loss 0.13475531339645386\n",
      "This is the loss 0.1374407857656479\n",
      "This is the loss 0.16604845225811005\n",
      "This is the loss 0.16403742134571075\n",
      "This is the loss 0.16220661997795105\n",
      "This is the loss 0.11719778180122375\n",
      "This is the loss 0.13269047439098358\n",
      "This is the loss 0.12096776068210602\n",
      "This is the loss 0.13204726576805115\n",
      "This is the loss 0.11795394867658615\n",
      "This is the loss 0.149387389421463\n",
      "This is the loss 0.1541561484336853\n",
      "This is the loss 0.1469164490699768\n",
      "This is the loss 0.11132096499204636\n",
      "This is the loss 0.13559512794017792\n",
      "This is the loss 0.1340072900056839\n",
      "This is the loss 0.13691645860671997\n",
      "This is the loss 0.16240106523036957\n",
      "This is the loss 0.14921021461486816\n",
      "This is the loss 0.13474011421203613\n",
      "This is the loss 0.1287051886320114\n",
      "This is the loss 0.12565459311008453\n",
      "This is the loss 0.13749919831752777\n",
      "This is the loss 0.16405433416366577\n",
      "This is the loss 0.15129929780960083\n",
      "This is the loss 0.16795533895492554\n",
      "This is the loss 0.17797592282295227\n",
      "This is the loss 0.12025325000286102\n",
      "This is the loss 0.16010341048240662\n",
      "This is the loss 0.11723775416612625\n",
      "This is the loss 0.15258632600307465\n",
      "This is the loss 0.13421672582626343\n",
      "This is the loss 0.1256323754787445\n",
      "This is the loss 0.1496938169002533\n",
      "This is the loss 0.12316745519638062\n",
      "This is the loss 0.16002602875232697\n",
      "This is the loss 0.13023798167705536\n",
      "This is the loss 0.1229715496301651\n",
      "This is the loss 0.15661466121673584\n",
      "This is the loss 0.11070477962493896\n",
      "This is the loss 0.12727877497673035\n",
      "This is the loss 0.11342285573482513\n",
      "This is the loss 0.12750205397605896\n",
      "This is the loss 0.15837305784225464\n",
      "This is the loss 0.12878704071044922\n",
      "This is the loss 0.13428471982479095\n",
      "This is the loss 0.12897351384162903\n",
      "This is the loss 0.09755980968475342\n",
      "This is the loss 0.12602031230926514\n",
      "This is the loss 0.11536586284637451\n",
      "This is the loss 0.10336065292358398\n",
      "This is the loss 0.14230182766914368\n",
      "This is the loss 0.12615416944026947\n",
      "This is the loss 0.09787601977586746\n",
      "This is the loss 0.1235920786857605\n",
      "This is the loss 0.13987673819065094\n",
      "This is the loss 0.14679935574531555\n",
      "This is the loss 0.12158148735761642\n",
      "This is the loss 0.11636537313461304\n",
      "This is the loss 0.12970298528671265\n",
      "This is the loss 0.11921270936727524\n",
      "This is the loss 0.12056823074817657\n",
      "This is the loss 0.156820610165596\n",
      "This is the loss 0.16271106898784637\n",
      "This is the loss 0.13036467134952545\n",
      "This is the loss 0.1518622785806656\n",
      "This is the loss 0.13375282287597656\n",
      "This is the loss 0.14165395498275757\n",
      "This is the loss 0.13669496774673462\n",
      "This is the loss 0.12628519535064697\n",
      "This is the loss 0.1670025885105133\n",
      "This is the loss 0.12569665908813477\n",
      "This is the loss 0.1080937311053276\n",
      "This is the loss 0.14160838723182678\n",
      "This is the loss 0.13640108704566956\n",
      "This is the loss 0.15660852193832397\n",
      "This is the loss 0.16269442439079285\n",
      "This is the loss 0.15233805775642395\n",
      "This is the loss 0.12851634621620178\n",
      "This is the loss 0.15178018808364868\n",
      "This is the loss 0.1434565782546997\n",
      "This is the loss 0.14896105229854584\n",
      "This is the loss 0.17032811045646667\n",
      "This is the loss 0.14197352528572083\n",
      "This is the loss 0.13733285665512085\n",
      "This is the loss 0.14429476857185364\n",
      "This is the loss 0.1375177949666977\n",
      "This is the loss 0.16920366883277893\n",
      "This is the loss 0.13453568518161774\n",
      "This is the loss 0.1465344876050949\n",
      "This is the loss 0.12627023458480835\n",
      "This is the loss 0.149875670671463\n",
      "This is the loss 0.14622145891189575\n",
      "This is the loss 0.14523595571517944\n",
      "This is the loss 0.13749273121356964\n",
      "This is the loss 0.16149470210075378\n",
      "This is the loss 0.12153978645801544\n",
      "This is the loss 0.15104714035987854\n",
      "This is the loss 0.15731331706047058\n",
      "This is the loss 0.1551198661327362\n",
      "This is the loss 0.14490050077438354\n",
      "This is the loss 0.14165830612182617\n",
      "This is the loss 0.14337274432182312\n",
      "This is the loss 0.1623401939868927\n",
      "This is the loss 0.13645261526107788\n",
      "This is the loss 0.12693551182746887\n",
      "This is the loss 0.15015816688537598\n",
      "This is the loss 0.17119699716567993\n",
      "This is the loss 0.16290493309497833\n",
      "This is the loss 0.1419811248779297\n",
      "This is the loss 0.14692774415016174\n",
      "This is the loss 0.1264987587928772\n",
      "This is the loss 0.1500047743320465\n",
      "This is the loss 0.13984426856040955\n",
      "This is the loss 0.17649956047534943\n",
      "This is the loss 0.1835542619228363\n",
      "This is the loss 0.14981916546821594\n",
      "This is the loss 0.14019513130187988\n",
      "This is the loss 0.13366718590259552\n",
      "This is the loss 0.11119674146175385\n",
      "This is the loss 0.16738082468509674\n",
      "This is the loss 0.1304255723953247\n",
      "This is the loss 0.15127961337566376\n",
      "This is the loss 0.1933455914258957\n",
      "This is the loss 0.13565467298030853\n",
      "This is the loss 0.16693513095378876\n",
      "This is the loss 0.12535832822322845\n",
      "This is the loss 0.11695516854524612\n",
      "This is the loss 0.17211036384105682\n",
      "This is the loss 0.1396019458770752\n",
      "This is the loss 0.1692110002040863\n",
      "This is the loss 0.1609896868467331\n",
      "This is the loss 0.15362229943275452\n",
      "This is the loss 0.15641838312149048\n",
      "This is the loss 0.16838720440864563\n",
      "This is the loss 0.15259408950805664\n",
      "This is the loss 0.12328562885522842\n",
      "This is the loss 0.1880386918783188\n",
      "This is the loss 0.1508142054080963\n",
      "This is the loss 0.1355566531419754\n",
      "This is the loss 0.13693276047706604\n",
      "This is the loss 0.14429640769958496\n",
      "This is the loss 0.15799641609191895\n",
      "This is the loss 0.151809960603714\n",
      "This is the loss 0.13576830923557281\n",
      "This is the loss 0.1624184101819992\n",
      "This is the loss 0.13837777078151703\n",
      "This is the loss 0.12413515895605087\n",
      "This is the loss 0.13627780973911285\n",
      "This is the loss 0.0969381034374237\n",
      "This is the loss 0.1375608593225479\n",
      "This is the loss 0.13993969559669495\n",
      "This is the loss 0.1636175811290741\n",
      "This is the loss 0.14265748858451843\n",
      "This is the loss 0.1476285457611084\n",
      "This is the loss 0.15603171288967133\n",
      "This is the loss 0.16271506249904633\n",
      "This is the loss 0.17451253533363342\n",
      "This is the loss 0.15586400032043457\n",
      "This is the loss 0.16099399328231812\n",
      "This is the loss 0.16388680040836334\n",
      "This is the loss 0.1820445954799652\n",
      "This is the loss 0.12076988816261292\n",
      "This is the loss 0.145270437002182\n",
      "This is the loss 0.15541845560073853\n",
      "This is the loss 0.18229827284812927\n",
      "This is the loss 0.152238667011261\n",
      "This is the loss 0.13968361914157867\n",
      "This is the loss 0.1864110231399536\n",
      "This is the loss 0.180130735039711\n",
      "This is the loss 0.1267293244600296\n",
      "This is the loss 0.1604653298854828\n",
      "This is the loss 0.15348492562770844\n",
      "This is the loss 0.14704309403896332\n",
      "This is the loss 0.1423097848892212\n",
      "This is the loss 0.14807532727718353\n",
      "This is the loss 0.12840192019939423\n",
      "This is the loss 0.15298514068126678\n",
      "This is the loss 0.12113295495510101\n",
      "This is the loss 0.13204193115234375\n",
      "This is the loss 0.13294897973537445\n",
      "This is the loss 0.1585015058517456\n",
      "This is the loss 0.1731327921152115\n",
      "This is the loss 0.1720200628042221\n",
      "This is the loss 0.14066073298454285\n",
      "This is the loss 0.13156285881996155\n",
      "This is the loss 0.12495121359825134\n",
      "This is the loss 0.11858241260051727\n",
      "This is the loss 0.12287785857915878\n",
      "This is the loss 0.11853263527154922\n",
      "This is the loss 0.155442014336586\n",
      "This is the loss 0.140273317694664\n",
      "This is the loss 0.16273187100887299\n",
      "This is the loss 0.13825055956840515\n",
      "This is the loss 0.11558423936367035\n",
      "This is the loss 0.13514597713947296\n",
      "This is the loss 0.12255798280239105\n",
      "This is the loss 0.14110518991947174\n",
      "This is the loss 0.1468682885169983\n",
      "This is the loss 0.1597745716571808\n",
      "This is the loss 0.15247921645641327\n",
      "This is the loss 0.1401316225528717\n",
      "This is the loss 0.15017342567443848\n",
      "This is the loss 0.14391522109508514\n",
      "This is the loss 0.14802376925945282\n",
      "This is the loss 0.1518293023109436\n",
      "This is the loss 0.11812074482440948\n",
      "This is the loss 0.15866100788116455\n",
      "This is the loss 0.13396196067333221\n",
      "This is the loss 0.12616312503814697\n",
      "This is the loss 0.13732179999351501\n",
      "This is the loss 0.14139169454574585\n",
      "This is the loss 0.16048595309257507\n",
      "This is the loss 0.12994231283664703\n",
      "This is the loss 0.14439646899700165\n",
      "This is the loss 0.13454031944274902\n",
      "This is the loss 0.12402653694152832\n",
      "This is the loss 0.15782855451107025\n",
      "This is the loss 0.13867473602294922\n",
      "This is the loss 0.14665719866752625\n",
      "This is the loss 0.1201186403632164\n",
      "This is the loss 0.11332794278860092\n",
      "This is the loss 0.1615939438343048\n",
      "This is the loss 0.14397472143173218\n",
      "This is the loss 0.11380141973495483\n",
      "This is the loss 0.13489392399787903\n",
      "This is the loss 0.14507606625556946\n",
      "This is the loss 0.12015871703624725\n",
      "This is the loss 0.15030400454998016\n",
      "This is the loss 0.10350152105093002\n",
      "This is the loss 0.15270857512950897\n",
      "This is the loss 0.14805220067501068\n",
      "This is the loss 0.13157717883586884\n",
      "This is the loss 0.13217413425445557\n",
      "This is the loss 0.12388357520103455\n",
      "This is the loss 0.16282545030117035\n",
      "This is the loss 0.1227436289191246\n",
      "This is the loss 0.15981252491474152\n",
      "This is the loss 0.1225123405456543\n",
      "This is the loss 0.1292113959789276\n",
      "This is the loss 0.15965232253074646\n",
      "This is the loss 0.14338307082653046\n",
      "This is the loss 0.13072317838668823\n",
      "This is the loss 0.13653820753097534\n",
      "This is the loss 0.13827547430992126\n",
      "This is the loss 0.12899154424667358\n",
      "This is the loss 0.16201092302799225\n",
      "This is the loss 0.11955469101667404\n",
      "This is the loss 0.1660871058702469\n",
      "This is the loss 0.12984080612659454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.11656336486339569\n",
      "This is the loss 0.13125233352184296\n",
      "This is the loss 0.14659324288368225\n",
      "This is the loss 0.13447219133377075\n",
      "This is the loss 0.16562683880329132\n",
      "This is the loss 0.14059995114803314\n",
      "This is the loss 0.12221517413854599\n",
      "This is the loss 0.1233254224061966\n",
      "This is the loss 0.16060681641101837\n",
      "This is the loss 0.14300334453582764\n",
      "This is the loss 0.12097866833209991\n",
      "This is the loss 0.12477134168148041\n",
      "This is the loss 0.15482847392559052\n",
      "This is the loss 0.15520311892032623\n",
      "This is the loss 0.13378116488456726\n",
      "This is the loss 0.13845956325531006\n",
      "This is the loss 0.12716640532016754\n",
      "This is the loss 0.1453220397233963\n",
      "This is the loss 0.15772701799869537\n",
      "This is the loss 0.11250299960374832\n",
      "This is the loss 0.10922349244356155\n",
      "This is the loss 0.13624542951583862\n",
      "This is the loss 0.17590072751045227\n",
      "This is the loss 0.14218758046627045\n",
      "This is the loss 0.1652420461177826\n",
      "This is the loss 0.17688822746276855\n",
      "This is the loss 0.17799918353557587\n",
      "This is the loss 0.14455914497375488\n",
      "This is the loss 0.14582020044326782\n",
      "This is the loss 0.13157933950424194\n",
      "This is the loss 0.12328515201807022\n",
      "This is the loss 0.12058083713054657\n",
      "This is the loss 0.13830244541168213\n",
      "This is the loss 0.12892501056194305\n",
      "This is the loss 0.1341010183095932\n",
      "This is the loss 0.12784573435783386\n",
      "This is the loss 0.12298847734928131\n",
      "This is the loss 0.1515006721019745\n",
      "This is the loss 0.14589591324329376\n",
      "This is the loss 0.1694847047328949\n",
      "This is the loss 0.11756348609924316\n",
      "This is the loss 0.1306932121515274\n",
      "This is the loss 0.1512378603219986\n",
      "This is the loss 0.11639479547739029\n",
      "This is the loss 0.142221137881279\n",
      "This is the loss 0.15172603726387024\n",
      "This is the loss 0.12200601398944855\n",
      "This is the loss 0.1581525057554245\n",
      "This is the loss 0.1160060241818428\n",
      "This is the loss 0.14863687753677368\n",
      "This is the loss 0.12660156190395355\n",
      "This is the loss 0.15353941917419434\n",
      "This is the loss 0.125751331448555\n",
      "This is the loss 0.1506441980600357\n",
      "This is the loss 0.11976130306720734\n",
      "This is the loss 0.1296394020318985\n",
      "This is the loss 0.14720258116722107\n",
      "This is the loss 0.1281120926141739\n",
      "This is the loss 0.13433843851089478\n",
      "This is the loss 0.09831876307725906\n",
      "This is the loss 0.14082738757133484\n",
      "This is the loss 0.13155174255371094\n",
      "This is the loss 0.1150762215256691\n",
      "This is the loss 0.14245444536209106\n",
      "This is the loss 0.15198513865470886\n",
      "This is the loss 0.1470092236995697\n",
      "This is the loss 0.1393619179725647\n",
      "This is the loss 0.16189514100551605\n",
      "This is the loss 0.10061395913362503\n",
      "This is the loss 0.1694260537624359\n",
      "This is the loss 0.11255344748497009\n",
      "This is the loss 0.1352851390838623\n",
      "This is the loss 0.13366448879241943\n",
      "This is the loss 0.13607102632522583\n",
      "This is the loss 0.13469639420509338\n",
      "This is the loss 0.12501944601535797\n",
      "This is the loss 0.11917084455490112\n",
      "This is the loss 0.13715867698192596\n",
      "This is the loss 0.15131162106990814\n",
      "This is the loss 0.1477046012878418\n",
      "This is the loss 0.1269417256116867\n",
      "This is the loss 0.12462639808654785\n",
      "This is the loss 0.11331343650817871\n",
      "This is the loss 0.14914372563362122\n",
      "This is the loss 0.16110171377658844\n",
      "This is the loss 0.12609781324863434\n",
      "This is the loss 0.12020218372344971\n",
      "This is the loss 0.13901285827159882\n",
      "This is the loss 0.14860594272613525\n",
      "This is the loss 0.13329386711120605\n",
      "This is the loss 0.14459246397018433\n",
      "This is the loss 0.15073566138744354\n",
      "This is the loss 0.17032884061336517\n",
      "This is the loss 0.16359420120716095\n",
      "This is the loss 0.15389153361320496\n",
      "This is the loss 0.13273324072360992\n",
      "This is the loss 0.16060733795166016\n",
      "This is the loss 0.15020334720611572\n",
      "This is the loss 0.12825293838977814\n",
      "This is the loss 0.1307545304298401\n",
      "This is the loss 0.14730125665664673\n",
      "This is the loss 0.1471911072731018\n",
      "This is the loss 0.14019103348255157\n",
      "This is the loss 0.14894166588783264\n",
      "This is the loss 0.14117316901683807\n",
      "This is the loss 0.12755902111530304\n",
      "This is the loss 0.1400938630104065\n",
      "This is the loss 0.14174064993858337\n",
      "This is the loss 0.1338430941104889\n",
      "This is the loss 0.15288981795310974\n",
      "This is the loss 0.12279416620731354\n",
      "This is the loss 0.12399529665708542\n",
      "This is the loss 0.1615796536207199\n",
      "This is the loss 0.1467784196138382\n",
      "This is the loss 0.16157187521457672\n",
      "This is the loss 0.11226045340299606\n",
      "This is the loss 0.1074460968375206\n",
      "This is the loss 0.14368006587028503\n",
      "This is the loss 0.11113978177309036\n",
      "This is the loss 0.11423468589782715\n",
      "This is the loss 0.1431347280740738\n",
      "This is the loss 0.13569897413253784\n",
      "This is the loss 0.13186559081077576\n",
      "This is the loss 0.12984944880008698\n",
      "This is the loss 0.16194377839565277\n",
      "This is the loss 0.15219746530056\n",
      "This is the loss 0.13949893414974213\n",
      "This is the loss 0.13226623833179474\n",
      "This is the loss 0.14414538443088531\n",
      "This is the loss 0.14007508754730225\n",
      "This is the loss 0.1517709195613861\n",
      "This is the loss 0.1150599867105484\n",
      "This is the loss 0.15529006719589233\n",
      "This is the loss 0.1642370969057083\n",
      "This is the loss 0.15633362531661987\n",
      "This is the loss 0.10795827209949493\n",
      "This is the loss 0.13428495824337006\n",
      "This is the loss 0.08919163048267365\n",
      "This is the loss 0.14416925609111786\n",
      "This is the loss 0.13359279930591583\n",
      "This is the loss 0.17373082041740417\n",
      "This is the loss 0.10828959941864014\n",
      "This is the loss 0.11487609148025513\n",
      "This is the loss 0.1300823986530304\n",
      "This is the loss 0.1263442039489746\n",
      "This is the loss 0.1599123328924179\n",
      "This is the loss 0.13231205940246582\n",
      "This is the loss 0.11528727412223816\n",
      "This is the loss 0.16219563782215118\n",
      "This is the loss 0.13033892214298248\n",
      "This is the loss 0.1569504737854004\n",
      "This is the loss 0.11905446648597717\n",
      "This is the loss 0.1453874409198761\n",
      "This is the loss 0.1418517529964447\n",
      "This is the loss 0.13056841492652893\n",
      "This is the loss 0.15456929802894592\n",
      "This is the loss 0.1276758313179016\n",
      "This is the loss 0.15658555924892426\n",
      "This is the loss 0.1368670016527176\n",
      "This is the loss 0.11426196992397308\n",
      "This is the loss 0.12985560297966003\n",
      "This is the loss 0.10496201366186142\n",
      "This is the loss 0.1111837774515152\n",
      "This is the loss 0.15624076128005981\n",
      "This is the loss 0.1323969066143036\n",
      "This is the loss 0.1351539045572281\n",
      "This is the loss 0.11426404118537903\n",
      "This is the loss 0.12088975310325623\n",
      "This is the loss 0.144192636013031\n",
      "This is the loss 0.1357830911874771\n",
      "This is the loss 0.13090835511684418\n",
      "This is the loss 0.1716754138469696\n",
      "This is the loss 0.15017826855182648\n",
      "This is the loss 0.13240492343902588\n",
      "This is the loss 0.11046809703111649\n",
      "This is the loss 0.13616280257701874\n",
      "This is the loss 0.14224126935005188\n",
      "This is the loss 0.141880065202713\n",
      "This is the loss 0.11229962110519409\n",
      "This is the loss 0.14435473084449768\n",
      "This is the loss 0.1281653195619583\n",
      "This is the loss 0.11628490686416626\n",
      "This is the loss 0.1284216344356537\n",
      "This is the loss 0.12829208374023438\n",
      "This is the loss 0.1343407779932022\n",
      "This is the loss 0.13285672664642334\n",
      "This is the loss 0.12896376848220825\n",
      "This is the loss 0.16821959614753723\n",
      "This is the loss 0.14983299374580383\n",
      "This is the loss 0.15095597505569458\n",
      "This is the loss 0.14465609192848206\n",
      "This is the loss 0.12014361470937729\n",
      "This is the loss 0.14328669011592865\n",
      "This is the loss 0.1237308531999588\n",
      "This is the loss 0.13991881906986237\n",
      "This is the loss 0.15323270857334137\n",
      "This is the loss 0.1204688549041748\n",
      "This is the loss 0.14631058275699615\n",
      "This is the loss 0.14192937314510345\n",
      "This is the loss 0.13740453124046326\n",
      "This is the loss 0.14627671241760254\n",
      "This is the loss 0.15089872479438782\n",
      "This is the loss 0.15560321509838104\n",
      "This is the loss 0.14040659368038177\n",
      "This is the loss 0.1460006684064865\n",
      "This is the loss 0.1417766511440277\n",
      "This is the loss 0.14506857097148895\n",
      "This is the loss 0.12462383508682251\n",
      "This is the loss 0.14964604377746582\n",
      "This is the loss 0.15751415491104126\n",
      "This is the loss 0.10683999955654144\n",
      "This is the loss 0.13013222813606262\n",
      "This is the loss 0.12033215910196304\n",
      "This is the loss 0.14955227077007294\n",
      "This is the loss 0.1377166360616684\n",
      "This is the loss 0.13536646962165833\n",
      "This is the loss 0.13002239167690277\n",
      "This is the loss 0.11579062789678574\n",
      "This is the loss 0.14658595621585846\n",
      "This is the loss 0.11782129108905792\n",
      "This is the loss 0.11505473405122757\n",
      "This is the loss 0.11226578056812286\n",
      "This is the loss 0.12336283922195435\n",
      "This is the loss 0.1268601417541504\n",
      "This is the loss 0.10993554443120956\n",
      "This is the loss 0.14666177332401276\n",
      "This is the loss 0.11941739916801453\n",
      "This is the loss 0.1400209218263626\n",
      "This is the loss 0.12605944275856018\n",
      "This is the loss 0.11611863970756531\n",
      "This is the loss 0.12061083316802979\n",
      "This is the loss 0.16578315198421478\n",
      "This is the loss 0.1314341425895691\n",
      "This is the loss 0.16817492246627808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.15244363248348236\n",
      "This is the loss 0.16572530567646027\n",
      "This is the loss 0.11705487966537476\n",
      "This is the loss 0.12709619104862213\n",
      "This is the loss 0.09038057923316956\n",
      "This is the loss 0.12051443010568619\n",
      "This is the loss 0.16067242622375488\n",
      "This is the loss 0.1062152162194252\n",
      "This is the loss 0.14734405279159546\n",
      "This is the loss 0.12017934024333954\n",
      "This is the loss 0.12493324279785156\n",
      "This is the loss 0.1413797289133072\n",
      "This is the loss 0.14856210350990295\n",
      "This is the loss 0.12116729468107224\n",
      "This is the loss 0.14116251468658447\n",
      "This is the loss 0.15270715951919556\n",
      "This is the loss 0.1316647231578827\n",
      "This is the loss 0.12246489524841309\n",
      "This is the loss 0.1194533258676529\n",
      "This is the loss 0.13720735907554626\n",
      "This is the loss 0.1403830647468567\n",
      "This is the loss 0.14267316460609436\n",
      "This is the loss 0.17723669111728668\n",
      "This is the loss 0.1417754739522934\n",
      "This is the loss 0.1434919238090515\n",
      "This is the loss 0.16192544996738434\n",
      "This is the loss 0.12974242866039276\n",
      "This is the loss 0.16143208742141724\n",
      "This is the loss 0.13795961439609528\n",
      "This is the loss 0.15157192945480347\n",
      "This is the loss 0.13511043787002563\n",
      "This is the loss 0.18375059962272644\n",
      "This is the loss 0.14997203648090363\n",
      "This is the loss 0.13642039895057678\n",
      "This is the loss 0.12385660409927368\n",
      "This is the loss 0.14292895793914795\n",
      "This is the loss 0.12650762498378754\n",
      "This is the loss 0.11667875945568085\n",
      "This is the loss 0.12921157479286194\n",
      "This is the loss 0.11169549822807312\n",
      "This is the loss 0.1395314335823059\n",
      "This is the loss 0.14703521132469177\n",
      "This is the loss 0.15741080045700073\n",
      "This is the loss 0.14821068942546844\n",
      "This is the loss 0.11656606197357178\n",
      "This is the loss 0.14891870319843292\n",
      "This is the loss 0.12952420115470886\n",
      "This is the loss 0.1341094970703125\n",
      "This is the loss 0.12690486013889313\n",
      "This is the loss 0.1297202706336975\n",
      "This is the loss 0.13546501100063324\n",
      "This is the loss 0.10698983818292618\n",
      "This is the loss 0.12200072407722473\n",
      "This is the loss 0.1428879201412201\n",
      "This is the loss 0.14262637495994568\n",
      "This is the loss 0.10393893718719482\n",
      "This is the loss 0.14365820586681366\n",
      "This is the loss 0.11516733467578888\n",
      "This is the loss 0.13291001319885254\n",
      "This is the loss 0.1430099606513977\n",
      "This is the loss 0.15226995944976807\n",
      "This is the loss 0.15406137704849243\n",
      "This is the loss 0.13155904412269592\n",
      "This is the loss 0.15648122131824493\n",
      "This is the loss 0.15924856066703796\n",
      "This is the loss 0.11955935508012772\n",
      "This is the loss 0.10381868481636047\n",
      "This is the loss 0.11726410686969757\n",
      "This is the loss 0.12443171441555023\n",
      "This is the loss 0.12061136215925217\n",
      "This is the loss 0.13743863999843597\n",
      "This is the loss 0.12362837791442871\n",
      "This is the loss 0.12746721506118774\n",
      "This is the loss 0.13176622986793518\n",
      "This is the loss 0.13580504059791565\n",
      "This is the loss 0.1415872722864151\n",
      "This is the loss 0.12941206991672516\n",
      "This is the loss 0.11670607328414917\n",
      "This is the loss 0.12502890825271606\n",
      "This is the loss 0.16978691518306732\n",
      "This is the loss 0.13748931884765625\n",
      "This is the loss 0.13297003507614136\n",
      "This is the loss 0.1355900764465332\n",
      "This is the loss 0.11108449101448059\n",
      "This is the loss 0.13427698612213135\n",
      "This is the loss 0.1386498212814331\n",
      "This is the loss 0.1354972869157791\n",
      "This is the loss 0.14735358953475952\n",
      "This is the loss 0.12092570960521698\n",
      "This is the loss 0.13950783014297485\n",
      "This is the loss 0.12900787591934204\n",
      "This is the loss 0.17641904950141907\n",
      "This is the loss 0.12260621041059494\n",
      "This is the loss 0.11983969062566757\n",
      "This is the loss 0.14796173572540283\n",
      "This is the loss 0.14688800275325775\n",
      "This is the loss 0.13025270402431488\n",
      "This is the loss 0.15221837162971497\n",
      "This is the loss 0.14276592433452606\n",
      "This is the loss 0.1707756668329239\n",
      "This is the loss 0.12745323777198792\n",
      "This is the loss 0.12037387490272522\n",
      "This is the loss 0.1295984387397766\n",
      "This is the loss 0.15276873111724854\n",
      "This is the loss 0.13959987461566925\n",
      "This is the loss 0.13029621541500092\n",
      "This is the loss 0.1370430886745453\n",
      "This is the loss 0.12813740968704224\n",
      "This is the loss 0.09502310305833817\n",
      "This is the loss 0.13501399755477905\n",
      "This is the loss 0.10578405112028122\n",
      "This is the loss 0.12097520381212234\n",
      "This is the loss 0.12936708331108093\n",
      "This is the loss 0.16396982967853546\n",
      "This is the loss 0.14643076062202454\n",
      "This is the loss 0.14991438388824463\n",
      "This is the loss 0.13932418823242188\n",
      "This is the loss 0.16677282750606537\n",
      "This is the loss 0.13767829537391663\n",
      "This is the loss 0.15433906018733978\n",
      "This is the loss 0.15591725707054138\n",
      "This is the loss 0.13712337613105774\n",
      "This is the loss 0.1356068253517151\n",
      "This is the loss 0.14206945896148682\n",
      "This is the loss 0.14414341747760773\n",
      "This is the loss 0.13651369512081146\n",
      "This is the loss 0.1447233408689499\n",
      "This is the loss 0.15492217242717743\n",
      "This is the loss 0.15387432277202606\n",
      "This is the loss 0.12103620171546936\n",
      "This is the loss 0.14903780817985535\n",
      "This is the loss 0.13877123594284058\n",
      "This is the loss 0.1280427873134613\n",
      "This is the loss 0.14011207222938538\n",
      "This is the loss 0.11170046776533127\n",
      "This is the loss 0.1572357714176178\n",
      "This is the loss 0.12312719970941544\n",
      "This is the loss 0.14315666258335114\n",
      "This is the loss 0.1328900158405304\n",
      "This is the loss 0.11783997714519501\n",
      "This is the loss 0.14390629529953003\n",
      "This is the loss 0.12182082235813141\n",
      "This is the loss 0.1241665929555893\n",
      "This is the loss 0.1360846608877182\n",
      "This is the loss 0.1636841893196106\n",
      "This is the loss 0.13961000740528107\n",
      "This is the loss 0.12223388254642487\n",
      "This is the loss 0.15817855298519135\n",
      "This is the loss 0.17822599411010742\n",
      "This is the loss 0.14088094234466553\n",
      "This is the loss 0.13663199543952942\n",
      "This is the loss 0.15724104642868042\n",
      "This is the loss 0.15589766204357147\n",
      "This is the loss 0.11842700839042664\n",
      "This is the loss 0.14524845778942108\n",
      "This is the loss 0.11719828844070435\n",
      "This is the loss 0.13877978920936584\n",
      "This is the loss 0.14862298965454102\n",
      "This is the loss 0.13637414574623108\n",
      "This is the loss 0.12101400643587112\n",
      "This is the loss 0.14063628017902374\n",
      "This is the loss 0.11227303743362427\n",
      "This is the loss 0.1267741322517395\n",
      "This is the loss 0.09000155329704285\n",
      "This is the loss 0.13242946565151215\n",
      "This is the loss 0.11105293780565262\n",
      "This is the loss 0.13624778389930725\n",
      "This is the loss 0.10426713526248932\n",
      "This is the loss 0.12783612310886383\n",
      "This is the loss 0.13618530333042145\n",
      "This is the loss 0.10520274937152863\n",
      "This is the loss 0.14036034047603607\n",
      "This is the loss 0.1620701551437378\n",
      "This is the loss 0.14427153766155243\n",
      "This is the loss 0.1554678976535797\n",
      "This is the loss 0.14107759296894073\n",
      "This is the loss 0.15301141142845154\n",
      "This is the loss 0.16279909014701843\n",
      "This is the loss 0.1481696367263794\n",
      "This is the loss 0.1621195673942566\n",
      "This is the loss 0.14102403819561005\n",
      "This is the loss 0.13133761286735535\n",
      "This is the loss 0.14456216990947723\n",
      "This is the loss 0.1207038164138794\n",
      "This is the loss 0.1417878419160843\n",
      "This is the loss 0.1191217452287674\n",
      "This is the loss 0.11523424088954926\n",
      "This is the loss 0.165155827999115\n",
      "This is the loss 0.14304403960704803\n",
      "This is the loss 0.11995665729045868\n",
      "This is the loss 0.1477072536945343\n",
      "This is the loss 0.14024607837200165\n",
      "This is the loss 0.14139704406261444\n",
      "This is the loss 0.12407444417476654\n",
      "This is the loss 0.13698308169841766\n",
      "This is the loss 0.13948169350624084\n",
      "This is the loss 0.13588443398475647\n",
      "This is the loss 0.13798527419567108\n",
      "This is the loss 0.13560760021209717\n",
      "This is the loss 0.14537149667739868\n",
      "This is the loss 0.1562708169221878\n",
      "This is the loss 0.1269075572490692\n",
      "This is the loss 0.1295813024044037\n",
      "This is the loss 0.10226363688707352\n",
      "This is the loss 0.1390150934457779\n",
      "This is the loss 0.13288167119026184\n",
      "This is the loss 0.16837754845619202\n",
      "This is the loss 0.14022007584571838\n",
      "This is the loss 0.14442883431911469\n",
      "This is the loss 0.1277710199356079\n",
      "This is the loss 0.11809603124856949\n",
      "This is the loss 0.11107461154460907\n",
      "This is the loss 0.13896583020687103\n",
      "This is the loss 0.1443028748035431\n",
      "This is the loss 0.15707150101661682\n",
      "This is the loss 0.1482287347316742\n",
      "This is the loss 0.14014184474945068\n",
      "This is the loss 0.13919697701931\n",
      "This is the loss 0.14672166109085083\n",
      "This is the loss 0.13773970305919647\n",
      "This is the loss 0.13800624012947083\n",
      "This is the loss 0.11028967797756195\n",
      "This is the loss 0.1510676145553589\n",
      "This is the loss 0.16321387887001038\n",
      "This is the loss 0.13458187878131866\n",
      "This is the loss 0.1614544689655304\n",
      "This is the loss 0.13728272914886475\n",
      "This is the loss 0.10374543815851212\n",
      "This is the loss 0.1407022327184677\n",
      "This is the loss 0.13461141288280487\n",
      "This is the loss 0.16953244805335999\n",
      "This is the loss 0.10492757707834244\n",
      "This is the loss 0.18496820330619812\n",
      "This is the loss 0.1390897035598755\n",
      "This is the loss 0.15809638798236847\n",
      "This is the loss 0.15600106120109558\n",
      "This is the loss 0.12141939997673035\n",
      "This is the loss 0.127186119556427\n",
      "This is the loss 0.12908753752708435\n",
      "This is the loss 0.11183542758226395\n",
      "This is the loss 0.13728733360767365\n",
      "This is the loss 0.13653191924095154\n",
      "This is the loss 0.11934459209442139\n",
      "This is the loss 0.12423060834407806\n",
      "This is the loss 0.148978129029274\n",
      "This is the loss 0.1388198882341385\n",
      "This is the loss 0.16572147607803345\n",
      "This is the loss 0.1299264132976532\n",
      "This is the loss 0.09994830936193466\n",
      "This is the loss 0.12066026031970978\n",
      "This is the loss 0.11640772223472595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.12101206183433533\n",
      "This is the loss 0.15449392795562744\n",
      "This is the loss 0.1526842564344406\n",
      "This is the loss 0.12405399978160858\n",
      "This is the loss 0.14453795552253723\n",
      "This is the loss 0.14392715692520142\n",
      "This is the loss 0.14307235181331635\n",
      "This is the loss 0.11786291748285294\n",
      "This is the loss 0.12485989183187485\n",
      "This is the loss 0.11732407659292221\n",
      "This is the loss 0.10879099369049072\n",
      "This is the loss 0.17001540958881378\n",
      "This is the loss 0.12415537983179092\n",
      "This is the loss 0.10959911346435547\n",
      "This is the loss 0.13034674525260925\n",
      "This is the loss 0.13007207214832306\n",
      "This is the loss 0.1660173237323761\n",
      "This is the loss 0.11296556144952774\n",
      "This is the loss 0.10135835409164429\n",
      "This is the loss 0.15356823801994324\n",
      "This is the loss 0.127943217754364\n",
      "This is the loss 0.14035765826702118\n",
      "This is the loss 0.15370303392410278\n",
      "This is the loss 0.13025476038455963\n",
      "This is the loss 0.14225471019744873\n",
      "This is the loss 0.13875538110733032\n",
      "This is the loss 0.12796024978160858\n",
      "This is the loss 0.19589227437973022\n",
      "This is the loss 0.1623213291168213\n",
      "This is the loss 0.12469600886106491\n",
      "This is the loss 0.14337831735610962\n",
      "This is the loss 0.15084251761436462\n",
      "This is the loss 0.1610775738954544\n",
      "This is the loss 0.1400022953748703\n",
      "This is the loss 0.1497461497783661\n",
      "This is the loss 0.13215701282024384\n",
      "This is the loss 0.12109450250864029\n",
      "This is the loss 0.15020516514778137\n",
      "This is the loss 0.12876400351524353\n",
      "This is the loss 0.14023323357105255\n",
      "This is the loss 0.15039001405239105\n",
      "This is the loss 0.14020439982414246\n",
      "This is the loss 0.1676507294178009\n",
      "This is the loss 0.13636857271194458\n",
      "This is the loss 0.12978439033031464\n",
      "This is the loss 0.15275177359580994\n",
      "This is the loss 0.11757811903953552\n",
      "This is the loss 0.150294691324234\n",
      "This is the loss 0.1490040421485901\n",
      "This is the loss 0.12164619565010071\n",
      "This is the loss 0.1550026535987854\n",
      "This is the loss 0.1564214825630188\n",
      "This is the loss 0.11411838233470917\n",
      "This is the loss 0.14042022824287415\n",
      "This is the loss 0.13356514275074005\n",
      "This is the loss 0.14312677085399628\n",
      "This is the loss 0.12418095767498016\n",
      "This is the loss 0.14561858773231506\n",
      "This is the loss 0.1246013194322586\n",
      "This is the loss 0.13269318640232086\n",
      "This is the loss 0.12825755774974823\n",
      "This is the loss 0.1357036828994751\n",
      "This is the loss 0.10897943377494812\n",
      "This is the loss 0.12961247563362122\n",
      "This is the loss 0.15140178799629211\n",
      "This is the loss 0.12529951333999634\n",
      "This is the loss 0.1386394053697586\n",
      "This is the loss 0.10235840827226639\n",
      "This is the loss 0.13952457904815674\n",
      "This is the loss 0.1206800639629364\n",
      "This is the loss 0.11740368604660034\n",
      "This is the loss 0.1414179801940918\n",
      "This is the loss 0.12048693001270294\n",
      "This is the loss 0.13947844505310059\n",
      "This is the loss 0.1384449303150177\n",
      "This is the loss 0.14766129851341248\n",
      "This is the loss 0.16922052204608917\n",
      "This is the loss 0.13738244771957397\n",
      "This is the loss 0.1386329084634781\n",
      "This is the loss 0.13885486125946045\n",
      "This is the loss 0.11941401660442352\n",
      "This is the loss 0.1378023475408554\n",
      "This is the loss 0.14531584084033966\n",
      "This is the loss 0.12702077627182007\n",
      "This is the loss 0.11947410553693771\n",
      "This is the loss 0.08553063124418259\n",
      "This is the loss 0.13737212121486664\n",
      "This is the loss 0.12864378094673157\n",
      "This is the loss 0.15406644344329834\n",
      "This is the loss 0.11369527876377106\n",
      "This is the loss 0.14148153364658356\n",
      "This is the loss 0.14533820748329163\n",
      "This is the loss 0.10710863023996353\n",
      "This is the loss 0.11959122121334076\n",
      "This is the loss 0.15324971079826355\n",
      "This is the loss 0.15888160467147827\n",
      "This is the loss 0.16862516105175018\n",
      "This is the loss 0.14480575919151306\n",
      "This is the loss 0.14879989624023438\n",
      "This is the loss 0.08692694455385208\n",
      "This is the loss 0.15181893110275269\n",
      "This is the loss 0.09930984675884247\n",
      "This is the loss 0.1208878830075264\n",
      "This is the loss 0.13306400179862976\n",
      "This is the loss 0.1254194974899292\n",
      "This is the loss 0.1355612576007843\n",
      "This is the loss 0.13757696747779846\n",
      "This is the loss 0.1494743824005127\n",
      "This is the loss 0.12070050835609436\n",
      "This is the loss 0.14116519689559937\n",
      "This is the loss 0.13195820152759552\n",
      "This is the loss 0.13578256964683533\n",
      "This is the loss 0.16314882040023804\n",
      "This is the loss 0.1625620722770691\n",
      "This is the loss 0.1385267823934555\n",
      "This is the loss 0.16295549273490906\n",
      "This is the loss 0.152296245098114\n",
      "This is the loss 0.14722169935703278\n",
      "This is the loss 0.14183446764945984\n",
      "This is the loss 0.16981348395347595\n",
      "This is the loss 0.15458402037620544\n",
      "This is the loss 0.17057789862155914\n",
      "This is the loss 0.13963887095451355\n",
      "This is the loss 0.14225919544696808\n",
      "This is the loss 0.14665833115577698\n",
      "This is the loss 0.1812046319246292\n",
      "This is the loss 0.15759889781475067\n",
      "This is the loss 0.16998443007469177\n",
      "This is the loss 0.12765845656394958\n",
      "This is the loss 0.10668165981769562\n",
      "This is the loss 0.11092369258403778\n",
      "This is the loss 0.14172683656215668\n",
      "This is the loss 0.1450224369764328\n",
      "This is the loss 0.14014184474945068\n",
      "This is the loss 0.11904837191104889\n",
      "This is the loss 0.10236377269029617\n",
      "This is the loss 0.14131322503089905\n",
      "This is the loss 0.12417411804199219\n",
      "This is the loss 0.12304791063070297\n",
      "This is the loss 0.12146477401256561\n",
      "This is the loss 0.12394058704376221\n",
      "This is the loss 0.1106238067150116\n",
      "This is the loss 0.14771024882793427\n",
      "This is the loss 0.1154492050409317\n",
      "This is the loss 0.16258279979228973\n",
      "This is the loss 0.13088595867156982\n",
      "This is the loss 0.13242100179195404\n",
      "This is the loss 0.11524634808301926\n",
      "This is the loss 0.12927719950675964\n",
      "This is the loss 0.16285033524036407\n",
      "This is the loss 0.16382285952568054\n",
      "This is the loss 0.15151455998420715\n",
      "This is the loss 0.11547186225652695\n",
      "This is the loss 0.15485122799873352\n",
      "This is the loss 0.16586117446422577\n",
      "This is the loss 0.14382076263427734\n",
      "This is the loss 0.15492784976959229\n",
      "This is the loss 0.11827856302261353\n",
      "This is the loss 0.14150062203407288\n",
      "This is the loss 0.11585405468940735\n",
      "This is the loss 0.16864541172981262\n",
      "This is the loss 0.1678527444601059\n",
      "This is the loss 0.15383018553256989\n",
      "This is the loss 0.13431969285011292\n",
      "This is the loss 0.13374562561511993\n",
      "This is the loss 0.12439960986375809\n",
      "This is the loss 0.13747122883796692\n",
      "This is the loss 0.11850861459970474\n",
      "This is the loss 0.13835494220256805\n",
      "This is the loss 0.11661738157272339\n",
      "This is the loss 0.08185205608606339\n",
      "This is the loss 0.11191801726818085\n",
      "This is the loss 0.12766903638839722\n",
      "This is the loss 0.1496635526418686\n",
      "This is the loss 0.12907548248767853\n",
      "This is the loss 0.12052150070667267\n",
      "This is the loss 0.1331506222486496\n",
      "This is the loss 0.13469146192073822\n",
      "This is the loss 0.13650977611541748\n",
      "This is the loss 0.13206367194652557\n",
      "This is the loss 0.13139599561691284\n",
      "This is the loss 0.13614152371883392\n",
      "This is the loss 0.13791945576667786\n",
      "This is the loss 0.1227165162563324\n",
      "This is the loss 0.14957760274410248\n",
      "This is the loss 0.1604122370481491\n",
      "This is the loss 0.14913707971572876\n",
      "This is the loss 0.14398038387298584\n",
      "This is the loss 0.1435864120721817\n",
      "This is the loss 0.14968141913414001\n",
      "This is the loss 0.1702709197998047\n",
      "This is the loss 0.16920655965805054\n",
      "This is the loss 0.1421494036912918\n",
      "This is the loss 0.16613832116127014\n",
      "This is the loss 0.165281280875206\n",
      "This is the loss 0.14894281327724457\n",
      "This is the loss 0.1645563691854477\n",
      "This is the loss 0.13817478716373444\n",
      "This is the loss 0.1147351935505867\n",
      "This is the loss 0.12879231572151184\n",
      "This is the loss 0.12069185823202133\n",
      "This is the loss 0.16031776368618011\n",
      "This is the loss 0.16009224951267242\n",
      "This is the loss 0.13820181787014008\n",
      "This is the loss 0.18648254871368408\n",
      "This is the loss 0.13106748461723328\n",
      "This is the loss 0.16550809144973755\n",
      "This is the loss 0.14138542115688324\n",
      "This is the loss 0.1930786669254303\n",
      "This is the loss 0.14996665716171265\n",
      "This is the loss 0.13554775714874268\n",
      "This is the loss 0.1498735100030899\n",
      "This is the loss 0.1578654646873474\n",
      "This is the loss 0.1412913054227829\n",
      "This is the loss 0.15142971277236938\n",
      "This is the loss 0.14440873265266418\n",
      "This is the loss 0.1369752287864685\n",
      "This is the loss 0.14600884914398193\n",
      "This is the loss 0.12946057319641113\n",
      "This is the loss 0.1275319755077362\n",
      "This is the loss 0.14879174530506134\n",
      "This is the loss 0.15479126572608948\n",
      "This is the loss 0.1255013793706894\n",
      "This is the loss 0.14002200961112976\n",
      "This is the loss 0.12019985169172287\n",
      "This is the loss 0.09989794343709946\n",
      "This is the loss 0.1215725913643837\n",
      "This is the loss 0.12795761227607727\n",
      "This is the loss 0.14131595194339752\n",
      "This is the loss 0.12063278257846832\n",
      "This is the loss 0.1552627980709076\n",
      "This is the loss 0.13191835582256317\n",
      "This is the loss 0.1252274215221405\n",
      "This is the loss 0.1498500257730484\n",
      "This is the loss 0.1099538579583168\n",
      "This is the loss 0.11532478779554367\n",
      "This is the loss 0.13433916866779327\n",
      "This is the loss 0.12315240502357483\n",
      "This is the loss 0.15906941890716553\n",
      "This is the loss 0.1525476574897766\n",
      "This is the loss 0.15068450570106506\n",
      "This is the loss 0.1419864147901535\n",
      "This is the loss 0.12319633364677429\n",
      "This is the loss 0.13920558989048004\n",
      "This is the loss 0.13112235069274902\n",
      "This is the loss 0.12793894112110138\n",
      "This is the loss 0.15089687705039978\n",
      "This is the loss 0.1568388193845749\n",
      "This is the loss 0.1531980186700821\n",
      "This is the loss 0.11587438732385635\n",
      "This is the loss 0.12385793030261993\n",
      "This is the loss 0.13109566271305084\n",
      "This is the loss 0.15896521508693695\n",
      "This is the loss 0.18372464179992676\n",
      "This is the loss 0.12326481938362122\n",
      "This is the loss 0.12616197764873505\n",
      "This is the loss 0.14239400625228882\n",
      "This is the loss 0.13126207888126373\n",
      "This is the loss 0.12609712779521942\n",
      "This is the loss 0.1268637627363205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.16555564105510712\n",
      "This is the loss 0.1276337057352066\n",
      "This is the loss 0.14595481753349304\n",
      "This is the loss 0.1173391193151474\n",
      "This is the loss 0.11916951835155487\n",
      "This is the loss 0.1064431220293045\n",
      "This is the loss 0.14190950989723206\n",
      "This is the loss 0.1463223248720169\n",
      "This is the loss 0.15702202916145325\n",
      "This is the loss 0.13825075328350067\n",
      "This is the loss 0.12886083126068115\n",
      "This is the loss 0.12324932962656021\n",
      "This is the loss 0.12151766568422318\n",
      "This is the loss 0.1080889031291008\n",
      "This is the loss 0.12997494637966156\n",
      "This is the loss 0.1249413937330246\n",
      "This is the loss 0.13774359226226807\n",
      "This is the loss 0.10011954605579376\n",
      "This is the loss 0.11847013235092163\n",
      "This is the loss 0.10890629887580872\n",
      "This is the loss 0.13574188947677612\n",
      "This is the loss 0.1346735954284668\n",
      "This is the loss 0.13729390501976013\n",
      "This is the loss 0.16636762022972107\n",
      "This is the loss 0.16322244703769684\n",
      "This is the loss 0.16194765269756317\n",
      "This is the loss 0.11725582927465439\n",
      "This is the loss 0.13250696659088135\n",
      "This is the loss 0.12040892243385315\n",
      "This is the loss 0.13181722164154053\n",
      "This is the loss 0.11719537526369095\n",
      "This is the loss 0.1494321972131729\n",
      "This is the loss 0.15361571311950684\n",
      "This is the loss 0.14690878987312317\n",
      "This is the loss 0.11100973188877106\n",
      "This is the loss 0.13537457585334778\n",
      "This is the loss 0.13386330008506775\n",
      "This is the loss 0.1366143822669983\n",
      "This is the loss 0.16260704398155212\n",
      "This is the loss 0.14891883730888367\n",
      "This is the loss 0.1351543664932251\n",
      "This is the loss 0.12801262736320496\n",
      "This is the loss 0.12503717839717865\n",
      "This is the loss 0.13763689994812012\n",
      "This is the loss 0.16366131603717804\n",
      "This is the loss 0.15109194815158844\n",
      "This is the loss 0.16720907390117645\n",
      "This is the loss 0.17730656266212463\n",
      "This is the loss 0.1202370747923851\n",
      "This is the loss 0.1602272242307663\n",
      "This is the loss 0.11787168681621552\n",
      "This is the loss 0.15223105251789093\n",
      "This is the loss 0.13421382009983063\n",
      "This is the loss 0.12548775970935822\n",
      "This is the loss 0.149966299533844\n",
      "This is the loss 0.1238381415605545\n",
      "This is the loss 0.15959474444389343\n",
      "This is the loss 0.1300443857908249\n",
      "This is the loss 0.1231561005115509\n",
      "This is the loss 0.15615999698638916\n",
      "This is the loss 0.11042557656764984\n",
      "This is the loss 0.12730658054351807\n",
      "This is the loss 0.11315611004829407\n",
      "This is the loss 0.1273922175168991\n",
      "This is the loss 0.15848380327224731\n",
      "This is the loss 0.12840019166469574\n",
      "This is the loss 0.13432836532592773\n",
      "This is the loss 0.12865769863128662\n",
      "This is the loss 0.09752292931079865\n",
      "This is the loss 0.1255769580602646\n",
      "This is the loss 0.11502882093191147\n",
      "This is the loss 0.10338205099105835\n",
      "This is the loss 0.1418224573135376\n",
      "This is the loss 0.12630173563957214\n",
      "This is the loss 0.09755631536245346\n",
      "This is the loss 0.12318853288888931\n",
      "This is the loss 0.14013317227363586\n",
      "This is the loss 0.14645272493362427\n",
      "This is the loss 0.12221136689186096\n",
      "This is the loss 0.11638259142637253\n",
      "This is the loss 0.12968547642230988\n",
      "This is the loss 0.11889893561601639\n",
      "This is the loss 0.12045693397521973\n",
      "This is the loss 0.15619641542434692\n",
      "This is the loss 0.16226141154766083\n",
      "This is the loss 0.1303427368402481\n",
      "This is the loss 0.15167959034442902\n",
      "This is the loss 0.13359320163726807\n",
      "This is the loss 0.1415579617023468\n",
      "This is the loss 0.13621361553668976\n",
      "This is the loss 0.12523043155670166\n",
      "This is the loss 0.16694585978984833\n",
      "This is the loss 0.12545019388198853\n",
      "This is the loss 0.10812375694513321\n",
      "This is the loss 0.14157524704933167\n",
      "This is the loss 0.1362380087375641\n",
      "This is the loss 0.15616220235824585\n",
      "This is the loss 0.16250649094581604\n",
      "This is the loss 0.152147114276886\n",
      "This is the loss 0.1278768926858902\n",
      "This is the loss 0.15142105519771576\n",
      "This is the loss 0.14382217824459076\n",
      "This is the loss 0.14898894727230072\n",
      "This is the loss 0.17085757851600647\n",
      "This is the loss 0.14194992184638977\n",
      "This is the loss 0.13719956576824188\n",
      "This is the loss 0.14427924156188965\n",
      "This is the loss 0.13799457252025604\n",
      "This is the loss 0.16935864090919495\n",
      "This is the loss 0.13466909527778625\n",
      "This is the loss 0.14632835984230042\n",
      "This is the loss 0.1263723522424698\n",
      "This is the loss 0.1494331806898117\n",
      "This is the loss 0.14620652794837952\n",
      "This is the loss 0.14544041454792023\n",
      "This is the loss 0.1373254954814911\n",
      "This is the loss 0.16183321177959442\n",
      "This is the loss 0.12116356194019318\n",
      "This is the loss 0.15077950060367584\n",
      "This is the loss 0.1573328822851181\n",
      "This is the loss 0.15548861026763916\n",
      "This is the loss 0.14467158913612366\n",
      "This is the loss 0.14162704348564148\n",
      "This is the loss 0.1439652293920517\n",
      "This is the loss 0.16271597146987915\n",
      "This is the loss 0.13611680269241333\n",
      "This is the loss 0.12653952836990356\n",
      "This is the loss 0.14975348114967346\n",
      "This is the loss 0.17072755098342896\n",
      "This is the loss 0.1632993072271347\n",
      "This is the loss 0.14268580079078674\n",
      "This is the loss 0.14674362540245056\n",
      "This is the loss 0.126571923494339\n",
      "This is the loss 0.15049298107624054\n",
      "This is the loss 0.13958528637886047\n",
      "This is the loss 0.17703190445899963\n",
      "This is the loss 0.18309231102466583\n",
      "This is the loss 0.1500009447336197\n",
      "This is the loss 0.14015401899814606\n",
      "This is the loss 0.13391020894050598\n",
      "This is the loss 0.11119255423545837\n",
      "This is the loss 0.16720496118068695\n",
      "This is the loss 0.13043268024921417\n",
      "This is the loss 0.15079139173030853\n",
      "This is the loss 0.19265428185462952\n",
      "This is the loss 0.13568280637264252\n",
      "This is the loss 0.1667765974998474\n",
      "This is the loss 0.1253088265657425\n",
      "This is the loss 0.1170133650302887\n",
      "This is the loss 0.1716802716255188\n",
      "This is the loss 0.13942860066890717\n",
      "This is the loss 0.16865792870521545\n",
      "This is the loss 0.16039393842220306\n",
      "This is the loss 0.15297070145606995\n",
      "This is the loss 0.15692421793937683\n",
      "This is the loss 0.1683223843574524\n",
      "This is the loss 0.15274402499198914\n",
      "This is the loss 0.12310110032558441\n",
      "This is the loss 0.18789611756801605\n",
      "This is the loss 0.15079814195632935\n",
      "This is the loss 0.13595426082611084\n",
      "This is the loss 0.13677144050598145\n",
      "This is the loss 0.14507755637168884\n",
      "This is the loss 0.15743829309940338\n",
      "This is the loss 0.15133461356163025\n",
      "This is the loss 0.13585704565048218\n",
      "This is the loss 0.1621655523777008\n",
      "This is the loss 0.13796263933181763\n",
      "This is the loss 0.12387408316135406\n",
      "This is the loss 0.13588488101959229\n",
      "This is the loss 0.09709758311510086\n",
      "This is the loss 0.13739129900932312\n",
      "This is the loss 0.1395896077156067\n",
      "This is the loss 0.1633790135383606\n",
      "This is the loss 0.1426359862089157\n",
      "This is the loss 0.14676569402217865\n",
      "This is the loss 0.1561337262392044\n",
      "This is the loss 0.1621636152267456\n",
      "This is the loss 0.17499513924121857\n",
      "This is the loss 0.1554962694644928\n",
      "This is the loss 0.1611672341823578\n",
      "This is the loss 0.1629907488822937\n",
      "This is the loss 0.1817772537469864\n",
      "This is the loss 0.1207403764128685\n",
      "This is the loss 0.14545808732509613\n",
      "This is the loss 0.15546667575836182\n",
      "This is the loss 0.1817312240600586\n",
      "This is the loss 0.1525249183177948\n",
      "This is the loss 0.13991987705230713\n",
      "This is the loss 0.1861773431301117\n",
      "This is the loss 0.1799955517053604\n",
      "This is the loss 0.12664449214935303\n",
      "This is the loss 0.16047020256519318\n",
      "This is the loss 0.15348947048187256\n",
      "This is the loss 0.14660698175430298\n",
      "This is the loss 0.14220842719078064\n",
      "This is the loss 0.14764143526554108\n",
      "This is the loss 0.1276383399963379\n",
      "This is the loss 0.1532168835401535\n",
      "This is the loss 0.12114529311656952\n",
      "This is the loss 0.1318833827972412\n",
      "This is the loss 0.13337823748588562\n",
      "This is the loss 0.1582452356815338\n",
      "This is the loss 0.17296800017356873\n",
      "This is the loss 0.17165900766849518\n",
      "This is the loss 0.14037872850894928\n",
      "This is the loss 0.13158433139324188\n",
      "This is the loss 0.12414734065532684\n",
      "This is the loss 0.11838214099407196\n",
      "This is the loss 0.12258343398571014\n",
      "This is the loss 0.11854365468025208\n",
      "This is the loss 0.15589232742786407\n",
      "This is the loss 0.1404523253440857\n",
      "This is the loss 0.16299115121364594\n",
      "This is the loss 0.13793456554412842\n",
      "This is the loss 0.11590812355279922\n",
      "This is the loss 0.13503216207027435\n",
      "This is the loss 0.12245446443557739\n",
      "This is the loss 0.14101243019104004\n",
      "This is the loss 0.14679645001888275\n",
      "This is the loss 0.1592041254043579\n",
      "This is the loss 0.15211626887321472\n",
      "This is the loss 0.1399175524711609\n",
      "This is the loss 0.1499115377664566\n",
      "This is the loss 0.14366088807582855\n",
      "This is the loss 0.14796270430088043\n",
      "This is the loss 0.15133686363697052\n",
      "This is the loss 0.11782543361186981\n",
      "This is the loss 0.15891437232494354\n",
      "This is the loss 0.13375210762023926\n",
      "This is the loss 0.12625108659267426\n",
      "This is the loss 0.13678224384784698\n",
      "This is the loss 0.14122727513313293\n",
      "This is the loss 0.16044433414936066\n",
      "This is the loss 0.12962420284748077\n",
      "This is the loss 0.14438782632350922\n",
      "This is the loss 0.13484089076519012\n",
      "This is the loss 0.12444980442523956\n",
      "This is the loss 0.1574261486530304\n",
      "This is the loss 0.1386515349149704\n",
      "This is the loss 0.1466197371482849\n",
      "This is the loss 0.11960793286561966\n",
      "This is the loss 0.11312702298164368\n",
      "This is the loss 0.16167181730270386\n",
      "This is the loss 0.14350208640098572\n",
      "This is the loss 0.11384463310241699\n",
      "This is the loss 0.1350392997264862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.14498549699783325\n",
      "This is the loss 0.12016976624727249\n",
      "This is the loss 0.14970889687538147\n",
      "This is the loss 0.10384023189544678\n",
      "This is the loss 0.15211358666419983\n",
      "This is the loss 0.14784985780715942\n",
      "This is the loss 0.1313561201095581\n",
      "This is the loss 0.13191545009613037\n",
      "This is the loss 0.12335224449634552\n",
      "This is the loss 0.16242516040802002\n",
      "This is the loss 0.12289407104253769\n",
      "This is the loss 0.1595914512872696\n",
      "This is the loss 0.12255221605300903\n",
      "This is the loss 0.12823474407196045\n",
      "This is the loss 0.1597505509853363\n",
      "This is the loss 0.14324353635311127\n",
      "This is the loss 0.13041973114013672\n",
      "This is the loss 0.13669314980506897\n",
      "This is the loss 0.1373351812362671\n",
      "This is the loss 0.12856100499629974\n",
      "This is the loss 0.16204415261745453\n",
      "This is the loss 0.11906322836875916\n",
      "This is the loss 0.1655159890651703\n",
      "This is the loss 0.13020294904708862\n",
      "This is the loss 0.11618614941835403\n",
      "This is the loss 0.13116973638534546\n",
      "This is the loss 0.14634129405021667\n",
      "This is the loss 0.1348888874053955\n",
      "This is the loss 0.16566556692123413\n",
      "This is the loss 0.1413397639989853\n",
      "This is the loss 0.12221699953079224\n",
      "This is the loss 0.1230241134762764\n",
      "This is the loss 0.16084149479866028\n",
      "This is the loss 0.1427634358406067\n",
      "This is the loss 0.12087439000606537\n",
      "This is the loss 0.123902827501297\n",
      "This is the loss 0.15512226521968842\n",
      "This is the loss 0.1549873799085617\n",
      "This is the loss 0.13362722098827362\n",
      "This is the loss 0.1377125233411789\n",
      "This is the loss 0.12690484523773193\n",
      "This is the loss 0.14477156102657318\n",
      "This is the loss 0.15779563784599304\n",
      "This is the loss 0.11239804327487946\n",
      "This is the loss 0.10885221511125565\n",
      "This is the loss 0.13649851083755493\n",
      "This is the loss 0.17546069622039795\n",
      "This is the loss 0.14235980808734894\n",
      "This is the loss 0.1648627370595932\n",
      "This is the loss 0.17641453444957733\n",
      "This is the loss 0.17809827625751495\n",
      "This is the loss 0.14436089992523193\n",
      "This is the loss 0.14616656303405762\n",
      "This is the loss 0.13165095448493958\n",
      "This is the loss 0.12289932370185852\n",
      "This is the loss 0.12030257284641266\n",
      "This is the loss 0.13787320256233215\n",
      "This is the loss 0.12880298495292664\n",
      "This is the loss 0.13424983620643616\n",
      "This is the loss 0.12818968296051025\n",
      "This is the loss 0.1228470504283905\n",
      "This is the loss 0.1510392427444458\n",
      "This is the loss 0.14637766778469086\n",
      "This is the loss 0.1695534586906433\n",
      "This is the loss 0.1177361011505127\n",
      "This is the loss 0.1304197758436203\n",
      "This is the loss 0.15105342864990234\n",
      "This is the loss 0.11590391397476196\n",
      "This is the loss 0.14169615507125854\n",
      "This is the loss 0.15148450434207916\n",
      "This is the loss 0.12182626873254776\n",
      "This is the loss 0.1574181318283081\n",
      "This is the loss 0.11578165739774704\n",
      "This is the loss 0.14873449504375458\n",
      "This is the loss 0.12664610147476196\n",
      "This is the loss 0.15280291438102722\n",
      "This is the loss 0.12551698088645935\n",
      "This is the loss 0.1506769210100174\n",
      "This is the loss 0.11953755468130112\n",
      "This is the loss 0.12960994243621826\n",
      "This is the loss 0.14695942401885986\n",
      "This is the loss 0.12789539992809296\n",
      "This is the loss 0.13383299112319946\n",
      "This is the loss 0.0985775738954544\n",
      "This is the loss 0.1409914791584015\n",
      "This is the loss 0.131394624710083\n",
      "This is the loss 0.11483023315668106\n",
      "This is the loss 0.14256930351257324\n",
      "This is the loss 0.15155430138111115\n",
      "This is the loss 0.14689210057258606\n",
      "This is the loss 0.1391097903251648\n",
      "This is the loss 0.16203278303146362\n",
      "This is the loss 0.10062312334775925\n",
      "This is the loss 0.1693211793899536\n",
      "This is the loss 0.11220000684261322\n",
      "This is the loss 0.13553623855113983\n",
      "This is the loss 0.13332869112491608\n",
      "This is the loss 0.13664862513542175\n",
      "This is the loss 0.1345212757587433\n",
      "This is the loss 0.12464377284049988\n",
      "This is the loss 0.11916114389896393\n",
      "This is the loss 0.1369771659374237\n",
      "This is the loss 0.15145769715309143\n",
      "This is the loss 0.14723214507102966\n",
      "This is the loss 0.12688449025154114\n",
      "This is the loss 0.12458792328834534\n",
      "This is the loss 0.11342182010412216\n",
      "This is the loss 0.1492069810628891\n",
      "This is the loss 0.1611720770597458\n",
      "This is the loss 0.12568318843841553\n",
      "This is the loss 0.12009765952825546\n",
      "This is the loss 0.13934876024723053\n",
      "This is the loss 0.14773277938365936\n",
      "This is the loss 0.13340799510478973\n",
      "This is the loss 0.14457841217517853\n",
      "This is the loss 0.15048286318778992\n",
      "This is the loss 0.1701330691576004\n",
      "This is the loss 0.16416876018047333\n",
      "This is the loss 0.15370680391788483\n",
      "This is the loss 0.13297885656356812\n",
      "This is the loss 0.1604945957660675\n",
      "This is the loss 0.14999321103096008\n",
      "This is the loss 0.12780185043811798\n",
      "This is the loss 0.13095444440841675\n",
      "This is the loss 0.14720700681209564\n",
      "This is the loss 0.1466299593448639\n",
      "This is the loss 0.14001134037971497\n",
      "This is the loss 0.14893940091133118\n",
      "This is the loss 0.14097540080547333\n",
      "This is the loss 0.12725882232189178\n",
      "This is the loss 0.1396642029285431\n",
      "This is the loss 0.14199897646903992\n",
      "This is the loss 0.133933886885643\n",
      "This is the loss 0.15280680358409882\n",
      "This is the loss 0.12313997745513916\n",
      "This is the loss 0.12416047602891922\n",
      "This is the loss 0.16116374731063843\n",
      "This is the loss 0.14597955346107483\n",
      "This is the loss 0.16066356003284454\n",
      "This is the loss 0.11228428035974503\n",
      "This is the loss 0.10715176910161972\n",
      "This is the loss 0.14361217617988586\n",
      "This is the loss 0.11163212358951569\n",
      "This is the loss 0.11457201838493347\n",
      "This is the loss 0.1425493210554123\n",
      "This is the loss 0.13560055196285248\n",
      "This is the loss 0.13164348900318146\n",
      "This is the loss 0.12925907969474792\n",
      "This is the loss 0.16162347793579102\n",
      "This is the loss 0.1524825543165207\n",
      "This is the loss 0.13944996893405914\n",
      "This is the loss 0.13247264921665192\n",
      "This is the loss 0.1441972702741623\n",
      "This is the loss 0.13973240554332733\n",
      "This is the loss 0.15160632133483887\n",
      "This is the loss 0.1152167022228241\n",
      "This is the loss 0.155521959066391\n",
      "This is the loss 0.16417554020881653\n",
      "This is the loss 0.15591111779212952\n",
      "This is the loss 0.10763926804065704\n",
      "This is the loss 0.1342005729675293\n",
      "This is the loss 0.08923660218715668\n",
      "This is the loss 0.1440589874982834\n",
      "This is the loss 0.13357418775558472\n",
      "This is the loss 0.1736331284046173\n",
      "This is the loss 0.1083696112036705\n",
      "This is the loss 0.11473724991083145\n",
      "This is the loss 0.13025733828544617\n",
      "This is the loss 0.1263812631368637\n",
      "This is the loss 0.15996669232845306\n",
      "This is the loss 0.13268102705478668\n",
      "This is the loss 0.11522102355957031\n",
      "This is the loss 0.16234710812568665\n",
      "This is the loss 0.13034240901470184\n",
      "This is the loss 0.15672683715820312\n",
      "This is the loss 0.119090236723423\n",
      "This is the loss 0.145246684551239\n",
      "This is the loss 0.14164428412914276\n",
      "This is the loss 0.1308760941028595\n",
      "This is the loss 0.15472470223903656\n",
      "This is the loss 0.1275959461927414\n",
      "This is the loss 0.15639282763004303\n",
      "This is the loss 0.13700754940509796\n",
      "This is the loss 0.11381430923938751\n",
      "This is the loss 0.1307506263256073\n",
      "This is the loss 0.10476081073284149\n",
      "This is the loss 0.11094533652067184\n",
      "This is the loss 0.15576085448265076\n",
      "This is the loss 0.1323215365409851\n",
      "This is the loss 0.1351015418767929\n",
      "This is the loss 0.11369910836219788\n",
      "This is the loss 0.12122176587581635\n",
      "This is the loss 0.1437002271413803\n",
      "This is the loss 0.13566195964813232\n",
      "This is the loss 0.13075682520866394\n",
      "This is the loss 0.17152942717075348\n",
      "This is the loss 0.15039803087711334\n",
      "This is the loss 0.13251391053199768\n",
      "This is the loss 0.11006824672222137\n",
      "This is the loss 0.13576355576515198\n",
      "This is the loss 0.14213864505290985\n",
      "This is the loss 0.14226216077804565\n",
      "This is the loss 0.11239627748727798\n",
      "This is the loss 0.14452889561653137\n",
      "This is the loss 0.12798774242401123\n",
      "This is the loss 0.11605581641197205\n",
      "This is the loss 0.1281708925962448\n",
      "This is the loss 0.12844990193843842\n",
      "This is the loss 0.13429611921310425\n",
      "This is the loss 0.13294729590415955\n",
      "This is the loss 0.12923003733158112\n",
      "This is the loss 0.16803891956806183\n",
      "This is the loss 0.15003684163093567\n",
      "This is the loss 0.15074098110198975\n",
      "This is the loss 0.14416177570819855\n",
      "This is the loss 0.11998069286346436\n",
      "This is the loss 0.14356179535388947\n",
      "This is the loss 0.12347862869501114\n",
      "This is the loss 0.14001798629760742\n",
      "This is the loss 0.15256893634796143\n",
      "This is the loss 0.12059682607650757\n",
      "This is the loss 0.1462486833333969\n",
      "This is the loss 0.14145401120185852\n",
      "This is the loss 0.13761389255523682\n",
      "This is the loss 0.14573578536510468\n",
      "This is the loss 0.1508086919784546\n",
      "This is the loss 0.15544265508651733\n",
      "This is the loss 0.14029689133167267\n",
      "This is the loss 0.14547193050384521\n",
      "This is the loss 0.14126595854759216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.14495478570461273\n",
      "This is the loss 0.12437841296195984\n",
      "This is the loss 0.14919644594192505\n",
      "This is the loss 0.1577208936214447\n",
      "This is the loss 0.10660004615783691\n",
      "This is the loss 0.13000841438770294\n",
      "This is the loss 0.12059526890516281\n",
      "This is the loss 0.14969328045845032\n",
      "This is the loss 0.13790854811668396\n",
      "This is the loss 0.13523787260055542\n",
      "This is the loss 0.12951883673667908\n",
      "This is the loss 0.11567795276641846\n",
      "This is the loss 0.14643102884292603\n",
      "This is the loss 0.11736609786748886\n",
      "This is the loss 0.11471512913703918\n",
      "This is the loss 0.11234764754772186\n",
      "This is the loss 0.12313530594110489\n",
      "This is the loss 0.12692461907863617\n",
      "This is the loss 0.10969561338424683\n",
      "This is the loss 0.1465994268655777\n",
      "This is the loss 0.11976366490125656\n",
      "This is the loss 0.14032381772994995\n",
      "This is the loss 0.1258653700351715\n",
      "This is the loss 0.11587844789028168\n",
      "This is the loss 0.12012539803981781\n",
      "This is the loss 0.16560465097427368\n",
      "This is the loss 0.13148541748523712\n",
      "This is the loss 0.16802841424942017\n",
      "This is the loss 0.15258026123046875\n",
      "This is the loss 0.1660124808549881\n",
      "This is the loss 0.11743666231632233\n",
      "This is the loss 0.12705521285533905\n",
      "This is the loss 0.09038684517145157\n",
      "This is the loss 0.12018252164125443\n",
      "This is the loss 0.1601260006427765\n",
      "This is the loss 0.10594622790813446\n",
      "This is the loss 0.14713843166828156\n",
      "This is the loss 0.11983250826597214\n",
      "This is the loss 0.12470139563083649\n",
      "This is the loss 0.14095906913280487\n",
      "This is the loss 0.1481303572654724\n",
      "This is the loss 0.12091302126646042\n",
      "This is the loss 0.1407095044851303\n",
      "This is the loss 0.152664452791214\n",
      "This is the loss 0.13233765959739685\n",
      "This is the loss 0.1222313717007637\n",
      "This is the loss 0.1193087249994278\n",
      "This is the loss 0.13718165457248688\n",
      "This is the loss 0.14056411385536194\n",
      "This is the loss 0.14213214814662933\n",
      "This is the loss 0.1767980307340622\n",
      "This is the loss 0.14173704385757446\n",
      "This is the loss 0.14344128966331482\n",
      "This is the loss 0.16169877350330353\n",
      "This is the loss 0.129502072930336\n",
      "This is the loss 0.16096970438957214\n",
      "This is the loss 0.13816969096660614\n",
      "This is the loss 0.1512889862060547\n",
      "This is the loss 0.13483157753944397\n",
      "This is the loss 0.1832684576511383\n",
      "This is the loss 0.14994500577449799\n",
      "This is the loss 0.13634884357452393\n",
      "This is the loss 0.12393901497125626\n",
      "This is the loss 0.14332333207130432\n",
      "This is the loss 0.12690520286560059\n",
      "This is the loss 0.11620227992534637\n",
      "This is the loss 0.12917178869247437\n",
      "This is the loss 0.11153744161128998\n",
      "This is the loss 0.13954712450504303\n",
      "This is the loss 0.14709287881851196\n",
      "This is the loss 0.15737342834472656\n",
      "This is the loss 0.14835958182811737\n",
      "This is the loss 0.11639965325593948\n",
      "This is the loss 0.148950457572937\n",
      "This is the loss 0.12896908819675446\n",
      "This is the loss 0.13384485244750977\n",
      "This is the loss 0.12633302807807922\n",
      "This is the loss 0.12908630073070526\n",
      "This is the loss 0.13524673879146576\n",
      "This is the loss 0.10718387365341187\n",
      "This is the loss 0.12215203046798706\n",
      "This is the loss 0.14252743124961853\n",
      "This is the loss 0.1424695998430252\n",
      "This is the loss 0.10359032452106476\n",
      "This is the loss 0.1430901139974594\n",
      "This is the loss 0.11501401662826538\n",
      "This is the loss 0.13296879827976227\n",
      "This is the loss 0.14282293617725372\n",
      "This is the loss 0.15206527709960938\n",
      "This is the loss 0.15399852395057678\n",
      "This is the loss 0.13157282769680023\n",
      "This is the loss 0.1564745306968689\n",
      "This is the loss 0.15920044481754303\n",
      "This is the loss 0.11950366944074631\n",
      "This is the loss 0.10353968292474747\n",
      "This is the loss 0.11685733497142792\n",
      "This is the loss 0.12366516143083572\n",
      "This is the loss 0.12016258388757706\n",
      "This is the loss 0.1374053657054901\n",
      "This is the loss 0.12373121827840805\n",
      "This is the loss 0.12771809101104736\n",
      "This is the loss 0.13232474029064178\n",
      "This is the loss 0.13546451926231384\n",
      "This is the loss 0.1415141075849533\n",
      "This is the loss 0.12914462387561798\n",
      "This is the loss 0.11690804362297058\n",
      "This is the loss 0.12440347671508789\n",
      "This is the loss 0.16952446103096008\n",
      "This is the loss 0.137590229511261\n",
      "This is the loss 0.1329263150691986\n",
      "This is the loss 0.13550761342048645\n",
      "This is the loss 0.11093206703662872\n",
      "This is the loss 0.13426105678081512\n",
      "This is the loss 0.1386241316795349\n",
      "This is the loss 0.13538116216659546\n",
      "This is the loss 0.14731523394584656\n",
      "This is the loss 0.12059059739112854\n",
      "This is the loss 0.13953012228012085\n",
      "This is the loss 0.1287832111120224\n",
      "This is the loss 0.17557178437709808\n",
      "This is the loss 0.12276176363229752\n",
      "This is the loss 0.12001506984233856\n",
      "This is the loss 0.14768919348716736\n",
      "This is the loss 0.14688988029956818\n",
      "This is the loss 0.1296045035123825\n",
      "This is the loss 0.15251056849956512\n",
      "This is the loss 0.1427895724773407\n",
      "This is the loss 0.17046813666820526\n",
      "This is the loss 0.1274021416902542\n",
      "This is the loss 0.1201215386390686\n",
      "This is the loss 0.12946489453315735\n",
      "This is the loss 0.15224304795265198\n",
      "This is the loss 0.13924521207809448\n",
      "This is the loss 0.12986056506633759\n",
      "This is the loss 0.13680508732795715\n",
      "This is the loss 0.1277676522731781\n",
      "This is the loss 0.09502041339874268\n",
      "This is the loss 0.13481871783733368\n",
      "This is the loss 0.10532107949256897\n",
      "This is the loss 0.12066804617643356\n",
      "This is the loss 0.12945231795310974\n",
      "This is the loss 0.16384555399417877\n",
      "This is the loss 0.1461559385061264\n",
      "This is the loss 0.14988112449645996\n",
      "This is the loss 0.13917241990566254\n",
      "This is the loss 0.16588671505451202\n",
      "This is the loss 0.13750343024730682\n",
      "This is the loss 0.15434126555919647\n",
      "This is the loss 0.15600082278251648\n",
      "This is the loss 0.13734832406044006\n",
      "This is the loss 0.13561712205410004\n",
      "This is the loss 0.14186085760593414\n",
      "This is the loss 0.14409026503562927\n",
      "This is the loss 0.13659265637397766\n",
      "This is the loss 0.1448488086462021\n",
      "This is the loss 0.15478086471557617\n",
      "This is the loss 0.15380744636058807\n",
      "This is the loss 0.12063866853713989\n",
      "This is the loss 0.14881575107574463\n",
      "This is the loss 0.1387869417667389\n",
      "This is the loss 0.1278473138809204\n",
      "This is the loss 0.14023637771606445\n",
      "This is the loss 0.11151771247386932\n",
      "This is the loss 0.15717743337154388\n",
      "This is the loss 0.12338326871395111\n",
      "This is the loss 0.14302363991737366\n",
      "This is the loss 0.13301099836826324\n",
      "This is the loss 0.11814634501934052\n",
      "This is the loss 0.14385943114757538\n",
      "This is the loss 0.12153035402297974\n",
      "This is the loss 0.12393106520175934\n",
      "This is the loss 0.13626110553741455\n",
      "This is the loss 0.16344046592712402\n",
      "This is the loss 0.13997550308704376\n",
      "This is the loss 0.12192510068416595\n",
      "This is the loss 0.15836603939533234\n",
      "This is the loss 0.17818836867809296\n",
      "This is the loss 0.1404174119234085\n",
      "This is the loss 0.13673612475395203\n",
      "This is the loss 0.1574879139661789\n",
      "This is the loss 0.1558331549167633\n",
      "This is the loss 0.11860889941453934\n",
      "This is the loss 0.1450151801109314\n",
      "This is the loss 0.11763875186443329\n",
      "This is the loss 0.13835036754608154\n",
      "This is the loss 0.14883914589881897\n",
      "This is the loss 0.13575737178325653\n",
      "This is the loss 0.12095168232917786\n",
      "This is the loss 0.14082227647304535\n",
      "This is the loss 0.11246273666620255\n",
      "This is the loss 0.12678466737270355\n",
      "This is the loss 0.090327188372612\n",
      "This is the loss 0.13174806535243988\n",
      "This is the loss 0.11120620369911194\n",
      "This is the loss 0.13625556230545044\n",
      "This is the loss 0.1041613221168518\n",
      "This is the loss 0.1277220994234085\n",
      "This is the loss 0.136063352227211\n",
      "This is the loss 0.10516528785228729\n",
      "This is the loss 0.14073695242404938\n",
      "This is the loss 0.16255277395248413\n",
      "This is the loss 0.1441916525363922\n",
      "This is the loss 0.15529495477676392\n",
      "This is the loss 0.14039091765880585\n",
      "This is the loss 0.15251918137073517\n",
      "This is the loss 0.1626882404088974\n",
      "This is the loss 0.1483275443315506\n",
      "This is the loss 0.16182047128677368\n",
      "This is the loss 0.14035363495349884\n",
      "This is the loss 0.13182039558887482\n",
      "This is the loss 0.14475831389427185\n",
      "This is the loss 0.1206696480512619\n",
      "This is the loss 0.1414022296667099\n",
      "This is the loss 0.11889950931072235\n",
      "This is the loss 0.11537961661815643\n",
      "This is the loss 0.16511270403862\n",
      "This is the loss 0.14337775111198425\n",
      "This is the loss 0.11987718939781189\n",
      "This is the loss 0.14707425236701965\n",
      "This is the loss 0.13988956809043884\n",
      "This is the loss 0.14130039513111115\n",
      "This is the loss 0.12357257306575775\n",
      "This is the loss 0.13757361471652985\n",
      "This is the loss 0.13979557156562805\n",
      "This is the loss 0.1363072395324707\n",
      "This is the loss 0.13824714720249176\n",
      "This is the loss 0.13596610724925995\n",
      "This is the loss 0.1449970006942749\n",
      "This is the loss 0.15610426664352417\n",
      "This is the loss 0.12660980224609375\n",
      "This is the loss 0.12947505712509155\n",
      "This is the loss 0.10207049548625946\n",
      "This is the loss 0.13918885588645935\n",
      "This is the loss 0.13292330503463745\n",
      "This is the loss 0.16798874735832214\n",
      "This is the loss 0.13951820135116577\n",
      "This is the loss 0.1442904770374298\n",
      "This is the loss 0.12767881155014038\n",
      "This is the loss 0.11799057573080063\n",
      "This is the loss 0.11081893742084503\n",
      "This is the loss 0.13907240331172943\n",
      "This is the loss 0.14420554041862488\n",
      "This is the loss 0.1570734977722168\n",
      "This is the loss 0.1480555385351181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.13957101106643677\n",
      "This is the loss 0.1397465020418167\n",
      "This is the loss 0.14634451270103455\n",
      "This is the loss 0.13744987547397614\n",
      "This is the loss 0.1382930725812912\n",
      "This is the loss 0.10997425019741058\n",
      "This is the loss 0.15122590959072113\n",
      "This is the loss 0.16334550082683563\n",
      "This is the loss 0.1343657225370407\n",
      "This is the loss 0.161662295460701\n",
      "This is the loss 0.13716885447502136\n",
      "This is the loss 0.10394987463951111\n",
      "This is the loss 0.14068886637687683\n",
      "This is the loss 0.13453839719295502\n",
      "This is the loss 0.16942134499549866\n",
      "This is the loss 0.10500889271497726\n",
      "This is the loss 0.18430691957473755\n",
      "This is the loss 0.13855522871017456\n",
      "This is the loss 0.1581212729215622\n",
      "This is the loss 0.15634992718696594\n",
      "This is the loss 0.12110618501901627\n",
      "This is the loss 0.12717264890670776\n",
      "This is the loss 0.12871892750263214\n",
      "This is the loss 0.11235425621271133\n",
      "This is the loss 0.13711658120155334\n",
      "This is the loss 0.13619455695152283\n",
      "This is the loss 0.11927207559347153\n",
      "This is the loss 0.12446168810129166\n",
      "This is the loss 0.14891602098941803\n",
      "This is the loss 0.13879933953285217\n",
      "This is the loss 0.16560539603233337\n",
      "This is the loss 0.13009154796600342\n",
      "This is the loss 0.10017909854650497\n",
      "This is the loss 0.12081651389598846\n",
      "This is the loss 0.1161937564611435\n",
      "This is the loss 0.12118855863809586\n",
      "This is the loss 0.15416912734508514\n",
      "This is the loss 0.1526063084602356\n",
      "This is the loss 0.1237194687128067\n",
      "This is the loss 0.1444566249847412\n",
      "This is the loss 0.14366823434829712\n",
      "This is the loss 0.14258070290088654\n",
      "This is the loss 0.11825862526893616\n",
      "This is the loss 0.12476044148206711\n",
      "This is the loss 0.1172938272356987\n",
      "This is the loss 0.10904154181480408\n",
      "This is the loss 0.1696256697177887\n",
      "This is the loss 0.12426210194826126\n",
      "This is the loss 0.10995734483003616\n",
      "This is the loss 0.13027343153953552\n",
      "This is the loss 0.1302400678396225\n",
      "This is the loss 0.16632604598999023\n",
      "This is the loss 0.11310207098722458\n",
      "This is the loss 0.10135013610124588\n",
      "This is the loss 0.1533164083957672\n",
      "This is the loss 0.12785619497299194\n",
      "This is the loss 0.13973119854927063\n",
      "This is the loss 0.15327875316143036\n",
      "This is the loss 0.13037317991256714\n",
      "This is the loss 0.1427888423204422\n",
      "This is the loss 0.1384575068950653\n",
      "This is the loss 0.127566397190094\n",
      "This is the loss 0.19546309113502502\n",
      "This is the loss 0.16188526153564453\n",
      "This is the loss 0.12445653229951859\n",
      "This is the loss 0.14366227388381958\n",
      "This is the loss 0.15052220225334167\n",
      "This is the loss 0.16063156723976135\n",
      "This is the loss 0.1400734931230545\n",
      "This is the loss 0.14978940784931183\n",
      "This is the loss 0.13195209205150604\n",
      "This is the loss 0.12071509659290314\n",
      "This is the loss 0.15001755952835083\n",
      "This is the loss 0.1287233829498291\n",
      "This is the loss 0.1400814652442932\n",
      "This is the loss 0.15009835362434387\n",
      "This is the loss 0.14001640677452087\n",
      "This is the loss 0.16751058399677277\n",
      "This is the loss 0.13648422062397003\n",
      "This is the loss 0.12983062863349915\n",
      "This is the loss 0.15287597477436066\n",
      "This is the loss 0.11719043552875519\n",
      "This is the loss 0.15030890703201294\n",
      "This is the loss 0.14895273745059967\n",
      "This is the loss 0.12177261710166931\n",
      "This is the loss 0.1546197533607483\n",
      "This is the loss 0.15648311376571655\n",
      "This is the loss 0.11402219533920288\n",
      "This is the loss 0.1399173140525818\n",
      "This is the loss 0.13335958123207092\n",
      "This is the loss 0.143220454454422\n",
      "This is the loss 0.12366871535778046\n",
      "This is the loss 0.14497171342372894\n",
      "This is the loss 0.12434061616659164\n",
      "This is the loss 0.13304269313812256\n",
      "This is the loss 0.12820154428482056\n",
      "This is the loss 0.13526606559753418\n",
      "This is the loss 0.10889945179224014\n",
      "This is the loss 0.12929698824882507\n",
      "This is the loss 0.15157568454742432\n",
      "This is the loss 0.12517912685871124\n",
      "This is the loss 0.1383989304304123\n",
      "This is the loss 0.10211938619613647\n",
      "This is the loss 0.13975752890110016\n",
      "This is the loss 0.12034837901592255\n",
      "This is the loss 0.11729592084884644\n",
      "This is the loss 0.14086319506168365\n",
      "This is the loss 0.12056849151849747\n",
      "This is the loss 0.1395167112350464\n",
      "This is the loss 0.13824273645877838\n",
      "This is the loss 0.14783057570457458\n",
      "This is the loss 0.1691940724849701\n",
      "This is the loss 0.13709954917430878\n",
      "This is the loss 0.13850608468055725\n",
      "This is the loss 0.1387382298707962\n",
      "This is the loss 0.11931432038545609\n",
      "This is the loss 0.13723023235797882\n",
      "This is the loss 0.14540694653987885\n",
      "This is the loss 0.1267700493335724\n",
      "This is the loss 0.11942523717880249\n",
      "This is the loss 0.08577317744493484\n",
      "This is the loss 0.13753528892993927\n",
      "This is the loss 0.1283571422100067\n",
      "This is the loss 0.15452726185321808\n",
      "This is the loss 0.11390476673841476\n",
      "This is the loss 0.14156408607959747\n",
      "This is the loss 0.14484088122844696\n",
      "This is the loss 0.10686350613832474\n",
      "This is the loss 0.1190812736749649\n",
      "This is the loss 0.1529720276594162\n",
      "This is the loss 0.15857376158237457\n",
      "This is the loss 0.16887104511260986\n",
      "This is the loss 0.14445407688617706\n",
      "This is the loss 0.14840655028820038\n",
      "This is the loss 0.0869031697511673\n",
      "This is the loss 0.15181517601013184\n",
      "This is the loss 0.0993248000741005\n",
      "This is the loss 0.1204293891787529\n",
      "This is the loss 0.13298435509204865\n",
      "This is the loss 0.12489967048168182\n",
      "This is the loss 0.13557487726211548\n",
      "This is the loss 0.13726665079593658\n",
      "This is the loss 0.1495833694934845\n",
      "This is the loss 0.12064098566770554\n",
      "This is the loss 0.14092205464839935\n",
      "This is the loss 0.1317417025566101\n",
      "This is the loss 0.13603632152080536\n",
      "This is the loss 0.16341163218021393\n",
      "This is the loss 0.1623411923646927\n",
      "This is the loss 0.13848941028118134\n",
      "This is the loss 0.16236133873462677\n",
      "This is the loss 0.15243029594421387\n",
      "This is the loss 0.14736399054527283\n",
      "This is the loss 0.14185670018196106\n",
      "This is the loss 0.16974885761737823\n",
      "This is the loss 0.1546916663646698\n",
      "This is the loss 0.1702950894832611\n",
      "This is the loss 0.13974374532699585\n",
      "This is the loss 0.1420620232820511\n",
      "This is the loss 0.14694832265377045\n",
      "This is the loss 0.18077439069747925\n",
      "This is the loss 0.1570669412612915\n",
      "This is the loss 0.1695757806301117\n",
      "This is the loss 0.1275552660226822\n",
      "This is the loss 0.1067834198474884\n",
      "This is the loss 0.11030159890651703\n",
      "This is the loss 0.14193780720233917\n",
      "This is the loss 0.14508706331253052\n",
      "This is the loss 0.13999633491039276\n",
      "This is the loss 0.11890161782503128\n",
      "This is the loss 0.1023845225572586\n",
      "This is the loss 0.14127714931964874\n",
      "This is the loss 0.12417591363191605\n",
      "This is the loss 0.12305139005184174\n",
      "This is the loss 0.12139229476451874\n",
      "This is the loss 0.12402743846178055\n",
      "This is the loss 0.11085256934165955\n",
      "This is the loss 0.147044375538826\n",
      "This is the loss 0.11546148359775543\n",
      "This is the loss 0.16242621839046478\n",
      "This is the loss 0.1308448612689972\n",
      "This is the loss 0.1325405240058899\n",
      "This is the loss 0.11527473479509354\n",
      "This is the loss 0.12922117114067078\n",
      "This is the loss 0.16203907132148743\n",
      "This is the loss 0.16381724178791046\n",
      "This is the loss 0.15159565210342407\n",
      "This is the loss 0.11514037847518921\n",
      "This is the loss 0.15446610748767853\n",
      "This is the loss 0.16576702892780304\n",
      "This is the loss 0.1434248387813568\n",
      "This is the loss 0.15430563688278198\n",
      "This is the loss 0.11808229982852936\n",
      "This is the loss 0.14125430583953857\n",
      "This is the loss 0.11525379121303558\n",
      "This is the loss 0.16886617243289948\n",
      "This is the loss 0.16792020201683044\n",
      "This is the loss 0.15351620316505432\n",
      "This is the loss 0.13409708440303802\n",
      "This is the loss 0.1338147222995758\n",
      "This is the loss 0.12412603944540024\n",
      "This is the loss 0.13700741529464722\n",
      "This is the loss 0.1184888407588005\n",
      "This is the loss 0.1384444385766983\n",
      "This is the loss 0.11669465154409409\n",
      "This is the loss 0.0816514790058136\n",
      "This is the loss 0.11196062713861465\n",
      "This is the loss 0.12800489366054535\n",
      "This is the loss 0.14923159778118134\n",
      "This is the loss 0.1290443390607834\n",
      "This is the loss 0.12053884565830231\n",
      "This is the loss 0.13309012353420258\n",
      "This is the loss 0.13461369276046753\n",
      "This is the loss 0.1366521716117859\n",
      "This is the loss 0.13171400129795074\n",
      "This is the loss 0.13127674162387848\n",
      "This is the loss 0.1360517293214798\n",
      "This is the loss 0.13786573708057404\n",
      "This is the loss 0.12271874397993088\n",
      "This is the loss 0.1499076783657074\n",
      "This is the loss 0.1599528193473816\n",
      "This is the loss 0.14914311468601227\n",
      "This is the loss 0.14414744079113007\n",
      "This is the loss 0.14332100749015808\n",
      "This is the loss 0.14926040172576904\n",
      "This is the loss 0.16954562067985535\n",
      "This is the loss 0.16920462250709534\n",
      "This is the loss 0.1414303332567215\n",
      "This is the loss 0.16608327627182007\n",
      "This is the loss 0.16530431807041168\n",
      "This is the loss 0.14890262484550476\n",
      "This is the loss 0.16395726799964905\n",
      "This is the loss 0.13807331025600433\n",
      "This is the loss 0.1147727519273758\n",
      "This is the loss 0.12846893072128296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.12034828215837479\n",
      "This is the loss 0.16022081673145294\n",
      "This is the loss 0.16021619737148285\n",
      "This is the loss 0.13782666623592377\n",
      "This is the loss 0.18545843660831451\n",
      "This is the loss 0.1312226504087448\n",
      "This is the loss 0.16505968570709229\n",
      "This is the loss 0.1412489265203476\n",
      "This is the loss 0.193211168050766\n",
      "This is the loss 0.1501447558403015\n",
      "This is the loss 0.13551990687847137\n",
      "This is the loss 0.14963899552822113\n",
      "This is the loss 0.15754751861095428\n",
      "This is the loss 0.14089033007621765\n",
      "This is the loss 0.15148980915546417\n",
      "This is the loss 0.144535094499588\n",
      "This is the loss 0.13701291382312775\n",
      "This is the loss 0.14539414644241333\n",
      "This is the loss 0.12889687716960907\n",
      "This is the loss 0.12766426801681519\n",
      "This is the loss 0.14871062338352203\n",
      "This is the loss 0.15446437895298004\n",
      "This is the loss 0.12580712139606476\n",
      "This is the loss 0.1401216983795166\n",
      "This is the loss 0.11973195523023605\n",
      "This is the loss 0.09991481900215149\n",
      "This is the loss 0.12103928625583649\n",
      "This is the loss 0.12837737798690796\n",
      "This is the loss 0.14088036119937897\n",
      "This is the loss 0.12051450461149216\n",
      "This is the loss 0.15473102033138275\n",
      "This is the loss 0.13202443718910217\n",
      "This is the loss 0.12545426189899445\n",
      "This is the loss 0.15000829100608826\n",
      "This is the loss 0.10957250744104385\n",
      "This is the loss 0.11551855504512787\n",
      "This is the loss 0.13424739241600037\n",
      "This is the loss 0.12316527962684631\n",
      "This is the loss 0.1591872274875641\n",
      "This is the loss 0.15217390656471252\n",
      "This is the loss 0.14981487393379211\n",
      "This is the loss 0.14192064106464386\n",
      "This is the loss 0.12286298722028732\n",
      "This is the loss 0.13928787410259247\n",
      "This is the loss 0.13147416710853577\n",
      "This is the loss 0.1281605362892151\n",
      "This is the loss 0.15042178332805634\n",
      "This is the loss 0.15655484795570374\n",
      "This is the loss 0.15369118750095367\n",
      "This is the loss 0.11626406013965607\n",
      "This is the loss 0.12395890057086945\n",
      "This is the loss 0.13110215961933136\n",
      "This is the loss 0.15852472186088562\n",
      "This is the loss 0.1835712045431137\n",
      "This is the loss 0.12323097884654999\n",
      "This is the loss 0.12654849886894226\n",
      "This is the loss 0.14213228225708008\n",
      "This is the loss 0.13153252005577087\n",
      "This is the loss 0.12611877918243408\n",
      "This is the loss 0.12706755101680756\n",
      "This is the loss 0.16570337116718292\n",
      "This is the loss 0.12774774432182312\n",
      "This is the loss 0.14560918509960175\n",
      "This is the loss 0.1171862781047821\n",
      "This is the loss 0.1192164197564125\n",
      "This is the loss 0.10611134767532349\n",
      "This is the loss 0.14203675091266632\n",
      "This is the loss 0.14600610733032227\n",
      "This is the loss 0.15687090158462524\n",
      "This is the loss 0.13820713758468628\n",
      "This is the loss 0.12898844480514526\n",
      "This is the loss 0.12317906320095062\n",
      "This is the loss 0.12124919891357422\n",
      "This is the loss 0.10802160948514938\n",
      "This is the loss 0.12946456670761108\n",
      "This is the loss 0.12495347112417221\n",
      "This is the loss 0.13751772046089172\n",
      "This is the loss 0.10012182593345642\n",
      "This is the loss 0.118071548640728\n",
      "This is the loss 0.10878045111894608\n",
      "This is the loss 0.13561871647834778\n",
      "This is the loss 0.13461443781852722\n",
      "This is the loss 0.13720016181468964\n",
      "This is the loss 0.1666233241558075\n",
      "This is the loss 0.16263774037361145\n",
      "This is the loss 0.16174376010894775\n",
      "This is the loss 0.11733319610357285\n",
      "This is the loss 0.1323668211698532\n",
      "This is the loss 0.11999919265508652\n",
      "This is the loss 0.1316462606191635\n",
      "This is the loss 0.11654730141162872\n",
      "This is the loss 0.14947383105754852\n",
      "This is the loss 0.1532297134399414\n",
      "This is the loss 0.1469152420759201\n",
      "This is the loss 0.11081544309854507\n",
      "This is the loss 0.13522475957870483\n",
      "This is the loss 0.1337549239397049\n",
      "This is the loss 0.1364406943321228\n",
      "This is the loss 0.16273154318332672\n",
      "This is the loss 0.1486503779888153\n",
      "This is the loss 0.13547885417938232\n",
      "This is the loss 0.12746301293373108\n",
      "This is the loss 0.12461927533149719\n",
      "This is the loss 0.13780784606933594\n",
      "This is the loss 0.1634116768836975\n",
      "This is the loss 0.15096625685691833\n",
      "This is the loss 0.1666339933872223\n",
      "This is the loss 0.17682026326656342\n",
      "This is the loss 0.1202399879693985\n",
      "This is the loss 0.16034027934074402\n",
      "This is the loss 0.11839526891708374\n",
      "This is the loss 0.15195047855377197\n",
      "This is the loss 0.13419398665428162\n",
      "This is the loss 0.12540310621261597\n",
      "This is the loss 0.15014955401420593\n",
      "This is the loss 0.12439664453268051\n",
      "This is the loss 0.15925638377666473\n",
      "This is the loss 0.12989117205142975\n",
      "This is the loss 0.1233314573764801\n",
      "This is the loss 0.15584713220596313\n",
      "This is the loss 0.11023366451263428\n",
      "This is the loss 0.12728829681873322\n",
      "This is the loss 0.11296489089727402\n",
      "This is the loss 0.12732012569904327\n",
      "This is the loss 0.1585981696844101\n",
      "This is the loss 0.1280721127986908\n",
      "This is the loss 0.13441359996795654\n",
      "This is the loss 0.12842980027198792\n",
      "This is the loss 0.09749267995357513\n",
      "This is the loss 0.12521538138389587\n",
      "This is the loss 0.11479195952415466\n",
      "This is the loss 0.10338353365659714\n",
      "This is the loss 0.14145220816135406\n",
      "This is the loss 0.1264246553182602\n",
      "This is the loss 0.09731987118721008\n",
      "This is the loss 0.12285394966602325\n",
      "This is the loss 0.14035619795322418\n",
      "This is the loss 0.14615470170974731\n",
      "This is the loss 0.12276383489370346\n",
      "This is the loss 0.11643125861883163\n",
      "This is the loss 0.12966111302375793\n",
      "This is the loss 0.11867909878492355\n",
      "This is the loss 0.12037406861782074\n",
      "This is the loss 0.15573379397392273\n",
      "This is the loss 0.1619415581226349\n",
      "This is the loss 0.1303495317697525\n",
      "This is the loss 0.15155641734600067\n",
      "This is the loss 0.13351112604141235\n",
      "This is the loss 0.14142343401908875\n",
      "This is the loss 0.13587705790996552\n",
      "This is the loss 0.12445957958698273\n",
      "This is the loss 0.16696734726428986\n",
      "This is the loss 0.1252424567937851\n",
      "This is the loss 0.10812419652938843\n",
      "This is the loss 0.1415306180715561\n",
      "This is the loss 0.1361095905303955\n",
      "This is the loss 0.1558297723531723\n",
      "This is the loss 0.16236352920532227\n",
      "This is the loss 0.15195173025131226\n",
      "This is the loss 0.12737827003002167\n",
      "This is the loss 0.15114057064056396\n",
      "This is the loss 0.1440879851579666\n",
      "This is the loss 0.148997962474823\n",
      "This is the loss 0.1713046133518219\n",
      "This is the loss 0.14189670979976654\n",
      "This is the loss 0.13712003827095032\n",
      "This is the loss 0.14426326751708984\n",
      "This is the loss 0.13832899928092957\n",
      "This is the loss 0.169498011469841\n",
      "This is the loss 0.13478121161460876\n",
      "This is the loss 0.14619919657707214\n",
      "This is the loss 0.12644916772842407\n",
      "This is the loss 0.14903438091278076\n",
      "This is the loss 0.14621470868587494\n",
      "This is the loss 0.1456145942211151\n",
      "This is the loss 0.13721051812171936\n",
      "This is the loss 0.16213324666023254\n",
      "This is the loss 0.1208607628941536\n",
      "This is the loss 0.1505557745695114\n",
      "This is the loss 0.15734624862670898\n",
      "This is the loss 0.15574133396148682\n",
      "This is the loss 0.144467294216156\n",
      "This is the loss 0.1416316032409668\n",
      "This is the loss 0.14443418383598328\n",
      "This is the loss 0.1630527824163437\n",
      "This is the loss 0.1358684003353119\n",
      "This is the loss 0.12619929015636444\n",
      "This is the loss 0.14948436617851257\n",
      "This is the loss 0.17038072645664215\n",
      "This is the loss 0.1636229008436203\n",
      "This is the loss 0.14327429234981537\n",
      "This is the loss 0.1465449333190918\n",
      "This is the loss 0.12667077779769897\n",
      "This is the loss 0.15086615085601807\n",
      "This is the loss 0.13939586281776428\n",
      "This is the loss 0.17748184502124786\n",
      "This is the loss 0.18267621099948883\n",
      "This is the loss 0.15016813576221466\n",
      "This is the loss 0.14011134207248688\n",
      "This is the loss 0.13412365317344666\n",
      "This is the loss 0.1111842691898346\n",
      "This is the loss 0.1670730859041214\n",
      "This is the loss 0.13046754896640778\n",
      "This is the loss 0.15041513741016388\n",
      "This is the loss 0.19214899837970734\n",
      "This is the loss 0.13571125268936157\n",
      "This is the loss 0.16669736802577972\n",
      "This is the loss 0.12530037760734558\n",
      "This is the loss 0.11706104129552841\n",
      "This is the loss 0.17134445905685425\n",
      "This is the loss 0.13931424915790558\n",
      "This is the loss 0.16824842989444733\n",
      "This is the loss 0.1599457859992981\n",
      "This is the loss 0.1524982750415802\n",
      "This is the loss 0.15726226568222046\n",
      "This is the loss 0.16831891238689423\n",
      "This is the loss 0.15288466215133667\n",
      "This is the loss 0.12296201288700104\n",
      "This is the loss 0.18776565790176392\n",
      "This is the loss 0.15076695382595062\n",
      "This is the loss 0.13627690076828003\n",
      "This is the loss 0.1366809606552124\n",
      "This is the loss 0.1457194834947586\n",
      "This is the loss 0.1570102572441101\n",
      "This is the loss 0.15095186233520508\n",
      "This is the loss 0.13591952621936798\n",
      "This is the loss 0.16194376349449158\n",
      "This is the loss 0.13763874769210815\n",
      "This is the loss 0.12366317212581635\n",
      "This is the loss 0.13554318249225616\n",
      "This is the loss 0.09720494598150253\n",
      "This is the loss 0.1372535079717636\n",
      "This is the loss 0.1393071711063385\n",
      "This is the loss 0.16313669085502625\n",
      "This is the loss 0.1426001638174057\n",
      "This is the loss 0.1461312174797058\n",
      "This is the loss 0.15618513524532318\n",
      "This is the loss 0.161729633808136\n",
      "This is the loss 0.17539750039577484\n",
      "This is the loss 0.15521299839019775\n",
      "This is the loss 0.16131076216697693\n",
      "This is the loss 0.1623145490884781\n",
      "This is the loss 0.18155629932880402\n",
      "This is the loss 0.12076152861118317\n",
      "This is the loss 0.14559046924114227\n",
      "This is the loss 0.15545456111431122\n",
      "This is the loss 0.1812586635351181\n",
      "This is the loss 0.1527298092842102\n",
      "This is the loss 0.14012134075164795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.18602192401885986\n",
      "This is the loss 0.17990198731422424\n",
      "This is the loss 0.12661072611808777\n",
      "This is the loss 0.16049334406852722\n",
      "This is the loss 0.15350544452667236\n",
      "This is the loss 0.14626333117485046\n",
      "This is the loss 0.14216846227645874\n",
      "This is the loss 0.14731696248054504\n",
      "This is the loss 0.12702111899852753\n",
      "This is the loss 0.15342147648334503\n",
      "This is the loss 0.12118234485387802\n",
      "This is the loss 0.13176387548446655\n",
      "This is the loss 0.13373391330242157\n",
      "This is the loss 0.15807640552520752\n",
      "This is the loss 0.1727910339832306\n",
      "This is the loss 0.1714133471250534\n",
      "This is the loss 0.14015962183475494\n",
      "This is the loss 0.13164082169532776\n",
      "This is the loss 0.12351541221141815\n",
      "This is the loss 0.11824902892112732\n",
      "This is the loss 0.12237118184566498\n",
      "This is the loss 0.11851242929697037\n",
      "This is the loss 0.15623894333839417\n",
      "This is the loss 0.1406419724225998\n",
      "This is the loss 0.1632409244775772\n",
      "This is the loss 0.1377234011888504\n",
      "This is the loss 0.11616162955760956\n",
      "This is the loss 0.13495634496212006\n",
      "This is the loss 0.12237117439508438\n",
      "This is the loss 0.1409584879875183\n",
      "This is the loss 0.1467827707529068\n",
      "This is the loss 0.15871299803256989\n",
      "This is the loss 0.15179048478603363\n",
      "This is the loss 0.13974040746688843\n",
      "This is the loss 0.14974090456962585\n",
      "This is the loss 0.14348401129245758\n",
      "This is the loss 0.14790983498096466\n",
      "This is the loss 0.15094608068466187\n",
      "This is the loss 0.11761817336082458\n",
      "This is the loss 0.1590462625026703\n",
      "This is the loss 0.13358235359191895\n",
      "This is the loss 0.12633301317691803\n",
      "This is the loss 0.13639040291309357\n",
      "This is the loss 0.14108619093894958\n",
      "This is the loss 0.16043290495872498\n",
      "This is the loss 0.1293608844280243\n",
      "This is the loss 0.14442980289459229\n",
      "This is the loss 0.13506338000297546\n",
      "This is the loss 0.12479346245527267\n",
      "This is the loss 0.15723051130771637\n",
      "This is the loss 0.1386428326368332\n",
      "This is the loss 0.14660784602165222\n",
      "This is the loss 0.11924231797456741\n",
      "This is the loss 0.11299546808004379\n",
      "This is the loss 0.16169576346874237\n",
      "This is the loss 0.14317212998867035\n",
      "This is the loss 0.11388501524925232\n",
      "This is the loss 0.13514843583106995\n",
      "This is the loss 0.1449277102947235\n",
      "This is the loss 0.12017758935689926\n",
      "This is the loss 0.1492547243833542\n",
      "This is the loss 0.10411237180233002\n",
      "This is the loss 0.1516418755054474\n",
      "This is the loss 0.14769618213176727\n",
      "This is the loss 0.13119475543498993\n",
      "This is the loss 0.13171042501926422\n",
      "This is the loss 0.12295413017272949\n",
      "This is the loss 0.16215284168720245\n",
      "This is the loss 0.12303006649017334\n",
      "This is the loss 0.15940400958061218\n",
      "This is the loss 0.1225714385509491\n",
      "This is the loss 0.12748251855373383\n",
      "This is the loss 0.15984587371349335\n",
      "This is the loss 0.14315149188041687\n",
      "This is the loss 0.13018447160720825\n",
      "This is the loss 0.13684794306755066\n",
      "This is the loss 0.136628195643425\n",
      "This is the loss 0.12820462882518768\n",
      "This is the loss 0.16212716698646545\n",
      "This is the loss 0.11866568773984909\n",
      "This is the loss 0.1650637686252594\n",
      "This is the loss 0.13046810030937195\n",
      "This is the loss 0.1158970445394516\n",
      "This is the loss 0.13113842904567719\n",
      "This is the loss 0.14611439406871796\n",
      "This is the loss 0.13522115349769592\n",
      "This is the loss 0.16566425561904907\n",
      "This is the loss 0.14189782738685608\n",
      "This is the loss 0.1222570464015007\n",
      "This is the loss 0.12278090417385101\n",
      "This is the loss 0.1610507369041443\n",
      "This is the loss 0.1425887942314148\n",
      "This is the loss 0.12079387903213501\n",
      "This is the loss 0.12321065366268158\n",
      "This is the loss 0.15537041425704956\n",
      "This is the loss 0.1548377126455307\n",
      "This is the loss 0.13352297246456146\n",
      "This is the loss 0.1370910406112671\n",
      "This is the loss 0.1267179697751999\n",
      "This is the loss 0.14434479176998138\n",
      "This is the loss 0.1578117161989212\n",
      "This is the loss 0.11232821643352509\n",
      "This is the loss 0.10861328989267349\n",
      "This is the loss 0.13674336671829224\n",
      "This is the loss 0.1750633865594864\n",
      "This is the loss 0.14251424372196198\n",
      "This is the loss 0.16457542777061462\n",
      "This is the loss 0.17602524161338806\n",
      "This is the loss 0.17817986011505127\n",
      "This is the loss 0.14420157670974731\n",
      "This is the loss 0.14647023379802704\n",
      "This is the loss 0.13170620799064636\n",
      "This is the loss 0.12263383716344833\n",
      "This is the loss 0.12011681497097015\n",
      "This is the loss 0.13758035004138947\n",
      "This is the loss 0.12871558964252472\n",
      "This is the loss 0.13436929881572723\n",
      "This is the loss 0.12849843502044678\n",
      "This is the loss 0.1227598637342453\n",
      "This is the loss 0.1507381945848465\n",
      "This is the loss 0.14680145680904388\n",
      "This is the loss 0.16960802674293518\n",
      "This is the loss 0.1178968995809555\n",
      "This is the loss 0.13021764159202576\n",
      "This is the loss 0.15094859898090363\n",
      "This is the loss 0.11551201343536377\n",
      "This is the loss 0.14131119847297668\n",
      "This is the loss 0.15130643546581268\n",
      "This is the loss 0.1216680184006691\n",
      "This is the loss 0.15688471496105194\n",
      "This is the loss 0.11559897661209106\n",
      "This is the loss 0.14883680641651154\n",
      "This is the loss 0.12669174373149872\n",
      "This is the loss 0.15227475762367249\n",
      "This is the loss 0.12534934282302856\n",
      "This is the loss 0.1506548672914505\n",
      "This is the loss 0.11935072392225266\n",
      "This is the loss 0.12953995168209076\n",
      "This is the loss 0.14675076305866241\n",
      "This is the loss 0.12772488594055176\n",
      "This is the loss 0.13340505957603455\n",
      "This is the loss 0.09882761538028717\n",
      "This is the loss 0.14112992584705353\n",
      "This is the loss 0.13126611709594727\n",
      "This is the loss 0.11466207355260849\n",
      "This is the loss 0.14268173277378082\n",
      "This is the loss 0.1512567698955536\n",
      "This is the loss 0.14679381251335144\n",
      "This is the loss 0.13893575966358185\n",
      "This is the loss 0.1621425598859787\n",
      "This is the loss 0.10059396922588348\n",
      "This is the loss 0.16923201084136963\n",
      "This is the loss 0.11193712055683136\n",
      "This is the loss 0.1357814073562622\n",
      "This is the loss 0.1330380141735077\n",
      "This is the loss 0.1371230185031891\n",
      "This is the loss 0.1344083547592163\n",
      "This is the loss 0.1243622824549675\n",
      "This is the loss 0.11919496208429337\n",
      "This is the loss 0.13680455088615417\n",
      "This is the loss 0.1515822410583496\n",
      "This is the loss 0.14687250554561615\n",
      "This is the loss 0.12686625123023987\n",
      "This is the loss 0.12453737109899521\n",
      "This is the loss 0.11349929869174957\n",
      "This is the loss 0.14930476248264313\n",
      "This is the loss 0.1612045019865036\n",
      "This is the loss 0.12538592517375946\n",
      "This is the loss 0.1199774444103241\n",
      "This is the loss 0.13965265452861786\n",
      "This is the loss 0.14705367386341095\n",
      "This is the loss 0.13353395462036133\n",
      "This is the loss 0.14460009336471558\n",
      "This is the loss 0.15031084418296814\n",
      "This is the loss 0.16996146738529205\n",
      "This is the loss 0.16468529403209686\n",
      "This is the loss 0.15349623560905457\n",
      "This is the loss 0.13316647708415985\n",
      "This is the loss 0.16044224798679352\n",
      "This is the loss 0.14988653361797333\n",
      "This is the loss 0.1274605393409729\n",
      "This is the loss 0.13110148906707764\n",
      "This is the loss 0.14715977013111115\n",
      "This is the loss 0.14619721472263336\n",
      "This is the loss 0.13986153900623322\n",
      "This is the loss 0.1489919126033783\n",
      "This is the loss 0.14080971479415894\n",
      "This is the loss 0.12701089680194855\n",
      "This is the loss 0.13931739330291748\n",
      "This is the loss 0.1422232985496521\n",
      "This is the loss 0.1340208649635315\n",
      "This is the loss 0.15272833406925201\n",
      "This is the loss 0.12342245131731033\n",
      "This is the loss 0.1243072897195816\n",
      "This is the loss 0.16079947352409363\n",
      "This is the loss 0.14532950520515442\n",
      "This is the loss 0.15993063151836395\n",
      "This is the loss 0.11228242516517639\n",
      "This is the loss 0.10694272816181183\n",
      "This is the loss 0.14357425272464752\n",
      "This is the loss 0.11204558610916138\n",
      "This is the loss 0.11487174779176712\n",
      "This is the loss 0.1421198844909668\n",
      "This is the loss 0.1355302780866623\n",
      "This is the loss 0.13145238161087036\n",
      "This is the loss 0.12879464030265808\n",
      "This is the loss 0.16140016913414001\n",
      "This is the loss 0.1526722013950348\n",
      "This is the loss 0.13944430649280548\n",
      "This is the loss 0.1326567530632019\n",
      "This is the loss 0.14423856139183044\n",
      "This is the loss 0.13945919275283813\n",
      "This is the loss 0.1514732390642166\n",
      "This is the loss 0.11534658074378967\n",
      "This is the loss 0.15571242570877075\n",
      "This is the loss 0.1641315519809723\n",
      "This is the loss 0.1555946320295334\n",
      "This is the loss 0.10739552229642868\n",
      "This is the loss 0.1341305375099182\n",
      "This is the loss 0.0892922654747963\n",
      "This is the loss 0.14395907521247864\n",
      "This is the loss 0.13356325030326843\n",
      "This is the loss 0.1735907942056656\n",
      "This is the loss 0.10847028344869614\n",
      "This is the loss 0.1146775484085083\n",
      "This is the loss 0.13040292263031006\n",
      "This is the loss 0.12642809748649597\n",
      "This is the loss 0.16001766920089722\n",
      "This is the loss 0.13294881582260132\n",
      "This is the loss 0.11519256234169006\n",
      "This is the loss 0.16244535148143768\n",
      "This is the loss 0.13031363487243652\n",
      "This is the loss 0.15653052926063538\n",
      "This is the loss 0.11913122236728668\n",
      "This is the loss 0.14516271650791168\n",
      "This is the loss 0.14150485396385193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.13112621009349823\n",
      "This is the loss 0.1548861712217331\n",
      "This is the loss 0.12756232917308807\n",
      "This is the loss 0.15621916949748993\n",
      "This is the loss 0.13720302283763885\n",
      "This is the loss 0.11348246037960052\n",
      "This is the loss 0.13147787749767303\n",
      "This is the loss 0.10462500900030136\n",
      "This is the loss 0.11077750474214554\n",
      "This is the loss 0.15538610517978668\n",
      "This is the loss 0.1322948932647705\n",
      "This is the loss 0.13514597713947296\n",
      "This is the loss 0.11326221376657486\n",
      "This is the loss 0.12149501591920853\n",
      "This is the loss 0.14331018924713135\n",
      "This is the loss 0.1356056183576584\n",
      "This is the loss 0.13062863051891327\n",
      "This is the loss 0.17145869135856628\n",
      "This is the loss 0.15057465434074402\n",
      "This is the loss 0.13263146579265594\n",
      "This is the loss 0.10975588858127594\n",
      "This is the loss 0.13548564910888672\n",
      "This is the loss 0.14208652079105377\n",
      "This is the loss 0.1425759345293045\n",
      "This is the loss 0.11248452216386795\n",
      "This is the loss 0.1447024643421173\n",
      "This is the loss 0.12783943116664886\n",
      "This is the loss 0.11591192334890366\n",
      "This is the loss 0.1279674470424652\n",
      "This is the loss 0.12856566905975342\n",
      "This is the loss 0.13424424827098846\n",
      "This is the loss 0.13306261599063873\n",
      "This is the loss 0.1294173002243042\n",
      "This is the loss 0.1679094433784485\n",
      "This is the loss 0.15019875764846802\n",
      "This is the loss 0.15058806538581848\n",
      "This is the loss 0.1437879055738449\n",
      "This is the loss 0.11987142264842987\n",
      "This is the loss 0.14382269978523254\n",
      "This is the loss 0.12329550087451935\n",
      "This is the loss 0.1400856226682663\n",
      "This is the loss 0.15206637978553772\n",
      "This is the loss 0.12076184898614883\n",
      "This is the loss 0.14620888233184814\n",
      "This is the loss 0.14105936884880066\n",
      "This is the loss 0.13781653344631195\n",
      "This is the loss 0.14530333876609802\n",
      "This is the loss 0.1507851928472519\n",
      "This is the loss 0.15535588562488556\n",
      "This is the loss 0.1402251422405243\n",
      "This is the loss 0.14504767954349518\n",
      "This is the loss 0.1408536285161972\n",
      "This is the loss 0.144902765750885\n",
      "This is the loss 0.12417110800743103\n",
      "This is the loss 0.148829847574234\n",
      "This is the loss 0.1578880250453949\n",
      "This is the loss 0.1063956469297409\n",
      "This is the loss 0.1299414187669754\n",
      "This is the loss 0.12081064283847809\n",
      "This is the loss 0.1498485803604126\n",
      "This is the loss 0.13807691633701324\n",
      "This is the loss 0.1351739466190338\n",
      "This is the loss 0.12913820147514343\n",
      "This is the loss 0.1156025156378746\n",
      "This is the loss 0.14630818367004395\n",
      "This is the loss 0.11705288290977478\n",
      "This is the loss 0.11444465816020966\n",
      "This is the loss 0.11246244609355927\n",
      "This is the loss 0.122978076338768\n",
      "This is the loss 0.1269647479057312\n",
      "This is the loss 0.10948346555233002\n",
      "This is the loss 0.14656783640384674\n",
      "This is the loss 0.1200542077422142\n",
      "This is the loss 0.14056825637817383\n",
      "This is the loss 0.12567944824695587\n",
      "This is the loss 0.11570218205451965\n",
      "This is the loss 0.11975283920764923\n",
      "This is the loss 0.1654619723558426\n",
      "This is the loss 0.13152039051055908\n",
      "This is the loss 0.16791783273220062\n",
      "This is the loss 0.1526927649974823\n",
      "This is the loss 0.1662674844264984\n",
      "This is the loss 0.11772806197404861\n",
      "This is the loss 0.12702204287052155\n",
      "This is the loss 0.09039188921451569\n",
      "This is the loss 0.11998176574707031\n",
      "This is the loss 0.15968699753284454\n",
      "This is the loss 0.10574575513601303\n",
      "This is the loss 0.14695186913013458\n",
      "This is the loss 0.11955849081277847\n",
      "This is the loss 0.1245371475815773\n",
      "This is the loss 0.14062586426734924\n",
      "This is the loss 0.1477876454591751\n",
      "This is the loss 0.12070814520120621\n",
      "This is the loss 0.14040318131446838\n",
      "This is the loss 0.15264010429382324\n",
      "This is the loss 0.13290207087993622\n",
      "This is the loss 0.1220378428697586\n",
      "This is the loss 0.11916998028755188\n",
      "This is the loss 0.13717645406723022\n",
      "This is the loss 0.14074406027793884\n",
      "This is the loss 0.14169639348983765\n",
      "This is the loss 0.17645786702632904\n",
      "This is the loss 0.141706183552742\n",
      "This is the loss 0.1433888077735901\n",
      "This is the loss 0.16149574518203735\n",
      "This is the loss 0.12928518652915955\n",
      "This is the loss 0.16055944561958313\n",
      "This is the loss 0.13835367560386658\n",
      "This is the loss 0.15111014246940613\n",
      "This is the loss 0.1346322000026703\n",
      "This is the loss 0.1828605979681015\n",
      "This is the loss 0.14991022646427155\n",
      "This is the loss 0.1363012045621872\n",
      "This is the loss 0.12400452792644501\n",
      "This is the loss 0.14365527033805847\n",
      "This is the loss 0.12723156809806824\n",
      "This is the loss 0.11585252732038498\n",
      "This is the loss 0.12917698919773102\n",
      "This is the loss 0.1114770844578743\n",
      "This is the loss 0.13955974578857422\n",
      "This is the loss 0.1471593677997589\n",
      "This is the loss 0.15737250447273254\n",
      "This is the loss 0.14849980175495148\n",
      "This is the loss 0.11626853793859482\n",
      "This is the loss 0.14898882806301117\n",
      "This is the loss 0.1285371482372284\n",
      "This is the loss 0.1336648166179657\n",
      "This is the loss 0.12589913606643677\n",
      "This is the loss 0.12855909764766693\n",
      "This is the loss 0.13508912920951843\n",
      "This is the loss 0.1073174774646759\n",
      "This is the loss 0.12228322774171829\n",
      "This is the loss 0.14223304390907288\n",
      "This is the loss 0.1423182189464569\n",
      "This is the loss 0.10331625491380692\n",
      "This is the loss 0.14263656735420227\n",
      "This is the loss 0.1149127259850502\n",
      "This is the loss 0.1330324411392212\n",
      "This is the loss 0.14269588887691498\n",
      "This is the loss 0.15192581713199615\n",
      "This is the loss 0.15397411584854126\n",
      "This is the loss 0.13158556818962097\n",
      "This is the loss 0.15646307170391083\n",
      "This is the loss 0.15915252268314362\n",
      "This is the loss 0.11950603127479553\n",
      "This is the loss 0.10333151370286942\n",
      "This is the loss 0.11656169593334198\n",
      "This is the loss 0.1230761706829071\n",
      "This is the loss 0.11983775347471237\n",
      "This is the loss 0.13736014068126678\n",
      "This is the loss 0.12383913993835449\n",
      "This is the loss 0.12794426083564758\n",
      "This is the loss 0.13273563981056213\n",
      "This is the loss 0.13518120348453522\n",
      "This is the loss 0.14145530760288239\n",
      "This is the loss 0.12894678115844727\n",
      "This is the loss 0.117073193192482\n",
      "This is the loss 0.12391502410173416\n",
      "This is the loss 0.16932690143585205\n",
      "This is the loss 0.13764815032482147\n",
      "This is the loss 0.13290265202522278\n",
      "This is the loss 0.13542643189430237\n",
      "This is the loss 0.11081341654062271\n",
      "This is the loss 0.13424605131149292\n",
      "This is the loss 0.1386098712682724\n",
      "This is the loss 0.13531142473220825\n",
      "This is the loss 0.147293820977211\n",
      "This is the loss 0.12034955620765686\n",
      "This is the loss 0.13959044218063354\n",
      "This is the loss 0.1286371499300003\n",
      "This is the loss 0.1749207228422165\n",
      "This is the loss 0.12290320545434952\n",
      "This is the loss 0.12017660588026047\n",
      "This is the loss 0.14747285842895508\n",
      "This is the loss 0.14689496159553528\n",
      "This is the loss 0.12912115454673767\n",
      "This is the loss 0.15275165438652039\n",
      "This is the loss 0.14285632967948914\n",
      "This is the loss 0.17022350430488586\n",
      "This is the loss 0.12739326059818268\n",
      "This is the loss 0.11996448785066605\n",
      "This is the loss 0.12938201427459717\n",
      "This is the loss 0.15181246399879456\n",
      "This is the loss 0.1389826238155365\n",
      "This is the loss 0.12954087555408478\n",
      "This is the loss 0.13660122454166412\n",
      "This is the loss 0.12747503817081451\n",
      "This is the loss 0.09503284096717834\n",
      "This is the loss 0.13463418185710907\n",
      "This is the loss 0.10499945282936096\n",
      "This is the loss 0.12044108659029007\n",
      "This is the loss 0.1295062005519867\n",
      "This is the loss 0.16375966370105743\n",
      "This is the loss 0.14593009650707245\n",
      "This is the loss 0.14984393119812012\n",
      "This is the loss 0.1390729546546936\n",
      "This is the loss 0.16515955328941345\n",
      "This is the loss 0.1373509168624878\n",
      "This is the loss 0.15436890721321106\n",
      "This is the loss 0.15605910122394562\n",
      "This is the loss 0.13755753636360168\n",
      "This is the loss 0.13562999665737152\n",
      "This is the loss 0.14168590307235718\n",
      "This is the loss 0.1440725326538086\n",
      "This is the loss 0.13662020862102509\n",
      "This is the loss 0.14496344327926636\n",
      "This is the loss 0.154682457447052\n",
      "This is the loss 0.15375107526779175\n",
      "This is the loss 0.12031545490026474\n",
      "This is the loss 0.1486438512802124\n",
      "This is the loss 0.13878120481967926\n",
      "This is the loss 0.1277039349079132\n",
      "This is the loss 0.1403663158416748\n",
      "This is the loss 0.11138793081045151\n",
      "This is the loss 0.15714667737483978\n",
      "This is the loss 0.12360088527202606\n",
      "This is the loss 0.1429254412651062\n",
      "This is the loss 0.13315187394618988\n",
      "This is the loss 0.1183881089091301\n",
      "This is the loss 0.14382584393024445\n",
      "This is the loss 0.12128598988056183\n",
      "This is the loss 0.12376891076564789\n",
      "This is the loss 0.13643109798431396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.16328567266464233\n",
      "This is the loss 0.14029264450073242\n",
      "This is the loss 0.12172701954841614\n",
      "This is the loss 0.15853305160999298\n",
      "This is the loss 0.17815321683883667\n",
      "This is the loss 0.14005593955516815\n",
      "This is the loss 0.13681551814079285\n",
      "This is the loss 0.1577153354883194\n",
      "This is the loss 0.1557418704032898\n",
      "This is the loss 0.11878873407840729\n",
      "This is the loss 0.14484184980392456\n",
      "This is the loss 0.11800418049097061\n",
      "This is the loss 0.13800829648971558\n",
      "This is the loss 0.14902295172214508\n",
      "This is the loss 0.13531090319156647\n",
      "This is the loss 0.12090344727039337\n",
      "This is the loss 0.140984907746315\n",
      "This is the loss 0.1126355305314064\n",
      "This is the loss 0.12679938971996307\n",
      "This is the loss 0.09059781581163406\n",
      "This is the loss 0.13122650980949402\n",
      "This is the loss 0.11134950816631317\n",
      "This is the loss 0.13624750077724457\n",
      "This is the loss 0.1040789932012558\n",
      "This is the loss 0.1276620775461197\n",
      "This is the loss 0.13597705960273743\n",
      "This is the loss 0.10516126453876495\n",
      "This is the loss 0.14104780554771423\n",
      "This is the loss 0.16296719014644623\n",
      "This is the loss 0.14415456354618073\n",
      "This is the loss 0.1551763117313385\n",
      "This is the loss 0.13985934853553772\n",
      "This is the loss 0.15213258564472198\n",
      "This is the loss 0.1626361608505249\n",
      "This is the loss 0.1484708935022354\n",
      "This is the loss 0.16162729263305664\n",
      "This is the loss 0.1397966891527176\n",
      "This is the loss 0.13220691680908203\n",
      "This is the loss 0.14491261541843414\n",
      "This is the loss 0.12063722312450409\n",
      "This is the loss 0.14108556509017944\n",
      "This is the loss 0.1187201589345932\n",
      "This is the loss 0.11548235267400742\n",
      "This is the loss 0.16512328386306763\n",
      "This is the loss 0.14365148544311523\n",
      "This is the loss 0.1198110431432724\n",
      "This is the loss 0.14657658338546753\n",
      "This is the loss 0.13964030146598816\n",
      "This is the loss 0.14125783741474152\n",
      "This is the loss 0.12315486371517181\n",
      "This is the loss 0.13805165886878967\n",
      "This is the loss 0.14008809626102448\n",
      "This is the loss 0.13662004470825195\n",
      "This is the loss 0.13843396306037903\n",
      "This is the loss 0.13624386489391327\n",
      "This is the loss 0.14474700391292572\n",
      "This is the loss 0.15603815019130707\n",
      "This is the loss 0.12643060088157654\n",
      "This is the loss 0.1293686330318451\n",
      "This is the loss 0.1019127368927002\n",
      "This is the loss 0.1393526941537857\n",
      "This is the loss 0.13295869529247284\n",
      "This is the loss 0.16765975952148438\n",
      "This is the loss 0.1389903575181961\n",
      "This is the loss 0.14418110251426697\n",
      "This is the loss 0.1276146024465561\n",
      "This is the loss 0.11789249628782272\n",
      "This is the loss 0.11061975359916687\n",
      "This is the loss 0.139139324426651\n",
      "This is the loss 0.1441633701324463\n",
      "This is the loss 0.157118022441864\n",
      "This is the loss 0.14791539311408997\n",
      "This is the loss 0.13911816477775574\n",
      "This is the loss 0.1401742845773697\n",
      "This is the loss 0.14605207741260529\n",
      "This is the loss 0.13720573484897614\n",
      "This is the loss 0.13853123784065247\n",
      "This is the loss 0.10974512249231339\n",
      "This is the loss 0.15134695172309875\n",
      "This is the loss 0.16348636150360107\n",
      "This is the loss 0.13422387838363647\n",
      "This is the loss 0.16185833513736725\n",
      "This is the loss 0.13708683848381042\n",
      "This is the loss 0.10411199927330017\n",
      "This is the loss 0.14071333408355713\n",
      "This is the loss 0.13449326157569885\n",
      "This is the loss 0.16930562257766724\n",
      "This is the loss 0.10507160425186157\n",
      "This is the loss 0.18377171456813812\n",
      "This is the loss 0.13811245560646057\n",
      "This is the loss 0.15814000368118286\n",
      "This is the loss 0.15663079917430878\n",
      "This is the loss 0.1208222284913063\n",
      "This is the loss 0.1271647810935974\n",
      "This is the loss 0.12845610082149506\n",
      "This is the loss 0.1127755343914032\n",
      "This is the loss 0.13698674738407135\n",
      "This is the loss 0.1359121948480606\n",
      "This is the loss 0.11923302710056305\n",
      "This is the loss 0.124703548848629\n",
      "This is the loss 0.14888809621334076\n",
      "This is the loss 0.13878458738327026\n",
      "This is the loss 0.16553658246994019\n",
      "This is the loss 0.13020221889019012\n",
      "This is the loss 0.10036846250295639\n",
      "This is the loss 0.12097073346376419\n",
      "This is the loss 0.11603932827711105\n",
      "This is the loss 0.12134157866239548\n",
      "This is the loss 0.15389178693294525\n",
      "This is the loss 0.15256018936634064\n",
      "This is the loss 0.12348417937755585\n",
      "This is the loss 0.14441616833209991\n",
      "This is the loss 0.14345036447048187\n",
      "This is the loss 0.14217111468315125\n",
      "This is the loss 0.1185760572552681\n",
      "This is the loss 0.12466644495725632\n",
      "This is the loss 0.1172557920217514\n",
      "This is the loss 0.10921767354011536\n",
      "This is the loss 0.16928662359714508\n",
      "This is the loss 0.12434275448322296\n",
      "This is the loss 0.11025135964155197\n",
      "This is the loss 0.1302286833524704\n",
      "This is the loss 0.1303665041923523\n",
      "This is the loss 0.16655966639518738\n",
      "This is the loss 0.11322471499443054\n",
      "This is the loss 0.10133694857358932\n",
      "This is the loss 0.15315678715705872\n",
      "This is the loss 0.12780329585075378\n",
      "This is the loss 0.13922196626663208\n",
      "This is the loss 0.1529354453086853\n",
      "This is the loss 0.1304510533809662\n",
      "This is the loss 0.14324840903282166\n",
      "This is the loss 0.13822010159492493\n",
      "This is the loss 0.12731604278087616\n",
      "This is the loss 0.19508600234985352\n",
      "This is the loss 0.16149874031543732\n",
      "This is the loss 0.12425839900970459\n",
      "This is the loss 0.14393538236618042\n",
      "This is the loss 0.15028057992458344\n",
      "This is the loss 0.160303995013237\n",
      "This is the loss 0.1401277333498001\n",
      "This is the loss 0.14986291527748108\n",
      "This is the loss 0.13178743422031403\n",
      "This is the loss 0.12043174356222153\n",
      "This is the loss 0.14984358847141266\n",
      "This is the loss 0.12869863212108612\n",
      "This is the loss 0.1399664580821991\n",
      "This is the loss 0.14991557598114014\n",
      "This is the loss 0.1399027407169342\n",
      "This is the loss 0.16740404069423676\n",
      "This is the loss 0.1365794539451599\n",
      "This is the loss 0.12988360226154327\n",
      "This is the loss 0.15299560129642487\n",
      "This is the loss 0.11687002331018448\n",
      "This is the loss 0.15036317706108093\n",
      "This is the loss 0.1488925814628601\n",
      "This is the loss 0.1218808963894844\n",
      "This is the loss 0.15430915355682373\n",
      "This is the loss 0.15653520822525024\n",
      "This is the loss 0.11392109096050262\n",
      "This is the loss 0.13950344920158386\n",
      "This is the loss 0.13315781950950623\n",
      "This is the loss 0.1432889699935913\n",
      "This is the loss 0.12327062338590622\n",
      "This is the loss 0.14444994926452637\n",
      "This is the loss 0.12413132935762405\n",
      "This is the loss 0.13331490755081177\n",
      "This is the loss 0.12815624475479126\n",
      "This is the loss 0.13491412997245789\n",
      "This is the loss 0.10883831977844238\n",
      "This is the loss 0.12907814979553223\n",
      "This is the loss 0.15172028541564941\n",
      "This is the loss 0.12506960332393646\n",
      "This is the loss 0.13822750747203827\n",
      "This is the loss 0.1019492819905281\n",
      "This is the loss 0.13995899260044098\n",
      "This is the loss 0.12008024752140045\n",
      "This is the loss 0.11721643060445786\n",
      "This is the loss 0.14043156802654266\n",
      "This is the loss 0.12063023447990417\n",
      "This is the loss 0.13949739933013916\n",
      "This is the loss 0.13811051845550537\n",
      "This is the loss 0.14794540405273438\n",
      "This is the loss 0.16912724077701569\n",
      "This is the loss 0.13689228892326355\n",
      "This is the loss 0.13844208419322968\n",
      "This is the loss 0.13866347074508667\n",
      "This is the loss 0.11926202476024628\n",
      "This is the loss 0.13679198920726776\n",
      "This is the loss 0.14552760124206543\n",
      "This is the loss 0.12654177844524384\n",
      "This is the loss 0.11939242482185364\n",
      "This is the loss 0.08597151935100555\n",
      "This is the loss 0.13769873976707458\n",
      "This is the loss 0.12813660502433777\n",
      "This is the loss 0.15487870573997498\n",
      "This is the loss 0.11408646404743195\n",
      "This is the loss 0.1416301131248474\n",
      "This is the loss 0.14443522691726685\n",
      "This is the loss 0.10668107122182846\n",
      "This is the loss 0.11870421469211578\n",
      "This is the loss 0.15276581048965454\n",
      "This is the loss 0.15836621820926666\n",
      "This is the loss 0.16908229887485504\n",
      "This is the loss 0.14417049288749695\n",
      "This is the loss 0.14811208844184875\n",
      "This is the loss 0.08690816909074783\n",
      "This is the loss 0.15180252492427826\n",
      "This is the loss 0.09932666271924973\n",
      "This is the loss 0.12007442116737366\n",
      "This is the loss 0.13292895257472992\n",
      "This is the loss 0.12449607998132706\n",
      "This is the loss 0.13558554649353027\n",
      "This is the loss 0.13702410459518433\n",
      "This is the loss 0.14965863525867462\n",
      "This is the loss 0.12058716267347336\n",
      "This is the loss 0.1407342553138733\n",
      "This is the loss 0.13159695267677307\n",
      "This is the loss 0.13623768091201782\n",
      "This is the loss 0.16363073885440826\n",
      "This is the loss 0.16214828193187714\n",
      "This is the loss 0.1384616196155548\n",
      "This is the loss 0.16189712285995483\n",
      "This is the loss 0.15256670117378235\n",
      "This is the loss 0.1474868506193161\n",
      "This is the loss 0.14190195500850677\n",
      "This is the loss 0.16973841190338135\n",
      "This is the loss 0.15478038787841797\n",
      "This is the loss 0.17011332511901855\n",
      "This is the loss 0.1398019939661026\n",
      "This is the loss 0.14191527664661407\n",
      "This is the loss 0.1471511274576187\n",
      "This is the loss 0.18038788437843323\n",
      "This is the loss 0.1566157042980194\n",
      "This is the loss 0.1692383587360382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.12749901413917542\n",
      "This is the loss 0.10687913745641708\n",
      "This is the loss 0.10981722176074982\n",
      "This is the loss 0.14210845530033112\n",
      "This is the loss 0.1451384425163269\n",
      "This is the loss 0.1398783177137375\n",
      "This is the loss 0.11878178268671036\n",
      "This is the loss 0.1024019718170166\n",
      "This is the loss 0.1412699818611145\n",
      "This is the loss 0.12418879568576813\n",
      "This is the loss 0.12304703891277313\n",
      "This is the loss 0.12136125564575195\n",
      "This is the loss 0.12413029372692108\n",
      "This is the loss 0.11102959513664246\n",
      "This is the loss 0.1465255469083786\n",
      "This is the loss 0.11548206955194473\n",
      "This is the loss 0.1623016893863678\n",
      "This is the loss 0.13082768023014069\n",
      "This is the loss 0.13263022899627686\n",
      "This is the loss 0.11529581248760223\n",
      "This is the loss 0.12917084991931915\n",
      "This is the loss 0.16138142347335815\n",
      "This is the loss 0.1638186126947403\n",
      "This is the loss 0.1516743153333664\n",
      "This is the loss 0.1148725301027298\n",
      "This is the loss 0.15418407320976257\n",
      "This is the loss 0.1656692922115326\n",
      "This is the loss 0.14313481748104095\n",
      "This is the loss 0.1538219451904297\n",
      "This is the loss 0.11793648451566696\n",
      "This is the loss 0.14106318354606628\n",
      "This is the loss 0.11479145288467407\n",
      "This is the loss 0.1690710484981537\n",
      "This is the loss 0.16798922419548035\n",
      "This is the loss 0.15328899025917053\n",
      "This is the loss 0.1339418739080429\n",
      "This is the loss 0.1338803619146347\n",
      "This is the loss 0.12391837686300278\n",
      "This is the loss 0.1366783231496811\n",
      "This is the loss 0.1184719055891037\n",
      "This is the loss 0.13851724565029144\n",
      "This is the loss 0.11678291857242584\n",
      "This is the loss 0.08149128407239914\n",
      "This is the loss 0.11199891567230225\n",
      "This is the loss 0.12829172611236572\n",
      "This is the loss 0.14888298511505127\n",
      "This is the loss 0.12902121245861053\n",
      "This is the loss 0.12057480216026306\n",
      "This is the loss 0.13305720686912537\n",
      "This is the loss 0.13456597924232483\n",
      "This is the loss 0.13679154217243195\n",
      "This is the loss 0.1314408779144287\n",
      "This is the loss 0.13115105032920837\n",
      "This is the loss 0.13595862686634064\n",
      "This is the loss 0.13786064088344574\n",
      "This is the loss 0.12271372973918915\n",
      "This is the loss 0.15017667412757874\n",
      "This is the loss 0.15958799421787262\n",
      "This is the loss 0.14920014142990112\n",
      "This is the loss 0.14428862929344177\n",
      "This is the loss 0.14312684535980225\n",
      "This is the loss 0.14890708029270172\n",
      "This is the loss 0.16899177432060242\n",
      "This is the loss 0.16922064125537872\n",
      "This is the loss 0.14085465669631958\n",
      "This is the loss 0.16603684425354004\n",
      "This is the loss 0.16530932486057281\n",
      "This is the loss 0.14892445504665375\n",
      "This is the loss 0.16348035633563995\n",
      "This is the loss 0.1379878669977188\n",
      "This is the loss 0.11481356620788574\n",
      "This is the loss 0.12824063003063202\n",
      "This is the loss 0.12007490545511246\n",
      "This is the loss 0.160159170627594\n",
      "This is the loss 0.16032546758651733\n",
      "This is the loss 0.13752081990242004\n",
      "This is the loss 0.18463128805160522\n",
      "This is the loss 0.1313449740409851\n",
      "This is the loss 0.164695605635643\n",
      "This is the loss 0.14114977419376373\n",
      "This is the loss 0.19333280622959137\n",
      "This is the loss 0.1502654254436493\n",
      "This is the loss 0.1355271339416504\n",
      "This is the loss 0.14946886897087097\n",
      "This is the loss 0.15729860961437225\n",
      "This is the loss 0.1405547708272934\n",
      "This is the loss 0.15154865384101868\n",
      "This is the loss 0.14465075731277466\n",
      "This is the loss 0.13705390691757202\n",
      "This is the loss 0.1449097990989685\n",
      "This is the loss 0.12845046818256378\n",
      "This is the loss 0.1277514100074768\n",
      "This is the loss 0.14866502583026886\n",
      "This is the loss 0.15423989295959473\n",
      "This is the loss 0.12607823312282562\n",
      "This is the loss 0.14019587635993958\n",
      "This is the loss 0.11937171220779419\n",
      "This is the loss 0.09997177124023438\n",
      "This is the loss 0.12064968049526215\n",
      "This is the loss 0.1287393420934677\n",
      "This is the loss 0.14057205617427826\n",
      "This is the loss 0.12045557051897049\n",
      "This is the loss 0.1543400138616562\n",
      "This is the loss 0.13213568925857544\n",
      "This is the loss 0.12564192712306976\n",
      "This is the loss 0.15016137063503265\n",
      "This is the loss 0.10928504168987274\n",
      "This is the loss 0.11566084623336792\n",
      "This is the loss 0.13419565558433533\n",
      "This is the loss 0.12317577004432678\n",
      "This is the loss 0.159265398979187\n",
      "This is the loss 0.15187306702136993\n",
      "This is the loss 0.14912725985050201\n",
      "This is the loss 0.14187616109848022\n",
      "This is the loss 0.12258946895599365\n",
      "This is the loss 0.13935059309005737\n",
      "This is the loss 0.13176748156547546\n",
      "This is the loss 0.128355473279953\n",
      "This is the loss 0.15004533529281616\n",
      "This is the loss 0.15634207427501678\n",
      "This is the loss 0.15410272777080536\n",
      "This is the loss 0.11657537519931793\n",
      "This is the loss 0.12407263368368149\n",
      "This is the loss 0.13109838962554932\n",
      "This is the loss 0.1581428498029709\n",
      "This is the loss 0.18343569338321686\n",
      "This is the loss 0.12322521209716797\n",
      "This is the loss 0.1268731653690338\n",
      "This is the loss 0.14189298450946808\n",
      "This is the loss 0.13174563646316528\n",
      "This is the loss 0.12610599398612976\n",
      "This is the loss 0.12724749743938446\n",
      "This is the loss 0.1657969355583191\n",
      "This is the loss 0.1278531402349472\n",
      "This is the loss 0.14531080424785614\n",
      "This is the loss 0.11706976592540741\n",
      "This is the loss 0.11925183981657028\n",
      "This is the loss 0.10584893077611923\n",
      "This is the loss 0.14213895797729492\n",
      "This is the loss 0.1457839012145996\n",
      "This is the loss 0.1567513346672058\n",
      "This is the loss 0.13814908266067505\n",
      "This is the loss 0.12912170588970184\n",
      "This is the loss 0.12310873717069626\n",
      "This is the loss 0.12101368606090546\n",
      "This is the loss 0.1079985573887825\n",
      "This is the loss 0.12906570732593536\n",
      "This is the loss 0.12493984401226044\n",
      "This is the loss 0.13737954199314117\n",
      "This is the loss 0.10015501827001572\n",
      "This is the loss 0.11773092299699783\n",
      "This is the loss 0.10867365449666977\n",
      "This is the loss 0.13551323115825653\n",
      "This is the loss 0.1345665007829666\n",
      "This is the loss 0.13712696731090546\n",
      "This is the loss 0.16683045029640198\n",
      "This is the loss 0.1622030884027481\n",
      "This is the loss 0.16157861053943634\n",
      "This is the loss 0.11741641163825989\n",
      "This is the loss 0.1322483867406845\n",
      "This is the loss 0.11968494206666946\n",
      "This is the loss 0.13153573870658875\n",
      "This is the loss 0.11601731181144714\n",
      "This is the loss 0.14951355755329132\n",
      "This is the loss 0.152927428483963\n",
      "This is the loss 0.1469218134880066\n",
      "This is the loss 0.11067263036966324\n",
      "This is the loss 0.13510246574878693\n",
      "This is the loss 0.1336858868598938\n",
      "This is the loss 0.1363145112991333\n",
      "This is the loss 0.16280099749565125\n",
      "This is the loss 0.14841203391551971\n",
      "This is the loss 0.13573960959911346\n",
      "This is the loss 0.12703204154968262\n",
      "This is the loss 0.12433694303035736\n",
      "This is the loss 0.13796943426132202\n",
      "This is the loss 0.16324330866336823\n",
      "This is the loss 0.15087422728538513\n",
      "This is the loss 0.16617543995380402\n",
      "This is the loss 0.17645372450351715\n",
      "This is the loss 0.12024637311697006\n",
      "This is the loss 0.16043543815612793\n",
      "This is the loss 0.11882761865854263\n",
      "This is the loss 0.15173250436782837\n",
      "This is the loss 0.13416413962841034\n",
      "This is the loss 0.12535765767097473\n",
      "This is the loss 0.15026617050170898\n",
      "This is the loss 0.12484249472618103\n",
      "This is the loss 0.15897633135318756\n",
      "This is the loss 0.1297663003206253\n",
      "This is the loss 0.12349231541156769\n",
      "This is the loss 0.15561850368976593\n",
      "This is the loss 0.11009366810321808\n",
      "This is the loss 0.12724518775939941\n",
      "This is the loss 0.11282448470592499\n",
      "This is the loss 0.1272677481174469\n",
      "This is the loss 0.1586846560239792\n",
      "This is the loss 0.1277971863746643\n",
      "This is the loss 0.13451963663101196\n",
      "This is the loss 0.12826131284236908\n",
      "This is the loss 0.09746292233467102\n",
      "This is the loss 0.12492337822914124\n",
      "This is the loss 0.11460146307945251\n",
      "This is the loss 0.1033712774515152\n",
      "This is the loss 0.14116162061691284\n",
      "This is the loss 0.12653495371341705\n",
      "This is the loss 0.09713432192802429\n",
      "This is the loss 0.12257581949234009\n",
      "This is the loss 0.14055566489696503\n",
      "This is the loss 0.14590196311473846\n",
      "This is the loss 0.12324798107147217\n",
      "This is the loss 0.11648254096508026\n",
      "This is the loss 0.12962175905704498\n",
      "This is the loss 0.11852351576089859\n",
      "This is the loss 0.12030840665102005\n",
      "This is the loss 0.15538689494132996\n",
      "This is the loss 0.16168704628944397\n",
      "This is the loss 0.13036371767520905\n",
      "This is the loss 0.15147574245929718\n",
      "This is the loss 0.1334625780582428\n",
      "This is the loss 0.14127394556999207\n",
      "This is the loss 0.13563525676727295\n",
      "This is the loss 0.12387989461421967\n",
      "This is the loss 0.16700799763202667\n",
      "This is the loss 0.12505841255187988\n",
      "This is the loss 0.10810957103967667\n",
      "This is the loss 0.1415022313594818\n",
      "This is the loss 0.13601700961589813\n",
      "This is the loss 0.1555742621421814\n",
      "This is the loss 0.1622597724199295\n",
      "This is the loss 0.15176498889923096\n",
      "This is the loss 0.12697644531726837\n",
      "This is the loss 0.15092751383781433\n",
      "This is the loss 0.14429712295532227\n",
      "This is the loss 0.1489977091550827\n",
      "This is the loss 0.17168784141540527\n",
      "This is the loss 0.14183738827705383\n",
      "This is the loss 0.13707537949085236\n",
      "This is the loss 0.1442534476518631\n",
      "This is the loss 0.13856995105743408\n",
      "This is the loss 0.1696358621120453\n",
      "This is the loss 0.13489633798599243\n",
      "This is the loss 0.146110400557518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.12650130689144135\n",
      "This is the loss 0.14867949485778809\n",
      "This is the loss 0.14623932540416718\n",
      "This is the loss 0.14576514065265656\n",
      "This is the loss 0.1371331512928009\n",
      "This is the loss 0.16239964962005615\n",
      "This is the loss 0.12059789896011353\n",
      "This is the loss 0.1503765881061554\n",
      "This is the loss 0.15735919773578644\n",
      "This is the loss 0.1559177041053772\n",
      "This is the loss 0.14428701996803284\n",
      "This is the loss 0.14165329933166504\n",
      "This is the loss 0.14480656385421753\n",
      "This is the loss 0.1633581668138504\n",
      "This is the loss 0.13568711280822754\n",
      "This is the loss 0.1259048879146576\n",
      "This is the loss 0.14929865300655365\n",
      "This is the loss 0.17011529207229614\n",
      "This is the loss 0.16388091444969177\n",
      "This is the loss 0.14376574754714966\n",
      "This is the loss 0.14636650681495667\n",
      "This is the loss 0.12676572799682617\n",
      "This is the loss 0.15116246044635773\n",
      "This is the loss 0.13926607370376587\n",
      "This is the loss 0.17785149812698364\n",
      "This is the loss 0.18230301141738892\n",
      "This is the loss 0.15032845735549927\n",
      "This is the loss 0.1400645226240158\n",
      "This is the loss 0.134299635887146\n",
      "This is the loss 0.11117421835660934\n",
      "This is the loss 0.16697411239147186\n",
      "This is the loss 0.13050827383995056\n",
      "This is the loss 0.15010683238506317\n",
      "This is the loss 0.19176633656024933\n",
      "This is the loss 0.13574236631393433\n",
      "This is the loss 0.166669562458992\n",
      "This is the loss 0.12531191110610962\n",
      "This is the loss 0.11711427569389343\n",
      "This is the loss 0.17107687890529633\n",
      "This is the loss 0.13924278318881989\n",
      "This is the loss 0.167937770485878\n",
      "This is the loss 0.15961000323295593\n",
      "This is the loss 0.1521403193473816\n",
      "This is the loss 0.15750861167907715\n",
      "This is the loss 0.16832700371742249\n",
      "This is the loss 0.15301331877708435\n",
      "This is the loss 0.12284287810325623\n",
      "This is the loss 0.18763990700244904\n",
      "This is the loss 0.15073908865451813\n",
      "This is the loss 0.13654565811157227\n",
      "This is the loss 0.13662824034690857\n",
      "This is the loss 0.14624439179897308\n",
      "This is the loss 0.15667983889579773\n",
      "This is the loss 0.15064015984535217\n",
      "This is the loss 0.1359621286392212\n",
      "This is the loss 0.1617380976676941\n",
      "This is the loss 0.13738436996936798\n",
      "This is the loss 0.12348872423171997\n",
      "This is the loss 0.1352735161781311\n",
      "This is the loss 0.09728105366230011\n",
      "This is the loss 0.13714465498924255\n",
      "This is the loss 0.139070063829422\n",
      "This is the loss 0.16290542483329773\n",
      "This is the loss 0.14255648851394653\n",
      "This is the loss 0.14566265046596527\n",
      "This is the loss 0.15620115399360657\n",
      "This is the loss 0.16138097643852234\n",
      "This is the loss 0.17574013769626617\n",
      "This is the loss 0.15498767793178558\n",
      "This is the loss 0.16143783926963806\n",
      "This is the loss 0.16179022192955017\n",
      "This is the loss 0.18138889968395233\n",
      "This is the loss 0.12080156803131104\n",
      "This is the loss 0.14568880200386047\n",
      "This is the loss 0.15541337430477142\n",
      "This is the loss 0.18085475265979767\n",
      "This is the loss 0.15287025272846222\n",
      "This is the loss 0.1403011679649353\n",
      "This is the loss 0.1859094798564911\n",
      "This is the loss 0.17983098328113556\n",
      "This is the loss 0.12660042941570282\n",
      "This is the loss 0.16050881147384644\n",
      "This is the loss 0.15351159870624542\n",
      "This is the loss 0.14597918093204498\n",
      "This is the loss 0.14217130839824677\n",
      "This is the loss 0.14706696569919586\n",
      "This is the loss 0.12650609016418457\n",
      "This is the loss 0.15360255539417267\n",
      "This is the loss 0.12123364210128784\n",
      "This is the loss 0.1316860169172287\n",
      "This is the loss 0.13403764367103577\n",
      "This is the loss 0.1579701155424118\n",
      "This is the loss 0.17261478304862976\n",
      "This is the loss 0.17124155163764954\n",
      "This is the loss 0.13999103009700775\n",
      "This is the loss 0.13171106576919556\n",
      "This is the loss 0.12300392985343933\n",
      "This is the loss 0.11814793944358826\n",
      "This is the loss 0.12221213430166245\n",
      "This is the loss 0.11847501248121262\n",
      "This is the loss 0.15651123225688934\n",
      "This is the loss 0.14081859588623047\n",
      "This is the loss 0.1634765863418579\n",
      "This is the loss 0.13758453726768494\n",
      "This is the loss 0.1163628101348877\n",
      "This is the loss 0.13490240275859833\n",
      "This is the loss 0.12229505926370621\n",
      "This is the loss 0.14091438055038452\n",
      "This is the loss 0.14678065478801727\n",
      "This is the loss 0.15830156207084656\n",
      "This is the loss 0.15150751173496246\n",
      "This is the loss 0.13960039615631104\n",
      "This is the loss 0.1496237963438034\n",
      "This is the loss 0.1433451771736145\n",
      "This is the loss 0.1478652060031891\n",
      "This is the loss 0.15063250064849854\n",
      "This is the loss 0.11746703833341599\n",
      "This is the loss 0.15913745760917664\n",
      "This is the loss 0.1334533989429474\n",
      "This is the loss 0.12640176713466644\n",
      "This is the loss 0.1360950917005539\n",
      "This is the loss 0.14096541702747345\n",
      "This is the loss 0.16043418645858765\n",
      "This is the loss 0.12914223968982697\n",
      "This is the loss 0.14448988437652588\n",
      "This is the loss 0.13524115085601807\n",
      "This is the loss 0.12507450580596924\n",
      "This is the loss 0.15712827444076538\n",
      "This is the loss 0.13861796259880066\n",
      "This is the loss 0.14660735428333282\n",
      "This is the loss 0.11897911131381989\n",
      "This is the loss 0.1129070445895195\n",
      "This is the loss 0.16168852150440216\n",
      "This is the loss 0.1429286152124405\n",
      "This is the loss 0.11392582207918167\n",
      "This is the loss 0.13524991273880005\n",
      "This is the loss 0.144902765750885\n",
      "This is the loss 0.12018613517284393\n",
      "This is the loss 0.14887630939483643\n",
      "This is the loss 0.10433391481637955\n",
      "This is the loss 0.15125924348831177\n",
      "This is the loss 0.14757154881954193\n",
      "This is the loss 0.1310652792453766\n",
      "This is the loss 0.13153792917728424\n",
      "This is the loss 0.12264272570610046\n",
      "This is the loss 0.16196124255657196\n",
      "This is the loss 0.12314927577972412\n",
      "This is the loss 0.15926223993301392\n",
      "This is the loss 0.12258248031139374\n",
      "This is the loss 0.12689298391342163\n",
      "This is the loss 0.15992705523967743\n",
      "This is the loss 0.14309453964233398\n",
      "This is the loss 0.12999282777309418\n",
      "This is the loss 0.1370014101266861\n",
      "This is the loss 0.13608793914318085\n",
      "This is the loss 0.12791937589645386\n",
      "This is the loss 0.162225142121315\n",
      "This is the loss 0.11835170537233353\n",
      "This is the loss 0.16467946767807007\n",
      "This is the loss 0.13067632913589478\n",
      "This is the loss 0.1156836450099945\n",
      "This is the loss 0.13112632930278778\n",
      "This is the loss 0.14590398967266083\n",
      "This is the loss 0.13548779487609863\n",
      "This is the loss 0.16563080251216888\n",
      "This is the loss 0.14233040809631348\n",
      "This is the loss 0.1223125159740448\n",
      "This is the loss 0.1225816160440445\n",
      "This is the loss 0.16123096644878387\n",
      "This is the loss 0.14245067536830902\n",
      "This is the loss 0.12073469161987305\n",
      "This is the loss 0.12265124171972275\n",
      "This is the loss 0.15557292103767395\n",
      "This is the loss 0.15474048256874084\n",
      "This is the loss 0.13344597816467285\n",
      "This is the loss 0.13658149540424347\n",
      "This is the loss 0.1265827864408493\n",
      "This is the loss 0.14400869607925415\n",
      "This is the loss 0.15779265761375427\n",
      "This is the loss 0.11227916181087494\n",
      "This is the loss 0.10844404250383377\n",
      "This is the loss 0.13697436451911926\n",
      "This is the loss 0.17471083998680115\n",
      "This is the loss 0.14263975620269775\n",
      "This is the loss 0.1643379032611847\n",
      "This is the loss 0.17570838332176208\n",
      "This is the loss 0.17824672162532806\n",
      "This is the loss 0.14407920837402344\n",
      "This is the loss 0.146750345826149\n",
      "This is the loss 0.1317634880542755\n",
      "This is the loss 0.12244676798582077\n",
      "This is the loss 0.11998820304870605\n",
      "This is the loss 0.13737016916275024\n",
      "This is the loss 0.12865886092185974\n",
      "This is the loss 0.13446566462516785\n",
      "This is the loss 0.1287687122821808\n",
      "This is the loss 0.12269441038370132\n",
      "This is the loss 0.1505269855260849\n",
      "This is the loss 0.14717751741409302\n",
      "This is the loss 0.16965001821517944\n",
      "This is the loss 0.11804512143135071\n",
      "This is the loss 0.13006797432899475\n",
      "This is the loss 0.15088430047035217\n",
      "This is the loss 0.11519624292850494\n",
      "This is the loss 0.1410156786441803\n",
      "This is the loss 0.15116697549819946\n",
      "This is the loss 0.12153272330760956\n",
      "This is the loss 0.15648402273654938\n",
      "This is the loss 0.11544806510210037\n",
      "This is the loss 0.1489378958940506\n",
      "This is the loss 0.12672676146030426\n",
      "This is the loss 0.1518811285495758\n",
      "This is the loss 0.12524278461933136\n",
      "This is the loss 0.15060749650001526\n",
      "This is the loss 0.11919355392456055\n",
      "This is the loss 0.1294841468334198\n",
      "This is the loss 0.14657898247241974\n",
      "This is the loss 0.12758995592594147\n",
      "This is the loss 0.13304072618484497\n",
      "This is the loss 0.09904876351356506\n",
      "This is the loss 0.1412522941827774\n",
      "This is the loss 0.1311659961938858\n",
      "This is the loss 0.11454319953918457\n",
      "This is the loss 0.14277611672878265\n",
      "This is the loss 0.15105003118515015\n",
      "This is the loss 0.14671236276626587\n",
      "This is the loss 0.13882723450660706\n",
      "This is the loss 0.16223451495170593\n",
      "This is the loss 0.10055286437273026\n",
      "This is the loss 0.1691700518131256\n",
      "This is the loss 0.11173650622367859\n",
      "This is the loss 0.13600562512874603\n",
      "This is the loss 0.13280260562896729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.13751830160617828\n",
      "This is the loss 0.13432922959327698\n",
      "This is the loss 0.12415067851543427\n",
      "This is the loss 0.11924298107624054\n",
      "This is the loss 0.13663522899150848\n",
      "This is the loss 0.15167157351970673\n",
      "This is the loss 0.14661641418933868\n",
      "This is the loss 0.12688520550727844\n",
      "This is the loss 0.12448477745056152\n",
      "This is the loss 0.11355960369110107\n",
      "This is the loss 0.1494254767894745\n",
      "This is the loss 0.16122019290924072\n",
      "This is the loss 0.125174880027771\n",
      "This is the loss 0.11986695230007172\n",
      "This is the loss 0.1399185061454773\n",
      "This is the loss 0.14651674032211304\n",
      "This is the loss 0.13365161418914795\n",
      "This is the loss 0.14464373886585236\n",
      "This is the loss 0.15018931031227112\n",
      "This is the loss 0.16981980204582214\n",
      "This is the loss 0.16513189673423767\n",
      "This is the loss 0.15330033004283905\n",
      "This is the loss 0.1333221048116684\n",
      "This is the loss 0.16042405366897583\n",
      "This is the loss 0.14984163641929626\n",
      "This is the loss 0.1271798461675644\n",
      "This is the loss 0.13121724128723145\n",
      "This is the loss 0.14714357256889343\n",
      "This is the loss 0.14586342871189117\n",
      "This is the loss 0.13973712921142578\n",
      "This is the loss 0.1490662842988968\n",
      "This is the loss 0.14067035913467407\n",
      "This is the loss 0.12680746614933014\n",
      "This is the loss 0.1390334963798523\n",
      "This is the loss 0.14241422712802887\n",
      "This is the loss 0.1340986043214798\n",
      "This is the loss 0.15265215933322906\n",
      "This is the loss 0.12365023046731949\n",
      "This is the loss 0.12443564832210541\n",
      "This is the loss 0.16048431396484375\n",
      "This is the loss 0.14478354156017303\n",
      "This is the loss 0.15932928025722504\n",
      "This is the loss 0.11226511001586914\n",
      "This is the loss 0.10678893327713013\n",
      "This is the loss 0.14355404675006866\n",
      "This is the loss 0.11240997910499573\n",
      "This is the loss 0.1151256337761879\n",
      "This is the loss 0.1417924016714096\n",
      "This is the loss 0.13548146188259125\n",
      "This is the loss 0.13128560781478882\n",
      "This is the loss 0.12842513620853424\n",
      "This is the loss 0.16122978925704956\n",
      "This is the loss 0.15280424058437347\n",
      "This is the loss 0.13946500420570374\n",
      "This is the loss 0.1328238844871521\n",
      "This is the loss 0.14427736401557922\n",
      "This is the loss 0.13923902809619904\n",
      "This is the loss 0.15137523412704468\n",
      "This is the loss 0.11545173823833466\n",
      "This is the loss 0.1558697521686554\n",
      "This is the loss 0.16410160064697266\n",
      "This is the loss 0.1553613692522049\n",
      "This is the loss 0.10720758885145187\n",
      "This is the loss 0.13406705856323242\n",
      "This is the loss 0.08934104442596436\n",
      "This is the loss 0.14387260377407074\n",
      "This is the loss 0.13355755805969238\n",
      "This is the loss 0.1735670119524002\n",
      "This is the loss 0.10857336223125458\n",
      "This is the loss 0.11465345323085785\n",
      "This is the loss 0.13052211701869965\n",
      "This is the loss 0.12647683918476105\n",
      "This is the loss 0.16006624698638916\n",
      "This is the loss 0.13315856456756592\n",
      "This is the loss 0.11517759412527084\n",
      "This is the loss 0.16252751648426056\n",
      "This is the loss 0.13026836514472961\n",
      "This is the loss 0.15636323392391205\n",
      "This is the loss 0.11917465925216675\n",
      "This is the loss 0.14511393010616302\n",
      "This is the loss 0.14140693843364716\n",
      "This is the loss 0.13132822513580322\n",
      "This is the loss 0.15504246950149536\n",
      "This is the loss 0.12754684686660767\n",
      "This is the loss 0.15607571601867676\n",
      "This is the loss 0.13741078972816467\n",
      "This is the loss 0.11323196440935135\n",
      "This is the loss 0.13208475708961487\n",
      "This is the loss 0.10453010350465775\n",
      "This is the loss 0.11066459119319916\n",
      "This is the loss 0.1550859659910202\n",
      "This is the loss 0.13230174779891968\n",
      "This is the loss 0.13523654639720917\n",
      "This is the loss 0.11291190981864929\n",
      "This is the loss 0.12171933054924011\n",
      "This is the loss 0.14298872649669647\n",
      "This is the loss 0.1355734020471573\n",
      "This is the loss 0.13053403794765472\n",
      "This is the loss 0.17141899466514587\n",
      "This is the loss 0.15071511268615723\n",
      "This is the loss 0.1327541470527649\n",
      "This is the loss 0.10950267314910889\n",
      "This is the loss 0.13528253138065338\n",
      "This is the loss 0.14205238223075867\n",
      "This is the loss 0.14283603429794312\n",
      "This is the loss 0.11256881058216095\n",
      "This is the loss 0.1448688954114914\n",
      "This is the loss 0.12772050499916077\n",
      "This is the loss 0.11581812053918839\n",
      "This is the loss 0.12778665125370026\n",
      "This is the loss 0.1286584883928299\n",
      "This is the loss 0.1341988742351532\n",
      "This is the loss 0.13317567110061646\n",
      "This is the loss 0.12955892086029053\n",
      "This is the loss 0.1678091287612915\n",
      "This is the loss 0.15033039450645447\n",
      "This is the loss 0.15047191083431244\n",
      "This is the loss 0.1434991955757141\n",
      "This is the loss 0.119777612388134\n",
      "This is the loss 0.14406070113182068\n",
      "This is the loss 0.12315601855516434\n",
      "This is the loss 0.14013276994228363\n",
      "This is the loss 0.15166576206684113\n",
      "This is the loss 0.12094059586524963\n",
      "This is the loss 0.14618849754333496\n",
      "This is the loss 0.14074835181236267\n",
      "This is the loss 0.13800185918807983\n",
      "This is the loss 0.14495448768138885\n",
      "This is the loss 0.15079419314861298\n",
      "This is the loss 0.15530455112457275\n",
      "This is the loss 0.14018748700618744\n",
      "This is the loss 0.14469656348228455\n",
      "This is the loss 0.14051523804664612\n",
      "This is the loss 0.14488649368286133\n",
      "This is the loss 0.12399125844240189\n",
      "This is the loss 0.14852280914783478\n",
      "This is the loss 0.15802651643753052\n",
      "This is the loss 0.1062176376581192\n",
      "This is the loss 0.1298980712890625\n",
      "This is the loss 0.12099287658929825\n",
      "This is the loss 0.15001001954078674\n",
      "This is the loss 0.13821688294410706\n",
      "This is the loss 0.13513609766960144\n",
      "This is the loss 0.12884220480918884\n",
      "This is the loss 0.11555192619562149\n",
      "This is the loss 0.14620651304721832\n",
      "This is the loss 0.11683089286088943\n",
      "This is the loss 0.11424104869365692\n",
      "This is the loss 0.11258374899625778\n",
      "This is the loss 0.12285879999399185\n",
      "This is the loss 0.1269938200712204\n",
      "This is the loss 0.10929825901985168\n",
      "This is the loss 0.14654353260993958\n",
      "This is the loss 0.12030155956745148\n",
      "This is the loss 0.14075464010238647\n",
      "This is the loss 0.1255108118057251\n",
      "This is the loss 0.11557572335004807\n",
      "This is the loss 0.11944794654846191\n",
      "This is the loss 0.16534847021102905\n",
      "This is the loss 0.131547749042511\n",
      "This is the loss 0.16783492267131805\n",
      "This is the loss 0.15279270708560944\n",
      "This is the loss 0.16648194193840027\n",
      "This is the loss 0.11798113584518433\n",
      "This is the loss 0.1270029991865158\n",
      "This is the loss 0.09039873629808426\n",
      "This is the loss 0.11984800547361374\n",
      "This is the loss 0.15933386981487274\n",
      "This is the loss 0.10559031367301941\n",
      "This is the loss 0.146781325340271\n",
      "This is the loss 0.11933562159538269\n",
      "This is the loss 0.12439796328544617\n",
      "This is the loss 0.14035925269126892\n",
      "This is the loss 0.1475251317024231\n",
      "This is the loss 0.12055075913667679\n",
      "This is the loss 0.14018955826759338\n",
      "This is the loss 0.15262240171432495\n",
      "This is the loss 0.13338515162467957\n",
      "This is the loss 0.1218763217329979\n",
      "This is the loss 0.11903971433639526\n",
      "This is the loss 0.13717326521873474\n",
      "This is the loss 0.14092029631137848\n",
      "This is the loss 0.14134351909160614\n",
      "This is the loss 0.17619670927524567\n",
      "This is the loss 0.14168015122413635\n",
      "This is the loss 0.1433432549238205\n",
      "This is the loss 0.1613134741783142\n",
      "This is the loss 0.1290999799966812\n",
      "This is the loss 0.16019263863563538\n",
      "This is the loss 0.13851264119148254\n",
      "This is the loss 0.15098492801189423\n",
      "This is the loss 0.1344727724790573\n",
      "This is the loss 0.18251875042915344\n",
      "This is the loss 0.14987701177597046\n",
      "This is the loss 0.13626287877559662\n",
      "This is the loss 0.12405331432819366\n",
      "This is the loss 0.14393121004104614\n",
      "This is the loss 0.12752987444400787\n",
      "This is the loss 0.11557573825120926\n",
      "This is the loss 0.12920346856117249\n",
      "This is the loss 0.1114666610956192\n",
      "This is the loss 0.13957171142101288\n",
      "This is the loss 0.14722345769405365\n",
      "This is the loss 0.1573771983385086\n",
      "This is the loss 0.14862102270126343\n",
      "This is the loss 0.11617204546928406\n",
      "This is the loss 0.14902709424495697\n",
      "This is the loss 0.12820804119110107\n",
      "This is the loss 0.13354052603244781\n",
      "This is the loss 0.12555193901062012\n",
      "This is the loss 0.12813793122768402\n",
      "This is the loss 0.1349658966064453\n",
      "This is the loss 0.10742005705833435\n",
      "This is the loss 0.1223965659737587\n",
      "This is the loss 0.14199335873126984\n",
      "This is the loss 0.14217185974121094\n",
      "This is the loss 0.10309045016765594\n",
      "This is the loss 0.1422746181488037\n",
      "This is the loss 0.11483874171972275\n",
      "This is the loss 0.13308757543563843\n",
      "This is the loss 0.1426335573196411\n",
      "This is the loss 0.15182505548000336\n",
      "This is the loss 0.15396374464035034\n",
      "This is the loss 0.1315961629152298\n",
      "This is the loss 0.15644852817058563\n",
      "This is the loss 0.15911029279232025\n",
      "This is the loss 0.11951673775911331\n",
      "This is the loss 0.10316965728998184\n",
      "This is the loss 0.11633636057376862\n",
      "This is the loss 0.12260611355304718\n",
      "This is the loss 0.119591623544693\n",
      "This is the loss 0.13731637597084045\n",
      "This is the loss 0.12393876910209656\n",
      "This is the loss 0.12814104557037354\n",
      "This is the loss 0.13305237889289856\n",
      "This is the loss 0.13494205474853516\n",
      "This is the loss 0.14140097796916962\n",
      "This is the loss 0.1287788301706314\n",
      "This is the loss 0.11721154302358627\n",
      "This is the loss 0.1235261783003807\n",
      "This is the loss 0.16917771100997925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.13768011331558228\n",
      "This is the loss 0.13288748264312744\n",
      "This is the loss 0.13534724712371826\n",
      "This is the loss 0.1107192412018776\n",
      "This is the loss 0.13423597812652588\n",
      "This is the loss 0.1386066973209381\n",
      "This is the loss 0.1352793574333191\n",
      "This is the loss 0.14728014171123505\n",
      "This is the loss 0.12017779052257538\n",
      "This is the loss 0.13966289162635803\n",
      "This is the loss 0.1285267323255539\n",
      "This is the loss 0.17440178990364075\n",
      "This is the loss 0.1230316311120987\n",
      "This is the loss 0.12031898647546768\n",
      "This is the loss 0.14729221165180206\n",
      "This is the loss 0.146897554397583\n",
      "This is the loss 0.12875407934188843\n",
      "This is the loss 0.15295036137104034\n",
      "This is the loss 0.14295092225074768\n",
      "This is the loss 0.17002548277378082\n",
      "This is the loss 0.12740540504455566\n",
      "This is the loss 0.11987113207578659\n",
      "This is the loss 0.12932291626930237\n",
      "This is the loss 0.15144740045070648\n",
      "This is the loss 0.1387723833322525\n",
      "This is the loss 0.12929046154022217\n",
      "This is the loss 0.13641922175884247\n",
      "This is the loss 0.12724867463111877\n",
      "This is the loss 0.09505677223205566\n",
      "This is the loss 0.13446277379989624\n",
      "This is the loss 0.10476864129304886\n",
      "This is the loss 0.12026029080152512\n",
      "This is the loss 0.1295408308506012\n",
      "This is the loss 0.16368848085403442\n",
      "This is the loss 0.14574478566646576\n",
      "This is the loss 0.14981424808502197\n",
      "This is the loss 0.13900862634181976\n",
      "This is the loss 0.1645500212907791\n",
      "This is the loss 0.1372223049402237\n",
      "This is the loss 0.15441468358039856\n",
      "This is the loss 0.156104177236557\n",
      "This is the loss 0.13774554431438446\n",
      "This is the loss 0.13564805686473846\n",
      "This is the loss 0.14153286814689636\n",
      "This is the loss 0.14407514035701752\n",
      "This is the loss 0.13666093349456787\n",
      "This is the loss 0.14507068693637848\n",
      "This is the loss 0.15460431575775146\n",
      "This is the loss 0.15370047092437744\n",
      "This is the loss 0.12004418671131134\n",
      "This is the loss 0.14851371943950653\n",
      "This is the loss 0.13876359164714813\n",
      "This is the loss 0.1276058703660965\n",
      "This is the loss 0.14048315584659576\n",
      "This is the loss 0.11130621284246445\n",
      "This is the loss 0.1571233868598938\n",
      "This is the loss 0.12378394603729248\n",
      "This is the loss 0.14285103976726532\n",
      "This is the loss 0.13329118490219116\n",
      "This is the loss 0.11857589334249496\n",
      "This is the loss 0.14379586279392242\n",
      "This is the loss 0.12108094990253448\n",
      "This is the loss 0.12365271151065826\n",
      "This is the loss 0.13657131791114807\n",
      "This is the loss 0.16318869590759277\n",
      "This is the loss 0.14056627452373505\n",
      "This is the loss 0.12160888314247131\n",
      "This is the loss 0.15868505835533142\n",
      "This is the loss 0.17812645435333252\n",
      "This is the loss 0.13975799083709717\n",
      "This is the loss 0.13687503337860107\n",
      "This is the loss 0.15790905058383942\n",
      "This is the loss 0.15563997626304626\n",
      "This is the loss 0.1189609169960022\n",
      "This is the loss 0.14470264315605164\n",
      "This is the loss 0.11830981075763702\n",
      "This is the loss 0.1377301961183548\n",
      "This is the loss 0.14916476607322693\n",
      "This is the loss 0.13497033715248108\n",
      "This is the loss 0.12086240202188492\n",
      "This is the loss 0.14112409949302673\n",
      "This is the loss 0.11278793960809708\n",
      "This is the loss 0.12681476771831512\n",
      "This is the loss 0.0908113643527031\n",
      "This is the loss 0.13081452250480652\n",
      "This is the loss 0.11148301512002945\n",
      "This is the loss 0.13623225688934326\n",
      "This is the loss 0.10401575267314911\n",
      "This is the loss 0.12764225900173187\n",
      "This is the loss 0.1359206736087799\n",
      "This is the loss 0.10516902804374695\n",
      "This is the loss 0.14130762219429016\n",
      "This is the loss 0.16332077980041504\n",
      "This is the loss 0.1441439837217331\n",
      "This is the loss 0.15508367121219635\n",
      "This is the loss 0.1394282579421997\n",
      "This is the loss 0.15182018280029297\n",
      "This is the loss 0.1626162827014923\n",
      "This is the loss 0.14861485362052917\n",
      "This is the loss 0.1614823341369629\n",
      "This is the loss 0.13932012021541595\n",
      "This is the loss 0.13252738118171692\n",
      "This is the loss 0.14504402875900269\n",
      "This is the loss 0.12059909105300903\n",
      "This is the loss 0.14081934094429016\n",
      "This is the loss 0.11857377737760544\n",
      "This is the loss 0.11555688083171844\n",
      "This is the loss 0.16515898704528809\n",
      "This is the loss 0.1438700258731842\n",
      "This is the loss 0.11976509541273117\n",
      "This is the loss 0.14618109166622162\n",
      "This is the loss 0.139473557472229\n",
      "This is the loss 0.1412409394979477\n",
      "This is the loss 0.12280154973268509\n",
      "This is the loss 0.13843941688537598\n",
      "This is the loss 0.14035023748874664\n",
      "This is the loss 0.13684846460819244\n",
      "This is the loss 0.13857167959213257\n",
      "This is the loss 0.13646982610225677\n",
      "This is the loss 0.1445714682340622\n",
      "This is the loss 0.15602508187294006\n",
      "This is the loss 0.1263132095336914\n",
      "This is the loss 0.12927661836147308\n",
      "This is the loss 0.10177603363990784\n",
      "This is the loss 0.13950490951538086\n",
      "This is the loss 0.1330004334449768\n",
      "This is the loss 0.1673600673675537\n",
      "This is the loss 0.13857465982437134\n",
      "This is the loss 0.14408618211746216\n",
      "This is the loss 0.12757958471775055\n",
      "This is the loss 0.11780232191085815\n",
      "This is the loss 0.11046867072582245\n",
      "This is the loss 0.1391906887292862\n",
      "This is the loss 0.14414814114570618\n",
      "This is the loss 0.15718139708042145\n",
      "This is the loss 0.1478024125099182\n",
      "This is the loss 0.13875706493854523\n",
      "This is the loss 0.14051049947738647\n",
      "This is the loss 0.14581838250160217\n",
      "This is the loss 0.1369880586862564\n",
      "This is the loss 0.1387244611978531\n",
      "This is the loss 0.10957279056310654\n",
      "This is the loss 0.1514519304037094\n",
      "This is the loss 0.1636076271533966\n",
      "This is the loss 0.1341242492198944\n",
      "This is the loss 0.1620320826768875\n",
      "This is the loss 0.1370241343975067\n",
      "This is the loss 0.10424977540969849\n",
      "This is the loss 0.14075638353824615\n",
      "This is the loss 0.13446541130542755\n",
      "This is the loss 0.16919034719467163\n",
      "This is the loss 0.1051281988620758\n",
      "This is the loss 0.1833312064409256\n",
      "This is the loss 0.13771937787532806\n",
      "This is the loss 0.15815454721450806\n",
      "This is the loss 0.15684916079044342\n",
      "This is the loss 0.12059427797794342\n",
      "This is the loss 0.12715467810630798\n",
      "This is the loss 0.12827140092849731\n",
      "This is the loss 0.11313004046678543\n",
      "This is the loss 0.13688290119171143\n",
      "This is the loss 0.13566434383392334\n",
      "This is the loss 0.11921118199825287\n",
      "This is the loss 0.12492254376411438\n",
      "This is the loss 0.14889375865459442\n",
      "This is the loss 0.138776957988739\n",
      "This is the loss 0.16548913717269897\n",
      "This is the loss 0.13028037548065186\n",
      "This is the loss 0.10052857547998428\n",
      "This is the loss 0.12112052738666534\n",
      "This is the loss 0.11592908203601837\n",
      "This is the loss 0.12145909667015076\n",
      "This is the loss 0.1536501795053482\n",
      "This is the loss 0.15253320336341858\n",
      "This is the loss 0.12330859899520874\n",
      "This is the loss 0.14440412819385529\n",
      "This is the loss 0.1432616114616394\n",
      "This is the loss 0.14183060824871063\n",
      "This is the loss 0.11883626878261566\n",
      "This is the loss 0.12458468973636627\n",
      "This is the loss 0.11721814423799515\n",
      "This is the loss 0.10935880243778229\n",
      "This is the loss 0.16900064051151276\n",
      "This is the loss 0.12441272288560867\n",
      "This is the loss 0.11048618704080582\n",
      "This is the loss 0.13021084666252136\n",
      "This is the loss 0.1304621696472168\n",
      "This is the loss 0.16674652695655823\n",
      "This is the loss 0.11333663016557693\n",
      "This is the loss 0.10132551938295364\n",
      "This is the loss 0.1530519276857376\n",
      "This is the loss 0.12777364253997803\n",
      "This is the loss 0.13878650963306427\n",
      "This is the loss 0.15264259278774261\n",
      "This is the loss 0.1305081844329834\n",
      "This is the loss 0.14365199208259583\n",
      "This is the loss 0.13802441954612732\n",
      "This is the loss 0.12715350091457367\n",
      "This is the loss 0.1947869509458542\n",
      "This is the loss 0.16115933656692505\n",
      "This is the loss 0.12408538162708282\n",
      "This is the loss 0.14419250190258026\n",
      "This is the loss 0.1500900238752365\n",
      "This is the loss 0.160040482878685\n",
      "This is the loss 0.14016883075237274\n",
      "This is the loss 0.1499449908733368\n",
      "This is the loss 0.13164764642715454\n",
      "This is the loss 0.12021344900131226\n",
      "This is the loss 0.14968572556972504\n",
      "This is the loss 0.12867824733257294\n",
      "This is the loss 0.13987773656845093\n",
      "This is the loss 0.1497984528541565\n",
      "This is the loss 0.13983431458473206\n",
      "This is the loss 0.16732363402843475\n",
      "This is the loss 0.13666144013404846\n",
      "This is the loss 0.12994542717933655\n",
      "This is the loss 0.15311552584171295\n",
      "This is the loss 0.11659535765647888\n",
      "This is the loss 0.15042859315872192\n",
      "This is the loss 0.14883866906166077\n",
      "This is the loss 0.12196517735719681\n",
      "This is the loss 0.1540461778640747\n",
      "This is the loss 0.15658551454544067\n",
      "This is the loss 0.11382264643907547\n",
      "This is the loss 0.13914969563484192\n",
      "This is the loss 0.1329948753118515\n",
      "This is the loss 0.14333966374397278\n",
      "This is the loss 0.12295369058847427\n",
      "This is the loss 0.14400938153266907\n",
      "This is the loss 0.12395676970481873\n",
      "This is the loss 0.13353992998600006\n",
      "This is the loss 0.12812428176403046\n",
      "This is the loss 0.13462409377098083\n",
      "This is the loss 0.10878099501132965\n",
      "This is the loss 0.1289183646440506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.15184827148914337\n",
      "This is the loss 0.12497379630804062\n",
      "This is the loss 0.1380959004163742\n",
      "This is the loss 0.1018211767077446\n",
      "This is the loss 0.1401301622390747\n",
      "This is the loss 0.11984966695308685\n",
      "This is the loss 0.11716164648532867\n",
      "This is the loss 0.14008279144763947\n",
      "This is the loss 0.12067509442567825\n",
      "This is the loss 0.13945038616657257\n",
      "This is the loss 0.1380237340927124\n",
      "This is the loss 0.14802566170692444\n",
      "This is the loss 0.16905124485492706\n",
      "This is the loss 0.1367272436618805\n",
      "This is the loss 0.13840770721435547\n",
      "This is the loss 0.13859254121780396\n",
      "This is the loss 0.11924563348293304\n",
      "This is the loss 0.13644558191299438\n",
      "This is the loss 0.14564529061317444\n",
      "This is the loss 0.12631700932979584\n",
      "This is the loss 0.11937244981527328\n",
      "This is the loss 0.08613895624876022\n",
      "This is the loss 0.13785189390182495\n",
      "This is the loss 0.12795421481132507\n",
      "This is the loss 0.15515026450157166\n",
      "This is the loss 0.11424984037876129\n",
      "This is the loss 0.14167870581150055\n",
      "This is the loss 0.14410284161567688\n",
      "This is the loss 0.10653679072856903\n",
      "This is the loss 0.11841023713350296\n",
      "This is the loss 0.15259960293769836\n",
      "This is the loss 0.1582198441028595\n",
      "This is the loss 0.1692613661289215\n",
      "This is the loss 0.14394345879554749\n",
      "This is the loss 0.14788676798343658\n",
      "This is the loss 0.08692183345556259\n",
      "This is the loss 0.1517849564552307\n",
      "This is the loss 0.09931796044111252\n",
      "This is the loss 0.11978918313980103\n",
      "This is the loss 0.13288532197475433\n",
      "This is the loss 0.12416624277830124\n",
      "This is the loss 0.13558915257453918\n",
      "This is the loss 0.1368255913257599\n",
      "This is the loss 0.1497168093919754\n",
      "This is the loss 0.12053155899047852\n",
      "This is the loss 0.14057867228984833\n",
      "This is the loss 0.13147936761379242\n",
      "This is the loss 0.1364012360572815\n",
      "This is the loss 0.1638156920671463\n",
      "This is the loss 0.161982923746109\n",
      "This is the loss 0.1384221911430359\n",
      "This is the loss 0.16152797639369965\n",
      "This is the loss 0.15269526839256287\n",
      "This is the loss 0.14759181439876556\n",
      "This is the loss 0.14196498692035675\n",
      "This is the loss 0.16975131630897522\n",
      "This is the loss 0.15485620498657227\n",
      "This is the loss 0.16999037563800812\n",
      "This is the loss 0.13983988761901855\n",
      "This is the loss 0.14179867506027222\n",
      "This is the loss 0.14729748666286469\n",
      "This is the loss 0.18003390729427338\n",
      "This is the loss 0.15622200071811676\n",
      "This is the loss 0.16894997656345367\n",
      "This is the loss 0.1274755448102951\n",
      "This is the loss 0.10696546733379364\n",
      "This is the loss 0.10942187905311584\n",
      "This is the loss 0.14225101470947266\n",
      "This is the loss 0.14518289268016815\n",
      "This is the loss 0.1397833377122879\n",
      "This is the loss 0.1186748594045639\n",
      "This is the loss 0.10241761803627014\n",
      "This is the loss 0.14128343760967255\n",
      "This is the loss 0.1242099180817604\n",
      "This is the loss 0.1230262741446495\n",
      "This is the loss 0.12134622782468796\n",
      "This is the loss 0.12423314899206161\n",
      "This is the loss 0.11116718500852585\n",
      "This is the loss 0.14610517024993896\n",
      "This is the loss 0.11550997197628021\n",
      "This is the loss 0.16218113899230957\n",
      "This is the loss 0.13082726299762726\n",
      "This is the loss 0.1326880156993866\n",
      "This is the loss 0.11531156301498413\n",
      "This is the loss 0.12912970781326294\n",
      "This is the loss 0.16083893179893494\n",
      "This is the loss 0.16382485628128052\n",
      "This is the loss 0.1517517864704132\n",
      "This is the loss 0.11464648693799973\n",
      "This is the loss 0.1539739966392517\n",
      "This is the loss 0.1655799150466919\n",
      "This is the loss 0.1429150253534317\n",
      "This is the loss 0.15343765914440155\n",
      "This is the loss 0.11783449351787567\n",
      "This is the loss 0.14090201258659363\n",
      "This is the loss 0.11442525684833527\n",
      "This is the loss 0.1692550629377365\n",
      "This is the loss 0.16806156933307648\n",
      "This is the loss 0.1531064361333847\n",
      "This is the loss 0.13381895422935486\n",
      "This is the loss 0.13393890857696533\n",
      "This is the loss 0.12374137341976166\n",
      "This is the loss 0.1364336609840393\n",
      "This is the loss 0.11845390498638153\n",
      "This is the loss 0.1385755091905594\n",
      "This is the loss 0.11687075346708298\n",
      "This is the loss 0.08135984092950821\n",
      "This is the loss 0.11203206330537796\n",
      "This is the loss 0.12853394448757172\n",
      "This is the loss 0.14859840273857117\n",
      "This is the loss 0.12899461388587952\n",
      "This is the loss 0.12062819302082062\n",
      "This is the loss 0.13304394483566284\n",
      "This is the loss 0.13453873991966248\n",
      "This is the loss 0.13692055642604828\n",
      "This is the loss 0.13121670484542847\n",
      "This is the loss 0.1310257613658905\n",
      "This is the loss 0.13586951792240143\n",
      "This is the loss 0.13788531720638275\n",
      "This is the loss 0.12270288169384003\n",
      "This is the loss 0.1504012793302536\n",
      "This is the loss 0.1592906266450882\n",
      "This is the loss 0.14926782250404358\n",
      "This is the loss 0.14443251490592957\n",
      "This is the loss 0.14295731484889984\n",
      "This is the loss 0.14860662817955017\n",
      "This is the loss 0.16856259107589722\n",
      "This is the loss 0.16924399137496948\n",
      "This is the loss 0.14039689302444458\n",
      "This is the loss 0.1660025268793106\n",
      "This is the loss 0.1652919054031372\n",
      "This is the loss 0.14898759126663208\n",
      "This is the loss 0.1630806028842926\n",
      "This is the loss 0.1379251480102539\n",
      "This is the loss 0.1148679181933403\n",
      "This is the loss 0.12809358537197113\n",
      "This is the loss 0.11984887719154358\n",
      "This is the loss 0.16011999547481537\n",
      "This is the loss 0.1604197472333908\n",
      "This is the loss 0.13726644217967987\n",
      "This is the loss 0.18394193053245544\n",
      "This is the loss 0.13144782185554504\n",
      "This is the loss 0.1643921136856079\n",
      "This is the loss 0.14107166230678558\n",
      "This is the loss 0.19344459474086761\n",
      "This is the loss 0.15034624934196472\n",
      "This is the loss 0.13553500175476074\n",
      "This is the loss 0.14933255314826965\n",
      "This is the loss 0.15710562467575073\n",
      "This is the loss 0.1402561068534851\n",
      "This is the loss 0.15160302817821503\n",
      "This is the loss 0.1447630524635315\n",
      "This is the loss 0.1370909959077835\n",
      "This is the loss 0.14452309906482697\n",
      "This is the loss 0.12810684740543365\n",
      "This is the loss 0.12781363725662231\n",
      "This is the loss 0.14862877130508423\n",
      "This is the loss 0.15407638251781464\n",
      "This is the loss 0.1262974888086319\n",
      "This is the loss 0.14025640487670898\n",
      "This is the loss 0.11908620595932007\n",
      "This is the loss 0.1000451073050499\n",
      "This is the loss 0.1203487440943718\n",
      "This is the loss 0.1290530264377594\n",
      "This is the loss 0.14033764600753784\n",
      "This is the loss 0.12042851746082306\n",
      "This is the loss 0.15402677655220032\n",
      "This is the loss 0.13224224746227264\n",
      "This is the loss 0.1258021593093872\n",
      "This is the loss 0.15029792487621307\n",
      "This is the loss 0.10905607789754868\n",
      "This is the loss 0.11576636880636215\n",
      "This is the loss 0.13417015969753265\n",
      "This is the loss 0.12318649888038635\n",
      "This is the loss 0.1593150943517685\n",
      "This is the loss 0.15162110328674316\n",
      "This is the loss 0.14856450259685516\n",
      "This is the loss 0.1418316811323166\n",
      "This is the loss 0.12236303836107254\n",
      "This is the loss 0.13939979672431946\n",
      "This is the loss 0.1320113092660904\n",
      "This is the loss 0.12853693962097168\n",
      "This is the loss 0.1497376561164856\n",
      "This is the loss 0.1561793088912964\n",
      "This is the loss 0.1544419825077057\n",
      "This is the loss 0.11682701855897903\n",
      "This is the loss 0.12418817728757858\n",
      "This is the loss 0.13109134137630463\n",
      "This is the loss 0.15779805183410645\n",
      "This is the loss 0.18331387639045715\n",
      "This is the loss 0.12323664873838425\n",
      "This is the loss 0.12715888023376465\n",
      "This is the loss 0.14170223474502563\n",
      "This is the loss 0.131919264793396\n",
      "This is the loss 0.1260853111743927\n",
      "This is the loss 0.1274089366197586\n",
      "This is the loss 0.16586080193519592\n",
      "This is the loss 0.12793868780136108\n",
      "This is the loss 0.14505767822265625\n",
      "This is the loss 0.11697734147310257\n",
      "This is the loss 0.11928121000528336\n",
      "This is the loss 0.10563991963863373\n",
      "This is the loss 0.14221146702766418\n",
      "This is the loss 0.14561623334884644\n",
      "This is the loss 0.156657874584198\n",
      "This is the loss 0.13807517290115356\n",
      "This is the loss 0.12925280630588531\n",
      "This is the loss 0.12303969264030457\n",
      "This is the loss 0.12081754207611084\n",
      "This is the loss 0.10800021141767502\n",
      "This is the loss 0.1287500113248825\n",
      "This is the loss 0.12491067498922348\n",
      "This is the loss 0.13729773461818695\n",
      "This is the loss 0.10019741207361221\n",
      "This is the loss 0.11745798587799072\n",
      "This is the loss 0.10857686400413513\n",
      "This is the loss 0.13542930781841278\n",
      "This is the loss 0.1345238983631134\n",
      "This is the loss 0.13705827295780182\n",
      "This is the loss 0.16699692606925964\n",
      "This is the loss 0.16186808049678802\n",
      "This is the loss 0.16143059730529785\n",
      "This is the loss 0.11749294400215149\n",
      "This is the loss 0.13216614723205566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.11943253874778748\n",
      "This is the loss 0.13145975768566132\n",
      "This is the loss 0.11557465046644211\n",
      "This is the loss 0.1495456099510193\n",
      "This is the loss 0.15267080068588257\n",
      "This is the loss 0.14693012833595276\n",
      "This is the loss 0.11057233810424805\n",
      "This is the loss 0.13499656319618225\n",
      "This is the loss 0.13364484906196594\n",
      "This is the loss 0.13620387017726898\n",
      "This is the loss 0.16283318400382996\n",
      "This is the loss 0.14820319414138794\n",
      "This is the loss 0.13595056533813477\n",
      "This is the loss 0.12669746577739716\n",
      "This is the loss 0.12411542981863022\n",
      "This is the loss 0.13812008500099182\n",
      "This is the loss 0.1631203293800354\n",
      "This is the loss 0.1508125215768814\n",
      "This is the loss 0.16580747067928314\n",
      "This is the loss 0.17616914212703705\n",
      "This is the loss 0.12025313079357147\n",
      "This is the loss 0.1605108380317688\n",
      "This is the loss 0.11918336153030396\n",
      "This is the loss 0.1515600085258484\n",
      "This is the loss 0.1341247856616974\n",
      "This is the loss 0.12534086406230927\n",
      "This is the loss 0.150345116853714\n",
      "This is the loss 0.12521585822105408\n",
      "This is the loss 0.1587510108947754\n",
      "This is the loss 0.12966345250606537\n",
      "This is the loss 0.12364018708467484\n",
      "This is the loss 0.15545222163200378\n",
      "This is the loss 0.10998768359422684\n",
      "This is the loss 0.1271904706954956\n",
      "This is the loss 0.11271537095308304\n",
      "This is the loss 0.12721990048885345\n",
      "This is the loss 0.1587567776441574\n",
      "This is the loss 0.12756460905075073\n",
      "This is the loss 0.13462308049201965\n",
      "This is the loss 0.1281377673149109\n",
      "This is the loss 0.09743476659059525\n",
      "This is the loss 0.12468759715557098\n",
      "This is the loss 0.1144557073712349\n",
      "This is the loss 0.10334955900907516\n",
      "This is the loss 0.14091996848583221\n",
      "This is the loss 0.12664149701595306\n",
      "This is the loss 0.096980020403862\n",
      "This is the loss 0.1223285123705864\n",
      "This is the loss 0.14074072241783142\n",
      "This is the loss 0.14568473398685455\n",
      "This is the loss 0.12367135286331177\n",
      "This is the loss 0.11653868108987808\n",
      "This is the loss 0.1295819729566574\n",
      "This is the loss 0.11839981377124786\n",
      "This is the loss 0.12025649100542068\n",
      "This is the loss 0.155114084482193\n",
      "This is the loss 0.16147421300411224\n",
      "This is the loss 0.1303817480802536\n",
      "This is the loss 0.15143173933029175\n",
      "This is the loss 0.13342557847499847\n",
      "This is the loss 0.1411200761795044\n",
      "This is the loss 0.13545915484428406\n",
      "This is the loss 0.1234351098537445\n",
      "This is the loss 0.16705356538295746\n",
      "This is the loss 0.12488798797130585\n",
      "This is the loss 0.10808785259723663\n",
      "This is the loss 0.14148089289665222\n",
      "This is the loss 0.1359413117170334\n",
      "This is the loss 0.15537627041339874\n",
      "This is the loss 0.1621873378753662\n",
      "This is the loss 0.15158677101135254\n",
      "This is the loss 0.12663863599300385\n",
      "This is the loss 0.1507708579301834\n",
      "This is the loss 0.14447130262851715\n",
      "This is the loss 0.1489897072315216\n",
      "This is the loss 0.17200586199760437\n",
      "This is the loss 0.1417773813009262\n",
      "This is the loss 0.13705229759216309\n",
      "This is the loss 0.1442500203847885\n",
      "This is the loss 0.13875305652618408\n",
      "This is the loss 0.16974890232086182\n",
      "This is the loss 0.1350116729736328\n",
      "This is the loss 0.14603401720523834\n",
      "This is the loss 0.12653081119060516\n",
      "This is the loss 0.1483742892742157\n",
      "This is the loss 0.14626778662204742\n",
      "This is the loss 0.14590586721897125\n",
      "This is the loss 0.13708099722862244\n",
      "This is the loss 0.16264048218727112\n",
      "This is the loss 0.12035998702049255\n",
      "This is the loss 0.15021713078022003\n",
      "This is the loss 0.157377690076828\n",
      "This is the loss 0.15603649616241455\n",
      "This is the loss 0.14412927627563477\n",
      "This is the loss 0.1416761577129364\n",
      "This is the loss 0.14510630071163177\n",
      "This is the loss 0.16361801326274872\n",
      "This is the loss 0.13553448021411896\n",
      "This is the loss 0.1256539225578308\n",
      "This is the loss 0.14916323125362396\n",
      "This is the loss 0.16990850865840912\n",
      "This is the loss 0.1640763133764267\n",
      "This is the loss 0.1441689282655716\n",
      "This is the loss 0.14621147513389587\n",
      "This is the loss 0.12684889137744904\n",
      "This is the loss 0.1514010727405548\n",
      "This is the loss 0.13917449116706848\n",
      "This is the loss 0.178152933716774\n",
      "This is the loss 0.1819799840450287\n",
      "This is the loss 0.15047018229961395\n",
      "This is the loss 0.1400153934955597\n",
      "This is the loss 0.13444125652313232\n",
      "This is the loss 0.11115482449531555\n",
      "This is the loss 0.16689902544021606\n",
      "This is the loss 0.13055013120174408\n",
      "This is the loss 0.1498548686504364\n",
      "This is the loss 0.19146087765693665\n",
      "This is the loss 0.1357804834842682\n",
      "This is the loss 0.16666704416275024\n",
      "This is the loss 0.12533609569072723\n",
      "This is the loss 0.11717062443494797\n",
      "This is the loss 0.17085884511470795\n",
      "This is the loss 0.13919001817703247\n",
      "This is the loss 0.16769175231456757\n",
      "This is the loss 0.15935930609703064\n",
      "This is the loss 0.15185904502868652\n",
      "This is the loss 0.15770050883293152\n",
      "This is the loss 0.16833843290805817\n",
      "This is the loss 0.15311485528945923\n",
      "This is the loss 0.12274248898029327\n",
      "This is the loss 0.1875258982181549\n",
      "This is the loss 0.1507122814655304\n",
      "This is the loss 0.13677504658699036\n",
      "This is the loss 0.1366031914949417\n",
      "This is the loss 0.14667585492134094\n",
      "This is the loss 0.15640592575073242\n",
      "This is the loss 0.15038567781448364\n",
      "This is the loss 0.13599014282226562\n",
      "This is the loss 0.1615673154592514\n",
      "This is the loss 0.13717886805534363\n",
      "This is the loss 0.12334705144166946\n",
      "This is the loss 0.13505950570106506\n",
      "This is the loss 0.09734757244586945\n",
      "This is the loss 0.13705861568450928\n",
      "This is the loss 0.13886618614196777\n",
      "This is the loss 0.16269613802433014\n",
      "This is the loss 0.14250774681568146\n",
      "This is the loss 0.145303875207901\n",
      "This is the loss 0.15618664026260376\n",
      "This is the loss 0.16109640896320343\n",
      "This is the loss 0.17601995170116425\n",
      "This is the loss 0.15479032695293427\n",
      "This is the loss 0.16154761612415314\n",
      "This is the loss 0.1613783985376358\n",
      "This is the loss 0.181264266371727\n",
      "This is the loss 0.1208491176366806\n",
      "This is the loss 0.1457623392343521\n",
      "This is the loss 0.15536832809448242\n",
      "This is the loss 0.18050867319107056\n",
      "This is the loss 0.15295326709747314\n",
      "This is the loss 0.1404770314693451\n",
      "This is the loss 0.18581822514533997\n",
      "This is the loss 0.17977619171142578\n",
      "This is the loss 0.12660282850265503\n",
      "This is the loss 0.16051289439201355\n",
      "This is the loss 0.15352673828601837\n",
      "This is the loss 0.14573611319065094\n",
      "This is the loss 0.1421993523836136\n",
      "This is the loss 0.146865576505661\n",
      "This is the loss 0.1260690689086914\n",
      "This is the loss 0.1537657529115677\n",
      "This is the loss 0.12129156291484833\n",
      "This is the loss 0.1316368579864502\n",
      "This is the loss 0.1342988908290863\n",
      "This is the loss 0.1579044908285141\n",
      "This is the loss 0.17245227098464966\n",
      "This is the loss 0.17109762132167816\n",
      "This is the loss 0.1398557722568512\n",
      "This is the loss 0.13177922368049622\n",
      "This is the loss 0.12257908284664154\n",
      "This is the loss 0.1180654764175415\n",
      "This is the loss 0.12210066616535187\n",
      "This is the loss 0.11844410747289658\n",
      "This is the loss 0.15673641860485077\n",
      "This is the loss 0.14096954464912415\n",
      "This is the loss 0.1636997014284134\n",
      "This is the loss 0.13749179244041443\n",
      "This is the loss 0.1165304183959961\n",
      "This is the loss 0.13484951853752136\n",
      "This is the loss 0.12222027778625488\n",
      "This is the loss 0.14088618755340576\n",
      "This is the loss 0.14676569402217865\n",
      "This is the loss 0.1579553186893463\n",
      "This is the loss 0.15126755833625793\n",
      "This is the loss 0.13949009776115417\n",
      "This is the loss 0.14955301582813263\n",
      "This is the loss 0.14323334395885468\n",
      "This is the loss 0.14781489968299866\n",
      "This is the loss 0.15038149058818817\n",
      "This is the loss 0.11735270172357559\n",
      "This is the loss 0.15919703245162964\n",
      "This is the loss 0.1333489567041397\n",
      "This is the loss 0.12646779417991638\n",
      "This is the loss 0.13586249947547913\n",
      "This is the loss 0.1408575028181076\n",
      "This is the loss 0.16044023633003235\n",
      "This is the loss 0.12895670533180237\n",
      "This is the loss 0.144561767578125\n",
      "This is the loss 0.13538758456707\n",
      "This is the loss 0.12530356645584106\n",
      "This is the loss 0.15707777440547943\n",
      "This is the loss 0.13859272003173828\n",
      "This is the loss 0.1465998739004135\n",
      "This is the loss 0.118785560131073\n",
      "This is the loss 0.11285121738910675\n",
      "This is the loss 0.16167804598808289\n",
      "This is the loss 0.14274391531944275\n",
      "This is the loss 0.11396605521440506\n",
      "This is the loss 0.13533735275268555\n",
      "This is the loss 0.14491300284862518\n",
      "This is the loss 0.12018617987632751\n",
      "This is the loss 0.14855775237083435\n",
      "This is the loss 0.10452301055192947\n",
      "This is the loss 0.15094663202762604\n",
      "This is the loss 0.14745865762233734\n",
      "This is the loss 0.13096824288368225\n",
      "This is the loss 0.13139578700065613\n",
      "This is the loss 0.12240971624851227\n",
      "This is the loss 0.1618250161409378\n",
      "This is the loss 0.1232529878616333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the loss 0.15915018320083618\n",
      "This is the loss 0.12258855253458023\n",
      "This is the loss 0.12642648816108704\n",
      "This is the loss 0.16000929474830627\n",
      "This is the loss 0.14305530488491058\n",
      "This is the loss 0.12983554601669312\n",
      "This is the loss 0.13714787364006042\n",
      "This is the loss 0.1356678605079651\n",
      "This is the loss 0.12768639624118805\n",
      "This is the loss 0.16233065724372864\n",
      "This is the loss 0.11810469627380371\n",
      "This is the loss 0.1643591821193695\n",
      "This is the loss 0.1308518797159195\n",
      "This is the loss 0.11552058160305023\n",
      "This is the loss 0.1311185359954834\n",
      "This is the loss 0.14571399986743927\n",
      "This is the loss 0.13570617139339447\n",
      "This is the loss 0.16557112336158752\n",
      "This is the loss 0.1426713466644287\n",
      "This is the loss 0.12238145619630814\n",
      "This is the loss 0.12241549044847488\n",
      "This is the loss 0.16138820350170135\n",
      "This is the loss 0.14233750104904175\n",
      "This is the loss 0.12068676948547363\n",
      "This is the loss 0.12218883633613586\n",
      "This is the loss 0.15574944019317627\n",
      "This is the loss 0.1546746790409088\n",
      "This is the loss 0.13338977098464966\n",
      "This is the loss 0.13615994155406952\n",
      "This is the loss 0.1264820694923401\n",
      "This is the loss 0.14373424649238586\n",
      "This is the loss 0.1577572077512741\n",
      "This is the loss 0.11223918199539185\n",
      "This is the loss 0.1083161011338234\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for _ in range(episodes):\n",
    "    for idx in range(int(len(df) / batch_size)):\n",
    "        curr_df = df.iloc[idx*batch_size:(idx+1)*batch_size]\n",
    "        white_tensors = player_tensors[curr_df[\"white_mapped\"].values, :]\n",
    "        black_tensors = player_tensors[curr_df[\"black_mapped\"].values, :]\n",
    "        predicted = model(white_tensors, black_tensors)\n",
    "        result = torch.from_numpy(curr_df['score'].values).reshape(-1, 1)\n",
    "        loss = criterion(result, predicted)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        player_tensors = player_tensors - lr_p * player_tensors.grad\n",
    "        player_tensors.grad = torch.zeros(player_tensors.shape)\n",
    "        optimizer.step()\n",
    "        print(f\"This is the loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5312)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5433, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(model(white_tensors, black_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(result, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1144, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(predicted, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2, 100, 200],\n",
       "        [  3,   4, 300, 400]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.tensor([[1, 2], [3, 4]]), torch.tensor([[100, 200], [300, 400]])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2694, -0.4357,  1.6758],\n",
       "        [ 0.2694, -0.4357,  1.6758]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors[[7029, 7029], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2694, -0.4357,  1.6758],\n",
       "        [ 0.2694, -0.4357,  1.6758],\n",
       "        [ 2.2708, -0.5154, -0.3654],\n",
       "        [ 2.2708, -0.5154, -0.3654],\n",
       "        [ 2.2708, -0.5154, -0.3654],\n",
       "        [ 2.2708, -0.5154, -0.3654],\n",
       "        [ 0.2392, -1.2029,  0.4947],\n",
       "        [ 0.0233,  0.2722, -1.4982],\n",
       "        [ 0.0233,  0.2722, -1.4982],\n",
       "        [ 0.0233,  0.2722, -1.4982],\n",
       "        [ 0.0233,  0.2722, -1.4982],\n",
       "        [ 0.7296,  0.7249,  1.0557],\n",
       "        [ 0.7296,  0.7249,  1.0557],\n",
       "        [ 0.7296,  0.7249,  1.0557],\n",
       "        [-1.1632,  0.0427,  1.5910],\n",
       "        [-1.1632,  0.0427,  1.5910],\n",
       "        [-1.7722,  0.1141,  0.3663],\n",
       "        [ 0.6022,  0.8493,  1.8661],\n",
       "        [ 0.6022,  0.8493,  1.8661],\n",
       "        [ 1.0264,  0.2670, -1.2382],\n",
       "        [ 1.0264,  0.2670, -1.2382],\n",
       "        [ 1.0264,  0.2670, -1.2382],\n",
       "        [ 1.0264,  0.2670, -1.2382],\n",
       "        [-1.6563, -0.7476,  1.3920],\n",
       "        [-1.6563, -0.7476,  1.3920],\n",
       "        [-0.8079, -0.5157, -0.6920],\n",
       "        [-0.8079, -0.5157, -0.6920],\n",
       "        [ 2.1594,  1.3099, -0.5062],\n",
       "        [ 0.1806,  0.7540, -1.6752],\n",
       "        [-0.5438, -1.6881,  1.3269],\n",
       "        [-1.5572, -0.4885, -0.4393],\n",
       "        [-1.5572, -0.4885, -0.4393],\n",
       "        [ 0.1992, -0.1270, -0.2603],\n",
       "        [ 0.1992, -0.1270, -0.2603],\n",
       "        [ 1.0746, -0.2690, -0.5695],\n",
       "        [ 1.0746, -0.2690, -0.5695],\n",
       "        [ 1.0746, -0.2690, -0.5695],\n",
       "        [ 0.3139,  1.7452,  0.6898],\n",
       "        [ 0.9992, -0.4497, -2.1869],\n",
       "        [ 0.9992, -0.4497, -2.1869],\n",
       "        [ 0.5275,  0.3121,  0.1365],\n",
       "        [ 0.2768, -1.1555,  1.8925],\n",
       "        [ 0.8042, -2.3442, -1.7678],\n",
       "        [ 1.7171,  0.3699,  0.5407],\n",
       "        [ 0.0816,  0.2116,  0.1976],\n",
       "        [ 1.5052, -0.3770,  1.4284],\n",
       "        [ 1.5052, -0.3770,  1.4284],\n",
       "        [ 1.5052, -0.3770,  1.4284],\n",
       "        [ 1.5396,  0.5971,  1.6219],\n",
       "        [-0.1253, -1.5633,  0.7793],\n",
       "        [ 0.6755,  0.0067, -0.9472],\n",
       "        [ 0.6755,  0.0067, -0.9472],\n",
       "        [ 0.4382,  1.2819,  1.1215],\n",
       "        [ 0.4382,  1.2819,  1.1215],\n",
       "        [ 0.4382,  1.2819,  1.1215],\n",
       "        [ 0.2063, -1.0139, -0.1396],\n",
       "        [ 1.2881,  0.0760,  0.2229],\n",
       "        [ 2.0172, -0.4534,  0.4113],\n",
       "        [ 2.0172, -0.4534,  0.4113],\n",
       "        [ 2.0172, -0.4534,  0.4113],\n",
       "        [ 2.0172, -0.4534,  0.4113],\n",
       "        [ 2.0172, -0.4534,  0.4113],\n",
       "        [-0.9477, -1.7905,  0.3770],\n",
       "        [-0.9477, -1.7905,  0.3770]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors[curr_df[\"white_mapped\"].values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7301.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.483359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.370023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.454035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.474351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.507286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.724442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  7301.000000\n",
       "mean      0.483359\n",
       "std       0.042100\n",
       "min       0.370023\n",
       "25%       0.454035\n",
       "50%       0.474351\n",
       "75%       0.507286\n",
       "max       0.724442"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model(player_tensors).detach().numpy()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7772,  2.0401,  0.7064],\n",
       "        [ 0.2645,  0.1542, -2.1027],\n",
       "        [-1.2121,  1.1437,  1.7996],\n",
       "        ...,\n",
       "        [ 0.2646,  0.4408,  1.6197],\n",
       "        [-1.2699,  1.3298,  0.5026],\n",
       "        [-0.6563,  0.0439,  0.0662]], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataframetrainer",
   "language": "python",
   "name": "dataframetrainer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
